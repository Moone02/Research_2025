{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba92b65f",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "# Step 1: Introduction and Core Physics\n",
    "\n",
    "This notebook is a self-contained tutorial that uses the `nrpy` library to construct a complete C-language project for integrating photon geodesics in curved spacetimes. The resulting C code is a flexible, high-performance ray-tracing engine capable of generating gravitationally lensed images of distant sources as seen by an observer near a black hole.\n",
    "\n",
    "The core of the project is the numerical solution of the geodesic equation, which describes the path of a free-falling particle (or photon) through curved spacetime. The geodesic equation, as detailed on [Wikipedia](https://en.wikipedia.org/wiki/Geodesic_equation), is a second-order ordinary differential equation (ODE) that relates a particle's acceleration to the spacetime curvature, represented by the Christoffel symbols ($\\Gamma^{\\alpha}_{\\mu\\nu}$):\n",
    "\n",
    "$$ \\frac{d^2x^{\\alpha}}{d\\lambda^2} = -\\Gamma^{\\alpha}_{\\mu\\nu} \\frac{dx^{\\mu}}{d\\lambda} \\frac{dx^{\\nu}}{d\\lambda} $$\n",
    "\n",
    "Here, $x^\\alpha = (t, x, y, z)$ are the spacetime coordinates, and $\\lambda$ is the affine parameter, which measures the proper distance along the path for a massive particle or a suitable path parameter for a photon.\n",
    "\n",
    "### The Reverse Ray-Tracing Transformation\n",
    "\n",
    "To render an image of what an observer sees, we must trace the photon's path from the observer's camera *backwards in time* to its source. While we could integrate the geodesic equation with a negative step `dλ < 0`, most ODE solvers are optimized for forward integration with a positive step. To accommodate this, we perform a change of variables on the affine parameter. We define a new parameter, $\\kappa$, that increases as the original parameter, $\\lambda$, decreases:\n",
    "\n",
    "$$ \\kappa = -\\lambda \\implies d\\kappa = -d\\lambda \\implies \\frac{d}{d\\lambda} = -\\frac{d}{d\\kappa} $$\n",
    "\n",
    "We now substitute this transformation directly into the second-order geodesic equation:\n",
    "\n",
    "$$ \\frac{d}{d\\lambda}\\left(\\frac{dx^{\\alpha}}{d\\lambda}\\right) = -\\Gamma^{\\alpha}_{\\mu\\nu} \\frac{dx^{\\mu}}{d\\lambda} \\frac{dx^{\\nu}}{d\\lambda} $$\n",
    "\n",
    "Applying the chain rule, $\\frac{d}{d\\lambda} = -\\frac{d}{d\\kappa}$:\n",
    "\n",
    "$$ \\left(-\\frac{d}{d\\kappa}\\right)\\left(-\\frac{dx^{\\alpha}}{d\\kappa}\\right) = -\\Gamma^{\\alpha}_{\\mu\\nu} \\left(-\\frac{dx^{\\mu}}{d\\kappa}\\right) \\left(-\\frac{dx^{\\nu}}{d\\kappa}\\right) $$\n",
    "\n",
    "The negatives on both sides cancel, yielding the reverse-time geodesic equation:\n",
    "\n",
    "$$ \\frac{d^2x^{\\alpha}}{d\\kappa^2} = -\\Gamma^{\\alpha}_{\\mu\\nu} \\frac{dx^{\\mu}}{d\\kappa} \\frac{dx^{\\nu}}{d\\kappa} $$\n",
    "\n",
    "This equation has the same form as the original, but describes the path integrated with respect to $\\kappa$. To solve it numerically, we now decompose this second-order ODE into a system of coupled first-order ODEs. We define the **reverse-time momentum**, $p^\\alpha$, as the 4-velocity with respect to our new parameter $\\kappa$:\n",
    "\n",
    "$$ p^{\\alpha} \\equiv \\frac{dx^{\\alpha}}{d\\kappa} $$\n",
    "\n",
    "This definition immediately gives us our first ODE. We find the second by substituting $p^\\alpha$ into the reverse-time geodesic equation:\n",
    "\n",
    "$$ \\frac{d}{d\\kappa}\\left(\\frac{dx^{\\alpha}}{d\\kappa}\\right) = -\\Gamma^{\\alpha}_{\\mu\\nu} \\left(\\frac{dx^{\\mu}}{d\\kappa}\\right) \\left(\\frac{dx^{\\nu}}{d\\kappa}\\right) \\implies \\frac{dp^{\\alpha}}{d\\kappa} = -\\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "\n",
    "This gives us the final set of ODEs that our C code will solve. We also add a third ODE to track the total proper distance traveled by the photon along its spatial path, using the spatial part of the metric $\\gamma_{ij}$:\n",
    "\n",
    "1.  **Position ODE**: $\\frac{dx^{\\alpha}}{d\\kappa} = p^{\\alpha}$\n",
    "2.  **Momentum ODE**: $\\frac{dp^{\\alpha}}{d\\kappa} = -\\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu}$\n",
    "3.  **Path Length ODE**: $\\frac{dL}{d\\kappa} = \\sqrt{\\gamma_{ij} \\frac{dx^i}{d\\kappa} \\frac{dx^j}{d\\kappa}} = \\sqrt{\\gamma_{ij}p^{i}p^{j}}$\n",
    "\n",
    "### Initial Conditions\n",
    "\n",
    "The initial value of the reverse-time momentum, $p^\\alpha_{\\text{initial}}$, determines the starting direction of the ray traced from the camera. It is physically equivalent to the *negative* of the final momentum of a photon that started at a distant source and ended its journey at the camera. If we denote the physical, forward-time 4-velocity as $k^\\alpha = dx^\\alpha/d\\lambda$, then:\n",
    "\n",
    "$$ p^\\alpha_{\\text{initial}} = \\left(\\frac{dx^\\alpha}{d\\kappa}\\right)_{\\text{initial}} = -\\left(\\frac{dx^\\alpha}{d\\lambda}\\right)_{\\text{final}} = -k^\\alpha_{\\text{final}} $$\n",
    "\n",
    "This relationship is key: setting the initial conditions for our reverse-time integration is equivalent to choosing the final momentum of a physically forward-propagating photon arriving at the camera.\n",
    "\n",
    "This notebook follows a modular, single-responsibility design pattern. It uses the `nrpy` library to first define the underlying physics symbolically, and then automatically generates a series of interoperable C functions, each with a specific job. This makes the final C project clear, efficient, and easily extensible.\n",
    "\n",
    "**Notebook Status:** <font color='green'><b>Validated</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bac0cb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "This notebook is organized into a series of logical steps, with each core Python function encapsulated in its own cell. This modular design enhances readability and maintainability.\n",
    "\n",
    "1.  [Step 1: Introduction and Core Physics](#introduction)\n",
    "    *   The Geodesic Equation\n",
    "    *   The Reverse Ray-Tracing Transformation\n",
    "    *   Initial Conditions\n",
    "2.  [Step 2: Project Initialization](#initialize)\n",
    "    *   Importing Libraries\n",
    "    *   Directory Management\n",
    "    *   Defining Physical and Runtime Parameters\n",
    "3.  [Step 3: The Symbolic Core - Foundational Math](#symbolic_core)\n",
    "    *   [3.a: Metric Tensor Derivatives](#deriv_g4DD)\n",
    "    *   [3.b: Christoffel Symbol Calculation](#four_connections)\n",
    "    *   [3.c: Geodesic Momentum RHS](#geodesic_mom_rhs)\n",
    "    *   [3.d: Geodesic Position RHS](#geodesic_pos_rhs)\n",
    "    *   [3.e: Proper Length ODE RHS](#proper_len_rhs)\n",
    "    *   [3.f: Symbolic Calculation of p⁰](#geodesic_mom0_calc)\n",
    "4.  [Step 4: Spacetime Definition in Kerr-Schild Coordinates](#spacetime_definition)\n",
    "    *   The Kerr-Schild Metric\n",
    "5.  [Step 5: Symbolic Workflow Execution](#symbolic_execution)\n",
    "    *   Applying Blueprints to the Metric\n",
    "6.  [Step 6: C Code Generation - Physics \"Engines\" and \"Workers\"](#generate_c_engines)\n",
    "    *   [6.a: `g4DD_kerr_schild()` Worker](#g4DD_kerr_schild_engine)\n",
    "    *   [6.b: `con_kerr_schild()` Worker](#con_kerr_schild_engine)\n",
    "    *   [6.c: `calculate_p0_reverse()` Engine](#calculate_p0_engine)\n",
    "    *   [6.d: `calculate_ode_rhs()` Engine](#calculate_ode_rhs_engine)\n",
    "    *   [6.e: `find_event_time_and_state()` Interpolation Engine](#lagrange_interp_engine)\n",
    "7.  [Step 7: C Code Generation - Orchestrators and Dispatchers](#generate_c_orchestrators)\n",
    "    *   [7.a: `g4DD_metric()` Dispatcher](#g4DD_metric_dispatcher)\n",
    "    *   [7.b: `connections()` Dispatcher](#connections_dispatcher)\n",
    "    *   [7.c: `set_initial_conditions_cartesian()` Orchestrator](#set_initial_conditions_cartesian)\n",
    "    *   [7.d: The GSL Wrapper Function](#gsl_wrapper)\n",
    "    *   [7.e: The Main Integration Loop](#integration_loop)\n",
    "    *   [7.f: Data Processing and Saving](#data_processing)\n",
    "    *   [7.g: The `main()` C Function Entry Point](#main_entry_point)\n",
    "8.  [Step 8: Project Assembly and Compilation](#assemble_project)\n",
    "    *   [8.a: Custom Data Structures](#register_structs)\n",
    "    *   [8.b: Final Build Command](#final_build)\n",
    "9.  [Step 9: Visualization and Analysis](#plotting)\n",
    "    *   [9.a: 3D Scene Geometry Visualizer](#plot_3d_scene)\n",
    "    *   [9.b: Blueprint File Statistical Analysis](#plot_stats)\n",
    "    *   [9.c: Unlensed Source Disk Visualizer](#plot_unlensed)\n",
    "    *   [9.d: Final Lensed Image Renderer](#plot_lensed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c0d30",
   "metadata": {},
   "source": [
    "<a id='initialize'></a>\n",
    "# Step 2: Project Initialization\n",
    "\n",
    "This cell sets up the foundational elements for our entire project. It performs three key tasks:\n",
    "\n",
    "1.  **Import Libraries**: We import necessary modules from standard Python libraries (`os`, `shutil`, `sympy`) and the core components of `nrpy`. The `nrpy` imports provide tools for C function registration, C code generation, parameter handling, and infrastructure management.\n",
    "\n",
    "2.  **Directory Management**: A clean output directory, `project/photon_geodesic_integrator/`, is created to store the generated C code, ensuring a fresh build every time the notebook is run.\n",
    "\n",
    "3.  **Physical and Runtime Parameter Definition**: We define the many parameters that control the simulation using `nrpy.params.CodeParameter`. This is the central object for defining a runtime parameter that will be accessible in the generated C code. It registers each parameter's name, C type, default value, and properties in a global dictionary, which `nrpy`'s build system then uses to construct the C interface.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.params.set_parval_from_str(par_name, value)`**:\n",
    "    *   **Source File**: `nrpy/params.py`\n",
    "    *   **Description**: Sets the value of a core `nrpy` parameter. Here, it is used to specify that we are using the `BHaH` (BlackHoles@Home) C code generation infrastructure.\n",
    "\n",
    "*   **`nrpy.params.CodeParameter(c_type, module, name, default_value, **kwargs)`**:\n",
    "    *   **Source File**: `nrpy/params.py`\n",
    "    *   **Description**: This is the primary function for registering a C-level parameter. It creates a parameter object that holds all its properties.\n",
    "    *   **Key Inputs**:\n",
    "        *   `c_type`: The data type of the parameter in the C code (e.g., `\"REAL\"`, `\"int\"`).\n",
    "        *   `module`: The name of the Python module where the parameter is defined (usually `__name__`).\n",
    "        *   `name`: The C variable name for the parameter.\n",
    "        *   `default_value`: The default value for the parameter.\n",
    "    *   **Key Keyword Arguments (`kwargs`)**:\n",
    "        *   `commondata=True`: Specifies that the parameter is \"common\" to the entire simulation (e.g., black hole mass `M_scale`). It will be stored in the `commondata_struct` in the generated C code. If `False`, it's stored in the grid-specific `params_struct`.\n",
    "        *   `add_to_parfile=True`: Instructs the build system to add an entry for this parameter to a default parameter file, making it easy to configure at runtime.\n",
    "        *   `add_to_set_CodeParameters_h=True`: This is a crucial flag that enables the \"automatic unpacking\" mechanism, also known as the \"Triple-Lock\" system. It tells `nrpy` to add an entry for the parameter to the `set_CodeParameters.h` convenience header. Any C function registered with `include_CodeParameters_h=True` will get a local `const REAL` variable with the same name as the parameter, making the C code clean and readable. This is handled by the `nrpy.infrastructures.BHaH.CodeParameters` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f07e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Registering CodeParameters for adaptive window grids...\n",
      "-> Registering CodeParameters for the disk bounding box...\n"
     ]
    }
   ],
   "source": [
    "# Cell ID: 33f07e1c (Replacement)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sympy as sp\n",
    "\n",
    "# NRPy-related imports for C-code generation\n",
    "import nrpy.c_function as cfc\n",
    "import nrpy.c_codegen as ccg\n",
    "import nrpy.params as par\n",
    "import nrpy.indexedexp as ixp\n",
    "import nrpy.infrastructures.BHaH.BHaH_defines_h as Bdefines_h\n",
    "import nrpy.infrastructures.BHaH.Makefile_helpers as Makefile\n",
    "from nrpy.infrastructures.BHaH import cmdline_input_and_parfiles\n",
    "import nrpy.helpers.generic as gh\n",
    "import nrpy.infrastructures.BHaH.CodeParameters as CPs\n",
    "\n",
    "\n",
    "# Set project name and clean the output directory\n",
    "project_name = \"photon_geodesic_integrator\"\n",
    "project_dir = os.path.join(\"project\", project_name)\n",
    "shutil.rmtree(project_dir, ignore_errors=True)\n",
    "\n",
    "# Set NRPy parameters for the BHaH infrastructure\n",
    "par.set_parval_from_str(\"Infrastructure\", \"BHaH\")\n",
    "\n",
    "# --- Core Simulation & Physics Parameters ---\n",
    "# Register single integer and boolean parameters\n",
    "_ = par.register_CodeParameter(\"int\", __name__, \"metric_choice\", 0, add_to_parfile=True, commondata=True)\n",
    "_ = par.register_CodeParameters(\n",
    "    \"bool\", __name__,\n",
    "    [\"perform_conservation_check\", \"debug_mode\"],\n",
    "    False,  # Assign False to both\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "\n",
    "# Register physical parameters for the black hole.\n",
    "# Note: These need add_to_set_CodeParameters_h=True\n",
    "M_scale, a_spin = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"M_scale\", \"a_spin\"],\n",
    "    [1.0, 0.0],\n",
    "    add_to_parfile=True, commondata=True, add_to_set_CodeParameters_h=True\n",
    ")\n",
    "\n",
    "# --- Universal Camera System Parameters ---\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"camera_pos_x\", \"camera_pos_y\", \"camera_pos_z\"],\n",
    "    [0.0, 0.0, 51.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"window_center_x\", \"window_center_y\", \"window_center_z\"],\n",
    "    [0.0, 0.0, 50.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"window_up_vec_x\", \"window_up_vec_y\", \"window_up_vec_z\"],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "\n",
    "# --- Independent Source Plane Definition ---\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"source_plane_normal_x\", \"source_plane_normal_y\", \"source_plane_normal_z\"],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "# Use a single default value for all center coordinates\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"source_plane_center_x\", \"source_plane_center_y\", \"source_plane_center_z\"],\n",
    "    0.0,\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"source_up_vec_x\", \"source_up_vec_y\", \"source_up_vec_z\"],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"source_r_min\", \"source_r_max\"],\n",
    "    [6.0, 25.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "\n",
    "# --- General Ray-Tracing & Integration Parameters ---\n",
    "_ = par.register_CodeParameter(\"int\", __name__, \"scan_density\", 512, add_to_parfile=True, commondata=True)\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"flatness_threshold\", \"r_escape\", \"t_integration_max\", \"t_start\", \"mass_snapshot_every_t\", \"delta_r_max\"],\n",
    "    [1e-2, 1500.0, 10000.0, 2000.0, 10.0, 2.0],\n",
    "    add_to_parfile=True, commondata=True\n",
    ")\n",
    "window_size = par.CodeParameter(\n",
    "    \"REAL\", __name__, \"window_size\", 1.5,\n",
    "    add_to_parfile=True, commondata=True, add_to_set_CodeParameters_h=True\n",
    ")\n",
    "\n",
    "# --- Adaptive Window Grid Parameters ---\n",
    "print(\"-> Registering CodeParameters for adaptive window grids...\")\n",
    "_ = par.register_CodeParameters(\n",
    "    \"int\", __name__,\n",
    "    [\"window_grid_type\", \"log_polar_num_r\", \"log_polar_num_phi\"],\n",
    "    [0, 512, 1024],\n",
    "    commondata=True, add_to_parfile=True\n",
    ")\n",
    "_ = par.CodeParameter(\n",
    "    \"REAL\", __name__, \"log_polar_r_min\", 0.1,\n",
    "    commondata=True, add_to_parfile=True\n",
    ")\n",
    "\n",
    "# --- Bounding Box Parameters for the Accretion Disk ---\n",
    "print(\"-> Registering CodeParameters for the disk bounding box...\")\n",
    "_ = par.register_CodeParameters(\n",
    "    \"REAL\", __name__,\n",
    "    [\"disk_bounds_x_min\", \"disk_bounds_x_max\", \"disk_bounds_y_min\", \"disk_bounds_y_max\", \"disk_bounds_z_min\", \"disk_bounds_z_max\"],\n",
    "    [-26.0, 26.0, -26.0, 26.0, -1.0, 1.0],\n",
    "    commondata=True, add_to_parfile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84ac02",
   "metadata": {},
   "source": [
    "<a id='symbolic_core'></a>\n",
    "# Step 3: The Symbolic Core - Foundational Math\n",
    "\n",
    "This section defines the pure mathematical logic of our problem using Python's `sympy` library. Each function in this section is a \"blueprint\" for a physical calculation. These functions take symbolic `sympy` objects as input and return new symbolic expressions as output. They have no knowledge of C code; they are concerned only with mathematics and will be called later to generate the \"recipes\" for our C code engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b659c",
   "metadata": {},
   "source": [
    "<a id='deriv_g4DD'></a>\n",
    "### 3.a: Metric Tensor Derivatives\n",
    "\n",
    "The first step in calculating the Christoffel symbols is to compute the partial derivatives of the metric tensor, $g_{\\mu\\nu}$. This function, `derivative_g4DD`, takes the symbolic 4x4 metric tensor `g4DD` and a list of the four coordinate symbols `xx` as input.\n",
    "\n",
    "The function iterates through all components to symbolically calculate the partial derivative of each metric component with respect to each coordinate. The resulting quantity, which we can denote using comma notation as $g_{\\mu\\nu,\\alpha}$, is defined as:\n",
    "\n",
    "$$ g_{\\mu\\nu,\\alpha} \\equiv \\frac{\\partial g_{\\mu\\nu}}{\\partial x^{\\alpha}} $$\n",
    "\n",
    "The nested `for` loops in the code directly correspond to the spacetime indices `μ, ν, α` in the physics equation. `sympy`'s built-in `sp.diff()` function is used to perform the symbolic differentiation, and the final result is returned as a rank-3 symbolic tensor.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank3(dimension)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: This function creates a symbolic rank-3 tensor (a Python list of lists of lists) of a specified dimension, with all elements initialized to the `sympy` integer 0. It is used here to create a container for the derivative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0d80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_g4DD(g4DD, xx):\n",
    "    \"\"\"Computes the symbolic first derivatives of the metric tensor.\"\"\"\n",
    "    g4DD_dD = ixp.zerorank3(dimension=4)\n",
    "    for nu in range(4):\n",
    "        for mu in range(4):\n",
    "            for alpha in range(4):\n",
    "                g4DD_dD[nu][mu][alpha] = sp.diff(g4DD[nu][mu], xx[alpha])\n",
    "    return g4DD_dD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79893b7",
   "metadata": {},
   "source": [
    "<a id='four_connections'></a>\n",
    "### 3.b: Christoffel Symbol Calculation\n",
    "\n",
    "This function implements the core formula for the Christoffel symbols of the second kind, $\\Gamma^{\\delta}_{\\mu\\nu}$. It takes the symbolic metric tensor `g4DD` ($g_{\\mu\\nu}$) and its derivatives `g4DD_dD` ($g_{\\mu\\nu,\\alpha}$) as input. The calculation requires the inverse metric, $g^{\\mu\\nu}$, which is computed using another `nrpy` helper function.\n",
    "\n",
    "The function then applies the well-known formula for the Christoffel symbols. Using the comma notation for partial derivatives, the formula is:\n",
    "\n",
    "$$ \\Gamma^{\\delta}_{\\mu\\nu} = \\frac{1}{2} g^{\\delta\\alpha} \\left( g_{\\nu\\alpha,\\mu} + g_{\\mu\\alpha,\\nu} - g_{\\mu\\nu,\\alpha} \\right) $$\n",
    "\n",
    "The Python `for` loops iterate over the spacetime indices `δ, μ, ν, α` to construct each component of the Christoffel symbol tensor. After the summation is complete, the `sp.trigsimp()` function is used to simplify the resulting expression. This trigonometric simplification is highly effective and much faster than a general `sp.simplify()` for the Kerr-Schild metric, which contains trigonometric functions of the coordinates.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank3(dimension)`**: Previously introduced. Used to initialize the Christoffel symbol tensor.\n",
    "*   **`nrpy.indexedexp.symm_matrix_inverter4x4(g4DD)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: This function takes a symbolic 4x4 symmetric matrix and analytically computes its inverse. It is highly optimized for this specific task, returning both the inverse matrix ($g^{\\mu\\nu}$) and its determinant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665bc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_connections(g4DD, g4DD_dD):\n",
    "    \"\"\"\n",
    "    Computes and simplifies Christoffel symbols from the metric and its derivatives.\n",
    "    \n",
    "    This version uses sp.trigsimp() which is highly effective and much faster\n",
    "    than sp.simplify() for the Kerr-Schild metric.\n",
    "    \"\"\"\n",
    "    Gamma4UDD = ixp.zerorank3(dimension=4)\n",
    "    g4UU, _ = ixp.symm_matrix_inverter4x4(g4DD)\n",
    "    \n",
    "    for mu in range(4):\n",
    "        for nu in range(4):\n",
    "            for delta in range(4):\n",
    "                # Calculate the Christoffel symbol component using the standard formula\n",
    "                for alpha in range(4):\n",
    "                    Gamma4UDD[delta][mu][nu] += sp.Rational(1, 2) * g4UU[delta][alpha] * \\\n",
    "                        (g4DD_dD[nu][alpha][mu] + g4DD_dD[mu][alpha][nu] - g4DD_dD[mu][nu][alpha])\n",
    "                \n",
    "                # Use sp.trigsimp() to simplify the resulting expression.\n",
    "                # This is the key to speeding up the symbolic calculation.\n",
    "                Gamma4UDD[delta][mu][nu] = sp.trigsimp(Gamma4UDD[delta][mu][nu])\n",
    "\n",
    "    return Gamma4UDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59295c0b",
   "metadata": {},
   "source": [
    "<a id='geodesic_mom_rhs'></a>\n",
    "### 3.c: Geodesic Momentum RHS\n",
    "\n",
    "This function defines the symbolic right-hand side (RHS) for the evolution of the **reverse-time momentum**, $p^{\\alpha}$. As established in the introduction, this is the second of our three first-order ODEs:\n",
    "$$ \\frac{dp^{\\alpha}}{d\\kappa} = -\\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "The function `geodesic_mom_rhs` takes the symbolic Christoffel symbols $\\Gamma^{\\alpha}_{\\mu\\nu}$ as its input. It then defines the symbolic momentum vector `pU` using `sympy`'s `sp.symbols()` function. A key `nrpy` technique is used here: the symbols are created with names that are already valid C array syntax (e.g., `\"y[4]\"`). This \"direct naming\" simplifies the final C code generation by eliminating the need for string substitutions.\n",
    "\n",
    "The core of this function constructs the symbolic expression for the RHS by performing the Einstein summation $-\\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu}$. A direct implementation would involve a double loop over both $\\mu$ and $\\nu$ from 0 to 3, resulting in $4 \\times 4 = 16$ terms for each component of $\\alpha$, which is computationally inefficient.\n",
    "\n",
    "However, we can significantly optimize this calculation by exploiting symmetry. The term $p^{\\mu} p^{\\nu}$ is symmetric with respect to the interchange of the indices $\\mu$ and $\\nu$. The Christoffel symbols $\\Gamma^{\\alpha}_{\\mu\\nu}$ are also symmetric in their lower two indices. Therefore, the full sum can be split into diagonal ($\\mu=\\nu$) and off-diagonal ($\\mu \\neq \\nu$) terms:\n",
    "$$ \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} =  \\Gamma^{\\alpha}_{\\mu\\mu} (p^{\\mu})^2 + \\sum_{\\mu \\neq \\nu} \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "The second sum over $\\mu \\neq \\nu$ contains pairs of identical terms (e.g., the $\\mu=1, \\nu=2$ term is the same as the $\\mu=2, \\nu=1$ term). We can combine all such pairs by summing over only one of the cases (e.g., $\\mu < \\nu$) and multiplying by two:\n",
    "$$ \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} =  \\Gamma^{\\alpha}_{\\mu\\mu} (p^{\\mu})^2 + 2 \\sum_{\\mu < \\nu} \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "The Python code implements this optimized version, ensuring that each component of the RHS is computed with the minimum number of floating point operations, leading to more efficient C code.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank1(dimension)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: Creates a symbolic rank-1 tensor (a Python list) of a specified dimension, with all elements initialized to the `sympy` integer 0. It is used here to create a container for the four components of the momentum RHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867e4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic_mom_rhs(Gamma4UDD):\n",
    "    \"\"\"\n",
    "    Symbolic RHS for momentum ODE: dp^a/dκ = -Γ^a_μν p^μ p^ν.\n",
    "    p is the reverse-momentum, y[4]...y[7].\n",
    "    \"\"\"\n",
    "    pt,pr,pth,pph = sp.symbols(\"y[4] y[5] y[6] y[7]\", Real=True)\n",
    "    pU = [pt,pr,pth,pph]\n",
    "    geodesic_rhs = ixp.zerorank1(dimension=4)\n",
    "    for alpha in range(4):\n",
    "        for mu in range(4):\n",
    "            geodesic_rhs[alpha] += Gamma4UDD[alpha][mu][mu] * pU[mu] * pU[mu]\n",
    "            for nu in range(mu + 1, 4):\n",
    "                geodesic_rhs[alpha] += 2 * Gamma4UDD[alpha][mu][nu] * pU[mu] * pU[nu]\n",
    "        geodesic_rhs[alpha] = -geodesic_rhs[alpha]\n",
    "    return geodesic_rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02fcc6",
   "metadata": {},
   "source": [
    "<a id='geodesic_pos_rhs'></a>\n",
    "### 3.d: Geodesic Position RHS\n",
    "\n",
    "This function defines the symbolic right-hand side (RHS) for the evolution of the position coordinates, $x^{\\alpha}$. As derived in the introduction, this is the first of our three first-order ODEs:\n",
    "\n",
    "$$ \\frac{dx^{\\alpha}}{d\\kappa} = p^{\\alpha} $$\n",
    "\n",
    "The Python function `geodesic_pos_rhs` is straightforward. It defines the components of the reverse-time momentum vector, `pU`, using `sympy`'s `sp.symbols()` function with the \"direct naming\" convention (`y[4]`, `y[5]`, etc.). It then simply returns a list containing these momentum components. This list of four symbolic expressions will serve as the first four components of the complete 9-component RHS vector that our C code will solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc97c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic_pos_rhs():\n",
    "    \"\"\"\n",
    "    Symbolic RHS for position ODE: dx^a/dκ = p^a.\n",
    "    p is the reverse-momentum, y[4]...y[7].\n",
    "    \"\"\"\n",
    "    pt,pr,pth,pph = sp.symbols(\"y[4] y[5] y[6] y[7]\", Real=True)\n",
    "    pU = [pt,pr,pth,pph]\n",
    "    return pU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976931d5",
   "metadata": {},
   "source": [
    "<a id='proper_len_rhs'></a>\n",
    "### 3.e: Proper Length ODE RHS\n",
    "\n",
    "This function defines the symbolic right-hand side for the evolution of the proper length, $L$. This is the final component of our ODE system and allows us to track the total distance the photon has traveled along its spatial path. The proper length element $dL$ is defined by the spatial part of the metric, $\\gamma_{ij} = g_{ij}$ for $i,j \\in \\{1,2,3\\}$:\n",
    "\n",
    "$$ dL^2 = \\gamma_{ij} dx^{i} dx^{j} $$\n",
    "\n",
    "Dividing by $d\\kappa^2$ and taking the square root gives us the rate of change of proper length with respect to our integration parameter $\\kappa$:\n",
    "\n",
    "$$ \\frac{dL}{d\\kappa} = \\sqrt{\\gamma_{ij} \\frac{dx^{i}}{d\\kappa} \\frac{dx^{j}}{d\\kappa}} = \\sqrt{\\gamma_{ij} p^{i} p^{j}} $$\n",
    "\n",
    "The function `proper_lengh_rhs` symbolically implements the formula under the square root, $\\sqrt{\\gamma_{ij} p^{i} p^{j}}$. It uses `sympy` symbols for the spatial momentum components (`pU[1]`, `pU[2]`, `pU[3]`) and programmatically constructs the optimized sum $\\gamma_{ij} p^{i} p^{j}$ using the same symmetry trick as the momentum RHS to reduce the number of terms. Finally, it returns a single-element list containing the square root of this sum. This will be the 9th component (`rhs_out[8]`) of our ODE system.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank2(name, dimension, sym)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: This function creates an *abstract* symbolic rank-2 tensor. Instead of creating symbols like `g11`, `g12`, etc., it creates symbols whose names are literally `name[1][1]`, `name[1][2]`, etc. This is a powerful technique for creating generic symbolic \"recipes\" that are later filled in with runtime data from a C struct. Here, it creates a placeholder for the metric components, `metric->g`, which will be provided by a C struct at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f82ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_lengh_rhs():\n",
    "    p0,p1,p2,p3,L= sp.symbols(\"y[4] y[5] y[6] y[7] y[8]\",Real=True)\n",
    "    pU=[p0,p1,p2,p3] \n",
    "\n",
    "    g4DD=ixp.declarerank2(\"metric->g\",dimension=4, sym=\"sym01\")\n",
    "\n",
    "    sum = sp.simplify(0)\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        sum += g4DD[i][i]*pU[i]*pU[i]\n",
    "\n",
    "        for j in range(i+1,4):\n",
    "            sum += 2*g4DD[i][j]*pU[i]*pU[j]\n",
    "\n",
    "    sp.simplify(sum)\n",
    "\n",
    "    return [sp.sqrt(sum)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fc9e6",
   "metadata": {},
   "source": [
    "<a id='geodesic_mom0_calc'></a>\n",
    "### 3.f: Symbolic Calculation of p⁰\n",
    "\n",
    "To complete our initial data, we must enforce the **null geodesic condition**, which states that the squared 4-momentum of a photon is zero. This is because photons travel along null paths where the spacetime interval $ds^2$ is zero. This condition must be satisfied by the 4-momentum of any photon. Let's write this for the **forward-in-time** photon, with physical 4-momentum $q^\\alpha$:\n",
    "\n",
    "$$ g_{\\mu\\nu}q^\\mu q^\\nu = 0 $$\n",
    "\n",
    "Expanding this equation into its time and space components gives us the quadratic equation for the time-component of the physical momentum, $q^0$:\n",
    "\n",
    "$$ g_{00}(q^0)^2 + 2\\left( g_{0i}q^i\\right)q^0 + \\left( g_{ij}q^i q^j\\right) = 0 $$\n",
    "\n",
    "For our reverse ray-tracing, we use the **reverse-time momentum**, $p^\\alpha$, which is related to the physical momentum by $p^\\alpha = -q^\\alpha$. We can substitute this relationship directly into the equation above, replacing $q^0$ with $-p^0$ and $q^i$ with $-p^i$:\n",
    "\n",
    "$$ g_{00}(-p^0)^2 + 2g_{0i}(-p^i)(-p^0) + \\left(g_{ij}(-p^i)(-p^j)\\right) = 0 $$\n",
    "\n",
    "The negative signs in the squared terms and the cross-term cancel out: `(-p^0)^2 = (p^0)^2`, `(-p^i)(-p^j) = p^i p^j`, and `(-p^i)(-p^0) = p^i p^0`. This yields a quadratic equation for $p^0$ that has the exact same form as the one for $q^0$:\n",
    "\n",
    "$$ g_{00}(p^0)^2 + 2\\left( g_{0i}p^i\\right)p^0 + \\left(g_{ij}p^i p^j\\right) = 0 $$\n",
    "\n",
    "We now solve this equation for $p^0$. It is a standard quadratic equation of the form $ax^2 + bx + c = 0$, where $x = p^0$. The coefficients are:\n",
    "*   $a = g_{00}$\n",
    "*   $b = 2g_{0i}p^i$\n",
    "*   $c =  g_{ij}p^i p^j$\n",
    "\n",
    "The solution for $p^0$ is given by the [quadratic formula](https://en.wikipedia.org/wiki/Quadratic_formula):\n",
    "\n",
    "$$ p^0 = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-2\\left(g_{0i}p^i\\right) \\pm \\sqrt{\\left(2 g_{0i}p^i\\right)^2 - 4g_{00}\\left(g_{ij}p^i p^j\\right)}}{2g_{00}} $$\n",
    "\n",
    "Simplifying by dividing the numerator and denominator by 2 gives:\n",
    "\n",
    "$$ p^0 = \\frac{-\\left( g_{0i}p^i\\right) \\pm \\sqrt{\\left( g_{0i}p^i\\right)^2 - g_{00}\\left( g_{ij}p^i p^j\\right)}}{g_{00}} $$\n",
    "\n",
    "The final step is to choose the physically correct root. For the reverse-traced photon, the parameter $\\kappa$ increases as coordinate time `t` decreases. Therefore, the derivative $p^0 = dt/d\\kappa$ must be **negative**. In a typical stationary spacetime outside a black hole, $g_{00}$ is negative. For the fraction to be negative, the numerator must be **positive**. The square root term is always positive and its magnitude is generally larger than the first term. To guarantee a positive numerator, we must choose the **plus sign (`+`)** in the `±`.\n",
    "\n",
    "This leads to the final, correct result implemented in the code:\n",
    "$$ p^0 = \\frac{-\\left( g_{0i}p^i\\right) + \\sqrt{\\left(g_{0i}p^i\\right)^2 - g_{00}\\left( g_{ij}p^i p^j\\right)}}{g_{00}} $$\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank2(name, dimension, sym)`**: Previously introduced. Used here to create an abstract symbolic tensor for the metric components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2b0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mom_time_p0_reverse():\n",
    "    \"\"\"\n",
    "    Solves g_μν p^μ p^ν = 0 for our reverse-time momentum p^0.\n",
    "    \"\"\"\n",
    "    p0,p1,p2,p3 = sp.symbols(\"y[4] y[5] y[6] y[7]\", Real=True)\n",
    "    pU=[p0,p1,p2,p3]\n",
    "    g4DD = ixp.declarerank2(\"g\", sym=\"sym01\", dimension=4)\n",
    "    sum_g0i_pi = sp.sympify(0)\n",
    "    for i in range(1,4):\n",
    "        sum_g0i_pi += g4DD[0][i]*pU[i]\n",
    "    sum_gij_pi_pj = sp.sympify(0)\n",
    "    for i in range(1,4):\n",
    "        sum_gij_pi_pj += g4DD[i][i]*pU[i]*pU[i]\n",
    "        for j in range(i+1,4):\n",
    "            sum_gij_pi_pj += 2*g4DD[i][j]*pU[i]*pU[j]\n",
    "    discriminant = sum_g0i_pi*sum_g0i_pi - g4DD[0][0]*sum_gij_pi_pj\n",
    "    answer = (-sum_g0i_pi + sp.sqrt(discriminant)) / g4DD[0][0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9bc84",
   "metadata": {},
   "source": [
    "# Markdown for conserved Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373ef962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolic_energy():\n",
    "    \"\"\"\n",
    "    Computes the symbolic expression for conserved energy E = -p_t.\n",
    "    E = -g_{t,mu} p^mu\n",
    "    \"\"\"\n",
    "    # Define the 4-momentum components using the y[4]...y[7] convention\n",
    "    pt, px, py, pz = sp.symbols(\"y[4] y[5] y[6] y[7]\", real=True)\n",
    "    pU = [pt, px, py, pz]\n",
    "    \n",
    "    # Define an abstract metric tensor to be filled by a C struct at runtime\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "    \n",
    "    # Calculate p_t = g_{t,mu} p^mu\n",
    "    p_t = sp.sympify(0)\n",
    "    for mu in range(4):\n",
    "        p_t += g4DD[0][mu] * pU[mu]\n",
    "        \n",
    "    return -p_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52801f14",
   "metadata": {},
   "source": [
    "# Markdown for conserved L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd4d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolic_L_components_cart():\n",
    "    \"\"\"\n",
    "    Computes the symbolic expressions for the three components of angular momentum,\n",
    "    correctly accounting for the symmetry of the metric tensor.\n",
    "    \"\"\"\n",
    "    # Define coordinate and 4-momentum components\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    pt, px, py, pz = sp.symbols(\"y[4] y[5] y[6] y[7]\", real=True)\n",
    "    pU = [pt, px, py, pz]\n",
    "    \n",
    "    # Define an abstract metric tensor\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "    \n",
    "    # --- THIS IS THE CORE FIX ---\n",
    "    # Calculate covariant momentum components p_k = g_{k,mu} p^mu,\n",
    "    # correctly exploiting the metric's symmetry g_mu,nu = g_nu,mu.\n",
    "    p_down = ixp.zerorank1(dimension=4)\n",
    "    for k in range(1, 4): # We only need p_x, p_y, p_z for L_i\n",
    "        # Sum over mu\n",
    "        for mu in range(4):\n",
    "            # Use g4DD[k][mu] if k <= mu, otherwise use g4DD[mu][k]\n",
    "            if k <= mu:\n",
    "                p_down[k] += g4DD[k][mu] * pU[mu]\n",
    "            else: # k > mu\n",
    "                p_down[k] += g4DD[mu][k] * pU[mu]\n",
    "            \n",
    "    p_x, p_y, p_z = p_down[1], p_down[2], p_down[3]\n",
    "\n",
    "    # Calculate angular momentum components \n",
    "    L_x = y*p_z - z*p_y\n",
    "    L_y = z*p_x - x*p_z\n",
    "    L_z = x*p_y - y*p_x\n",
    "    \n",
    "    return [L_x, L_y, L_z]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b68249",
   "metadata": {},
   "source": [
    "# Markdown for Carter Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1034fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final symbolic recipes for conserved quantities defined (Carter Constant re-derived).\n"
     ]
    }
   ],
   "source": [
    "# In file: V10_Python_to_C_via_NRPy.ipynb\n",
    "# In the \"Symbolic Recipes\" cell (Final, Corrected symbolic_carter_constant_Q_final)\n",
    "\n",
    "# symbolic_energy() and symbolic_L_components_cart() remain correct.\n",
    "\n",
    "def symbolic_carter_constant_Q():\n",
    "    \"\"\"\n",
    "    Computes the symbolic expression for the Carter Constant Q using a\n",
    "    verified formula, robustly handling the axial singularity.\n",
    "    \"\"\"\n",
    "    # Define all necessary symbolic variables\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    pt, px, py, pz = sp.symbols(\"y[4] y[5] y[6] y[7]\", real=True)\n",
    "    pU = [pt, px, py, pz]\n",
    "    a = a_spin\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "\n",
    "    # --- Step 1: Compute intermediate quantities E, Lz, and p_i ---\n",
    "    E = symbolic_energy()\n",
    "    _, _, Lz = symbolic_L_components_cart()\n",
    "    \n",
    "    p_down = ixp.zerorank1(dimension=4)\n",
    "    for k in range(1, 4):\n",
    "        for mu in range(4):\n",
    "            if k <= mu: p_down[k] += g4DD[k][mu] * pU[mu]\n",
    "            else: p_down[k] += g4DD[mu][k] * pU[mu]\n",
    "    p_x, p_y, p_z = p_down[1], p_down[2], p_down[3]\n",
    "\n",
    "    # --- Step 2: Compute geometric terms ---\n",
    "    r_sq = x**2 + y**2 + z**2\n",
    "    rho_sq = x**2 + y**2\n",
    "    \n",
    "    # --- Step 3: Compute p_theta^2 directly in Cartesian components ---\n",
    "    # This avoids square roots and potential complex number issues in sympy.\n",
    "    # p_theta^2 = r^2 * p_z^2 + cot^2(theta) * (x*p_x + y*p_y)^2 - 2*r*p_z*cot(theta)*(x*p_x+y*p_y)\n",
    "    # where cot(theta) = z / rho\n",
    "    \n",
    "    # This term is (x*p_x + y*p_y)\n",
    "    xpx_plus_ypy = x*p_x + y*p_y\n",
    "    \n",
    "    # This is p_theta^2, constructed to avoid dividing by rho before squaring.\n",
    "    # It is equivalent to (z*xpx_plus_ypy/rho - rho*p_z)^2\n",
    "    p_theta_sq = (z**2 * xpx_plus_ypy**2 / rho_sq) - (2 * z * p_z * xpx_plus_ypy) + (rho_sq * p_z**2)\n",
    "\n",
    "    # --- Step 4: Assemble the final formula for Q ---\n",
    "    # Q = p_theta^2 + cos^2(theta) * (-a^2*E^2 + L_z^2/sin^2(theta))\n",
    "    # where cos^2(theta) = z^2/r^2 and sin^2(theta) = rho^2/r^2\n",
    "    \n",
    "    # This is the second term in the Q formula\n",
    "    second_term = (z**2 / r_sq) * (-a**2 * E**2 + Lz**2 * (r_sq / rho_sq))\n",
    "    \n",
    "    Q_formula = p_theta_sq + second_term\n",
    "    \n",
    "    # --- Step 5: Handle the axial singularity ---\n",
    "    # For motion on the z-axis (rho_sq -> 0), Lz=0 and p_theta=0, so Q=0.\n",
    "    Q_final = sp.Piecewise(\n",
    "        (0, rho_sq < 1e-12),\n",
    "        (Q_formula, True)\n",
    "    )\n",
    "    \n",
    "    return Q_final\n",
    "\n",
    "print(\"Final symbolic recipes for conserved quantities defined (Carter Constant re-derived).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe10d0a",
   "metadata": {},
   "source": [
    "<a id='spacetime_definition'></a>\n",
    "# Step 4: Spacetime Definition in Kerr-Schild Coordinates\n",
    "\n",
    "This section defines the specific spacetime geometry in which the geodesics will be integrated. Instead of defining separate metrics for Schwarzschild (non-rotating) and Kerr (rotating) black holes, we use a single, powerful coordinate system: **Cartesian Kerr-Schild coordinates**. This system has a major advantage over more common coordinate systems like Boyer-Lindquist: it is regular everywhere, including at the event horizon. This means the metric components and their derivatives do not diverge, allowing the numerical integrator to trace a photon's path seamlessly across the horizon without encountering coordinate singularities.\n",
    "\n",
    "The Kerr-Schild metric $g_{\\mu\\nu}$ is constructed by adding a correction term to the flat Minkowski metric $\\eta_{\\mu\\nu}$:\n",
    "$$ g_{\\mu\\nu} = \\eta_{\\mu\\nu} + 2H l_\\mu l_\\nu $$\n",
    "where $\\eta_{\\mu\\nu}$ is the Minkowski metric `diag(-1, 1, 1, 1)`, $l_\\mu$ is a special null vector, and $H$ is a scalar function that depends on the black hole's mass $M$ and spin $a$.\n",
    "\n",
    "The function `define_kerr_metric_Cartesian_Kerr_Schild()` implements this formula symbolically. It defines the coordinates `(t, x, y, z)`, the mass `M`, and the spin `a` as `sympy` symbols. It then constructs the components of the null vector $l_\\mu$ and the scalar function $H$. Finally, it assembles the full metric tensor $g_{\\mu\\nu}$.\n",
    "\n",
    "A key feature of this formulation is that if the spin parameter `a` is set to zero, the metric automatically and exactly reduces to the Schwarzschild metric in Cartesian coordinates. This allows a single set of symbolic expressions and a single set of C functions to handle both spacetimes, with the specific behavior controlled by the runtime value of the `a_spin` parameter.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank1(dimension)`**: Previously introduced. Used to initialize the null vector $l_\\mu$.\n",
    "*   **`nrpy.indexedexp.zerorank2(dimension)`**: Previously introduced. Used to initialize the metric tensor $g_{\\mu\\nu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6282a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_kerr_metric_Cartesian_Kerr_Schild():\n",
    "    \"\"\"\n",
    "    Defines the Kerr metric tensor in Cartesian Kerr-Schild coordinates.\n",
    "\n",
    "    This function is the new, unified source for both Kerr (a != 0) and\n",
    "    Schwarzschild (a = 0) spacetimes. The coordinates are (t, x, y, z).\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (g4DD, xx), where g4DD is the symbolic 4x4 metric tensor\n",
    "        and xx is the list of symbolic coordinate variables.\n",
    "    \"\"\"\n",
    "    # Define the symbolic coordinates using the 'y[i]' convention for the integrator\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    xx = [t, x, y, z]\n",
    "\n",
    "    # Access the symbolic versions of the mass and spin parameters\n",
    "    M = M_scale\n",
    "    a = a_spin\n",
    "\n",
    "    # Define intermediate quantities\n",
    "    r2 = x**2 + y**2 + z**2\n",
    "    r = sp.sqrt(r2)\n",
    "    \n",
    "    # Define the Kerr-Schild null vector l_μ\n",
    "    l_down = ixp.zerorank1(dimension=4)\n",
    "    l_down[0] = 1\n",
    "    l_down[1] = (r*x + a*y) / (r2 + a**2)\n",
    "    l_down[2] = (r*y - a*x) / (r2 + a**2)\n",
    "    l_down[3] = z/r\n",
    "\n",
    "    # Define the scalar function H\n",
    "    H = (M * r**3) / (r**4 + a**2 * z**2)\n",
    "\n",
    "    # The Kerr-Schild metric is g_μν = η_μν + 2H * l_μ * l_ν\n",
    "    # where η_μν is the Minkowski metric diag(-1, 1, 1, 1)\n",
    "    g4DD = ixp.zerorank2(dimension=4)\n",
    "    for mu in range(4):\n",
    "        for nu in range(4):\n",
    "            eta_mu_nu = 0\n",
    "            if mu == nu:\n",
    "                eta_mu_nu = 1\n",
    "            if mu == 0 and nu == 0:\n",
    "                eta_mu_nu = -1\n",
    "            \n",
    "            g4DD[mu][nu] = eta_mu_nu + 2 * H * l_down[mu] * l_down[nu]\n",
    "            \n",
    "    return g4DD, xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234e04d",
   "metadata": {},
   "source": [
    "# Markdown for Schwarzschild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f01f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In the NEW CELL after define_kerr_metric_Cartesian_Kerr_Schild\n",
    "\n",
    "def define_schwarzschild_metric_cartesian():\n",
    "    \"\"\"\n",
    "    Defines the Schwarzschild metric tensor directly in Cartesian coordinates.\n",
    "    \n",
    "    This version uses the standard textbook formula and ensures all components\n",
    "    are sympy objects to prevent C-generation errors.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (g4DD, xx), where g4DD is the symbolic 4x4 metric tensor\n",
    "        and xx is the list of symbolic coordinate variables.\n",
    "    \"\"\"\n",
    "    # Define Cartesian coordinates\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    xx = [t, x, y, z]\n",
    "\n",
    "    # Access the symbolic mass parameter\n",
    "    M = M_scale\n",
    "\n",
    "    # Define r in terms of Cartesian coordinates\n",
    "    r = sp.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "    # Define the Cartesian Schwarzschild metric components directly\n",
    "    g4DD = ixp.zerorank2(dimension=4)\n",
    "    \n",
    "    # g_tt\n",
    "    g4DD[0][0] = -(1 - 2*M/r)\n",
    "    \n",
    "    # Spatial components g_ij = δ_ij + (2M/r) * (x_i * x_j / r^2)\n",
    "    x_i = [x, y, z]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # --- CORRECTED: Use sp.sympify() for the kronecker delta ---\n",
    "            delta_ij = sp.sympify(0)\n",
    "            if i == j:\n",
    "                delta_ij = sp.sympify(1)\n",
    "            \n",
    "            # The indices for g4DD are off by 1 from the spatial indices\n",
    "            g4DD[i+1][j+1] = delta_ij + (2*M/r) * (x_i[i] * x_i[j] / (r**2))\n",
    "\n",
    "    # --- CORRECTED: Ensure time-space components are sympy objects ---\n",
    "    g4DD[0][1] = g4DD[1][0] = sp.sympify(0)\n",
    "    g4DD[0][2] = g4DD[2][0] = sp.sympify(0)\n",
    "    g4DD[0][3] = g4DD[3][0] = sp.sympify(0)\n",
    "            \n",
    "    return g4DD, xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec78a4",
   "metadata": {},
   "source": [
    "<a id='symbolic_execution'></a>\n",
    "# Step 5: Symbolic Workflow Execution\n",
    "\n",
    "This cell acts as the central hub for the symbolic portion of our project. In the preceding cells, we *defined* a series of Python functions that perform individual mathematical tasks. Here, we *execute* those functions in the correct sequence to generate all the final symbolic expressions that will serve as \"recipes\" for our C code generators.\n",
    "\n",
    "This \"symbolic-first\" approach is a core `nrpy` principle and offers significant advantages:\n",
    "1.  **Efficiency**: The complex symbolic calculations, such as inverting the metric tensor and deriving the Christoffel symbols, are performed **only once** when this notebook is run. The results are stored in global Python variables, preventing redundant and time-consuming recalculations. This is especially important for the Kerr metric, whose Christoffel symbols can take several minutes to compute.\n",
    "2.  **Modularity**: This workflow creates a clean separation between the *specific solution* for a metric (e.g., the explicit formulas for the Kerr-Schild Christoffels) and the *generic form* of the equations of motion (which are valid for any metric).\n",
    "\n",
    "This cell produces two key sets of symbolic expressions that are stored in global variables for later use:\n",
    "*   **`Gamma4UDD_kerr`**: The explicit symbolic formulas for the Christoffel symbols of the unified Kerr-Schild metric.\n",
    "*   **`all_rhs_expressions`**: A Python list containing the 9 symbolic expressions for the right-hand-sides of our generic ODE system. To achieve this generality, we create a symbolic **placeholder** for the Christoffel symbols using `ixp.declarerank3(\"conn->Gamma4UDD\", ...)`. This placeholder is passed to `geodesic_mom_rhs()` to construct the geodesic equation in its abstract form. This elegant technique embeds the final C variable name (`conn->Gamma4UDD...`) directly into the symbolic expression, which dramatically simplifies the C code generation step for the `calculate_ode_rhs()` engine.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank3(name, dimension)`**: Previously introduced. Used here to create a symbolic placeholder for the Christoffel symbols that will be passed to the generic RHS engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbfe0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Computing Kerr-Schild metric and Christoffel symbols...\n",
      "    ... Done.\n",
      " -> Computing Standard Schwarzschild (Cartesian) metric and Christoffel symbols...\n",
      "    ... Done.\n",
      " -> Defined generic global symbolic variable for ODE RHS: all_rhs_expressions\n",
      " -> Generating symbolic recipes for conserved quantities...\n",
      "    ... Conservation recipes generated.\n",
      "Kerr expressions\n",
      "[-metric->g00*y[4] - metric->g01*y[5] - metric->g02*y[6] - metric->g03*y[7], y[2]*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) - y[3]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]), -y[1]*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) + y[3]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]), y[1]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]) - y[2]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]), Piecewise((0, y[1]**2 + y[2]**2 < 1.0e-12), (y[3]**2*(-a_spin**2*(-metric->g00*y[4] - metric->g01*y[5] - metric->g02*y[6] - metric->g03*y[7])**2 + (y[1]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]) - y[2]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]))**2*(y[1]**2 + y[2]**2 + y[3]**2)/(y[1]**2 + y[2]**2))/(y[1]**2 + y[2]**2 + y[3]**2) + y[3]**2*(y[1]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]) + y[2]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]))**2/(y[1]**2 + y[2]**2) - 2*y[3]*(y[1]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]) + y[2]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]))*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) + (y[1]**2 + y[2]**2)*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7])**2, True))]\n",
      "Schwarzs experessions\n",
      "[-metric->g00*y[4] - metric->g01*y[5] - metric->g02*y[6] - metric->g03*y[7], y[2]*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) - y[3]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]), -y[1]*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) + y[3]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]), y[1]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]) - y[2]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]), (y[1]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]) - y[2]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]))**2 + (-y[1]*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) + y[3]*(metric->g01*y[4] + metric->g11*y[5] + metric->g12*y[6] + metric->g13*y[7]))**2 + (y[2]*(metric->g03*y[4] + metric->g13*y[5] + metric->g23*y[6] + metric->g33*y[7]) - y[3]*(metric->g02*y[4] + metric->g12*y[5] + metric->g22*y[6] + metric->g23*y[7]))**2]\n",
      "\n",
      "Symbolic setup complete. All expressions are now available globally.\n"
     ]
    }
   ],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In cell [f9e18b56]\n",
    "\n",
    "# --- 1. Define the Kerr-Schild metric and get its derivatives ---\n",
    "print(\" -> Computing Kerr-Schild metric and Christoffel symbols...\")\n",
    "g4DD_kerr, xx_kerr = define_kerr_metric_Cartesian_Kerr_Schild()\n",
    "g4DD_dD_kerr = derivative_g4DD(g4DD_kerr, xx_kerr)\n",
    "Gamma4UDD_kerr = four_connections(g4DD_kerr, g4DD_dD_kerr)\n",
    "print(\"    ... Done.\")\n",
    "\n",
    "# --- 2. Define the Standard Schwarzschild metric in Cartesian and get its derivatives ---\n",
    "print(\" -> Computing Standard Schwarzschild (Cartesian) metric and Christoffel symbols...\")\n",
    "g4DD_schw_cart, xx_schw_cart = define_schwarzschild_metric_cartesian()\n",
    "g4DD_dD_schw_cart = derivative_g4DD(g4DD_schw_cart, xx_schw_cart)\n",
    "Gamma4UDD_schw_cart = four_connections(g4DD_schw_cart, g4DD_dD_schw_cart)\n",
    "print(\"    ... Done.\")\n",
    "\n",
    "# --- 3. Generate the GENERIC symbolic RHS expressions for the geodesic equations ---\n",
    "# This part is unchanged, as the ODEs are generic.\n",
    "Gamma4UDD_placeholder = ixp.declarerank3(\"conn->Gamma4UDD\", dimension=4)\n",
    "rhs_pos = geodesic_pos_rhs() \n",
    "rhs_mom = geodesic_mom_rhs(Gamma4UDD_placeholder)\n",
    "rhs_length = proper_lengh_rhs()\n",
    "all_rhs_expressions = rhs_pos + rhs_mom + rhs_length\n",
    "print(\" -> Defined generic global symbolic variable for ODE RHS: all_rhs_expressions\")\n",
    "\n",
    "# --- 4. Generate symbolic recipes for conserved quantities ---\n",
    "# This is now simplified, as all calculations are Cartesian.\n",
    "print(\" -> Generating symbolic recipes for conserved quantities...\")\n",
    "\n",
    "\n",
    "E_expr = symbolic_energy()\n",
    "Lx_expr, Ly_expr, Lz_expr = symbolic_L_components_cart()\n",
    "Q_expr_kerr = symbolic_carter_constant_Q()\n",
    "Q_expr_schw = Lx_expr**2 + Ly_expr**2 + Lz_expr**2\n",
    "\n",
    "# We now have two lists of expressions, both using Cartesian formulas.\n",
    "list_of_expressions_kerr = [E_expr, Lx_expr, Ly_expr, Lz_expr, Q_expr_kerr]\n",
    "list_of_expressions_schw = [E_expr, Lx_expr, Ly_expr, Lz_expr, Q_expr_schw]\n",
    "print(\"    ... Conservation recipes generated.\")\n",
    "print(\"Kerr expressions\")\n",
    "print(list_of_expressions_kerr)\n",
    "print(\"Schwarzs experessions\")\n",
    "print(list_of_expressions_schw)\n",
    "print(\"\\nSymbolic setup complete. All expressions are now available globally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032d263",
   "metadata": {},
   "source": [
    "<a id='generate_c_engines'></a>\n",
    "# Step 6: C Code Generation - Physics \"Engines\" and \"Workers\"\n",
    "\n",
    "This section marks our transition from pure symbolic mathematics to C code generation. The Python functions defined here are \"meta-functions\": their job is not to perform calculations themselves, but to **generate the C code** that will perform the calculations in the final compiled program.\n",
    "\n",
    "We distinguish between two types of generated functions:\n",
    "*   **Workers**: These are specialized functions that implement the physics for a *specific metric*. For example, `con_kerr_schild()` is a worker that only knows how to compute Christoffel symbols for the Kerr-Schild metric.\n",
    "*   **Engines**: These are generic functions that implement physics equations valid for *any metric*. For example, `calculate_ode_rhs()` is an engine that can compute the geodesic equations for any metric, as long as the Christoffel symbols are provided to it.\n",
    "\n",
    "This design pattern allows for maximum code reuse and extensibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c6765",
   "metadata": {},
   "source": [
    "# Schwarzschild Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29ef433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g4DD_schwarzschild_cartesian():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Schwarzschild\n",
    "    metric components in standard Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: g4DD_schwarzschild_cartesian()...\")\n",
    "    \n",
    "    # Use the globally defined g4DD_schw_cart from the symbolic execution step\n",
    "    list_of_g4DD_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_syms.append(g4DD_schw_cart[i][j])\n",
    "\n",
    "    list_of_g4DD_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_C_vars.append(f\"metric->g{i}{j}\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the 10 unique components of the Schwarzschild metric in Cartesian coords.\"\"\"\n",
    "    name = \"g4DD_schwarzschild_cartesian\"\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], metric_struct *restrict metric\"\n",
    "   \n",
    "    body = ccg.c_codegen(list_of_g4DD_syms, list_of_g4DD_C_vars, enable_cse=True)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... g4DD_schwarzschild_cartesian() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd610ccf",
   "metadata": {},
   "source": [
    "<a id='g4DD_kerr_schild_engine'></a>\n",
    "### 6.a: `g4DD_kerr_schild()` Worker\n",
    "\n",
    "This Python function generates the C **worker** function `g4DD_kerr_schild()`, whose only job is to compute the 10 unique components of the Kerr-Schild metric tensor, $g_{\\mu\\nu}$, at a given point in spacetime.\n",
    "\n",
    "The generation process is as follows:\n",
    "1.  **Access Symbolic Recipe:** It accesses the global `g4DD_kerr` variable, which holds the symbolic `sympy` expression for the Kerr-Schild metric tensor, generated in Step 5.\n",
    "2.  **Define C Assignment:** It creates two Python lists: one containing the 10 unique symbolic metric expressions (`list_of_g4DD_syms`) and another containing the corresponding C variable names for the members of the `metric_struct` (e.g., `metric->g00`, `metric->g01`, etc.) in `list_of_g4DD_C_vars`.\n",
    "3.  **Generate C Code:** It passes these two lists to `nrpy.c_codegen.c_codegen`. This powerful `nrpy` function converts the symbolic math into highly optimized C code, including performing Common Subexpression Elimination (CSE).\n",
    "4.  **Register C Function:** Finally, it bundles the generated C code with its metadata (description, parameters, etc.) and registers the complete function with `nrpy.c_function.register_CFunction`. Crucially, it sets `include_CodeParameters_h=True` to automatically handle access to both the `M_scale` and `a_spin` parameters via the \"Triple-Lock\" system.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_codegen.c_codegen(sympy_expressions, C_variable_names, **kwargs)`**:\n",
    "    *   **Source File**: `nrpy/c_codegen.py`\n",
    "    *   **Description**: The core symbolic-to-C translation engine. It takes a list of `sympy` expressions and a corresponding list of C variable names and generates optimized C code to perform the assignments.\n",
    "    *   **Key Inputs**:\n",
    "        *   `sympy_expressions`: A Python list of symbolic expressions to be converted to C code.\n",
    "        *   `C_variable_names`: A Python list of strings for the C variables that will store the results.\n",
    "    *   **Key Keyword Arguments (`kwargs`)**:\n",
    "        *   `enable_cse=True`: Enables Common Subexpression Elimination, which finds repeated mathematical operations, assigns them to temporary variables, and reuses those variables to reduce redundant calculations. This is essential for performance.\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(name, params, body, **kwargs)`**:\n",
    "    *   **Source File**: `nrpy/c_function.py`\n",
    "    *   **Description**: This is the workhorse for defining a C function. It takes all necessary metadata and stores it in a global dictionary, `cfc.CFunction_dict`. The final build system uses this dictionary to write all the `.c` source files.\n",
    "    *   **Key Inputs**:\n",
    "        *   `name`: The name of the C function.\n",
    "        *   `params`: A string defining the function's parameters (e.g., `\"const double y[4], ...\"`).\n",
    "        *   `body`: A string containing the C code for the function's body.\n",
    "    *   **Key Keyword Arguments (`kwargs`)**:\n",
    "        *   `include_CodeParameters_h=True`: Enables the \"Triple-Lock\" system for this function, automatically including `set_CodeParameters.h` at the top of the function body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78d52f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g4DD_kerr_schild():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Kerr-Schild\n",
    "    metric components in Cartesian coordinates. This is the new unified worker.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: g4DD_kerr_schild()...\")\n",
    "    \n",
    "    # We use the globally defined g4DD_kerr from the symbolic execution step\n",
    "    list_of_g4DD_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_syms.append(g4DD_kerr[i][j])\n",
    "\n",
    "    list_of_g4DD_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_C_vars.append(f\"metric->g{i}{j}\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the 10 unique components of the Kerr metric in Cartesian Kerr-Schild coords.\"\"\"\n",
    "    name = \"g4DD_kerr_schild\"\n",
    "    # The state vector y now contains (t, x, y, z)\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], metric_struct *restrict metric\"\n",
    "   \n",
    "    body = ccg.c_codegen(list_of_g4DD_syms, list_of_g4DD_C_vars, enable_cse=True)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... g4DD_kerr_schild() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f1222",
   "metadata": {},
   "source": [
    "<a id='con_kerr_schild_engine'></a>\n",
    "### 6.b: `con_kerr_schild()` Worker\n",
    "\n",
    "This function is structured identically to the `g4DD_kerr_schild` worker. It generates the C **worker** function `con_kerr_schild()`, whose only job is to compute the 40 unique Christoffel symbols for the unified Kerr-Schild metric.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Access Symbolic Recipe:** It accesses the pre-computed symbolic Christoffel formulas from the global `Gamma4UDD_kerr` variable, which was generated in Step 5.\n",
    "2.  **Define C Assignment:** It creates a list of the 40 unique symbolic expressions and a corresponding list of the C variable names for the members of the `connection_struct` (e.g., `conn->Gamma4UDD012`).\n",
    "3.  **Generate C Code:** It uses `nrpy.c_codegen.c_codegen` to convert these highly complex symbolic expressions into optimized C code. The Common Subexpression Elimination (CSE) performed by `c_codegen` is absolutely essential here, as it reduces what would be thousands of floating-point operations into a much more manageable and efficient set of calculations.\n",
    "4.  **Register C Function:** Like the other workers, it registers the function using `nrpy.c_function.register_CFunction` and sets `include_CodeParameters_h=True` to handle its dependency on both `M_scale` and `a_spin`.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank3(name, dimension)`**: Previously introduced. Used here to programmatically generate the C variable names for the Christoffel symbols that will be stored in the `connection_struct`.\n",
    "*   **`nrpy.c_codegen.c_codegen(...)`**: Previously introduced.\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb471c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_kerr_schild():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Kerr-Schild Christoffel symbols.\n",
    "    This is the new unified worker.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: con_kerr_schild()...\")\n",
    "    \n",
    "    # We use the globally defined Gamma4UDD_kerr from the symbolic execution step\n",
    "    list_of_Gamma_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_syms.append(Gamma4UDD_kerr[i][j][k])\n",
    "\n",
    "    conn_Gamma4UDD = ixp.declarerank3(\"conn->Gamma4UDD\", dimension=4)\n",
    "    list_of_Gamma_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_C_vars.append(str(conn_Gamma4UDD[i][j][k]))\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the 40 unique Christoffel symbols for the Kerr metric in Kerr-Schild coords.\"\"\"\n",
    "    name = \"con_kerr_schild\"\n",
    "    # The state vector y now contains (t, x, y, z)\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], connection_struct *restrict conn\"\n",
    "\n",
    "    body = ccg.c_codegen(list_of_Gamma_syms, list_of_Gamma_C_vars, enable_cse=True)\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... con_kerr_schild() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166625f",
   "metadata": {},
   "source": [
    "# Con Schwarzschild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d6072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_schwarzschild_cartesian():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Schwarzschild Christoffel symbols\n",
    "    in standard Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: con_schwarzschild_cartesian()...\")\n",
    "    \n",
    "    # Use the globally defined Gamma4UDD_schw_cart\n",
    "    list_of_Gamma_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_syms.append(Gamma4UDD_schw_cart[i][j][k])\n",
    "\n",
    "    conn_Gamma4UDD = ixp.declarerank3(\"conn->Gamma4UDD\", dimension=4)\n",
    "    list_of_Gamma_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_C_vars.append(str(conn_Gamma4UDD[i][j][k]))\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the unique Christoffel symbols for the Schwarzschild metric in Cartesian coords.\"\"\"\n",
    "    name = \"con_schwarzschild_cartesian\"\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], connection_struct *restrict conn\"\n",
    "\n",
    "    body = ccg.c_codegen(list_of_Gamma_syms, list_of_Gamma_C_vars, enable_cse=True)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... con_schwarzschild_cartesian() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce3df4",
   "metadata": {},
   "source": [
    "<a id='calculate_p0_engine'></a>\n",
    "### 6.c: `calculate_p0_reverse()` Engine\n",
    "\n",
    "This Python function generates the C **engine** `calculate_p0_reverse()`, which implements the general formula for the time-component of the reverse-time momentum, $p^0$, derived from the null geodesic condition $g_{\\mu\\nu} p^\\mu p^\\nu = 0$. This function is a prime example of a reusable component, as its logic is valid for any metric for which the components $g_{\\mu\\nu}$ are known.\n",
    "\n",
    "The code generation follows a pattern that is both robust and highly automated, showcasing a powerful `nrpy` technique called the **Preamble Pattern**:\n",
    "\n",
    "1.  **Symbolic Recipe:** It calls our pure-math `mom_time_p0_reverse()` function (from Step 3.f) to get the complete symbolic expression for $p^0$. This expression is built from abstract `sympy` symbols (e.g., `g00`, `g01`, etc.).\n",
    "2.  **Preamble Generation:** The function programmatically generates a C code \"preamble.\" This preamble consists of a series of `const double` declarations that unpack the numerical values from the input `metric_struct` pointer and assign them to local C variables that have the *exact same names* as our abstract `sympy` symbols (e.g., `const double g00 = metric->g00;`).\n",
    "3.  **C Code Generation:** It calls `nrpy.c_codegen.c_codegen` to convert the symbolic `p0_expr` into an optimized C expression, assigning it to a temporary variable `p0_val`. This works seamlessly because the symbols in the expression (`g00`, etc.) now match the local C variables created by the preamble. This avoids the need for brittle string substitutions.\n",
    "4.  **Return Value:** The final C function body is constructed by combining the preamble, the CSE-optimized calculation, and a `return p0_val;` statement. This creates a complete, efficient, and readable C function.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_codegen.c_codegen(...)`**: Previously introduced.\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bece041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p0_reverse():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the time component\n",
    "    of the reverse 4-momentum, p^0.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine function: calculate_p0_reverse()...\")\n",
    "    # The symbolic expression uses y[4] through y[7] for the 4-momentum\n",
    "    p0_expr = mom_time_p0_reverse()\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes reverse-time p^0 from the null condition g_munu p^mu p^nu = 0.\"\"\"\n",
    "    name = \"calculate_p0_reverse\"\n",
    "    c_type = \"double\"\n",
    "    # The function now takes the full 9-element state vector y.\n",
    "    params = \"const metric_struct *restrict metric, const double y[9]\"\n",
    "    \n",
    "    preamble = \"\"\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            preamble += f\"const double g{i}{j} = metric->g{i}{j};\\n\"\n",
    "            \n",
    "    # We generate the C code directly from the original expression.\n",
    "    # Since the C function takes the full y[9] vector, the array indices\n",
    "    # y[4], y[5], etc., in the generated code will be correct.\n",
    "    p0_C_code_lines = ccg.c_codegen(\n",
    "        p0_expr, 'double p0_val', enable_cse=True, include_braces=False\n",
    "    )\n",
    "    body = f\"{{\\n{preamble}\\n{p0_C_code_lines}\\nreturn p0_val;\\n}}\"\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, cfunc_type=c_type,\n",
    "        name=name, params=params, body=body\n",
    "    )\n",
    "    print(\"    ... calculate_p0_reverse() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e27c2c",
   "metadata": {},
   "source": [
    "<a id='calculate_ode_rhs_engine'></a>\n",
    "### 6.d: `calculate_ode_rhs()` Engine\n",
    "\n",
    "This function generates the core \"engine\" of our ODE solver: the C function `calculate_ode_rhs()`. Its single responsibility is to calculate the right-hand sides for our entire system of 9 ODEs. It is completely generic and has no knowledge of any specific metric; it only knows how to compute the geodesic equations given a set of Christoffel symbols and the spatial metric components.\n",
    "\n",
    "The generation process is straightforward:\n",
    "1.  **Access Generic Recipe:** It accesses the global `all_rhs_expressions` list. This list contains the generic symbolic form of the ODEs for position, momentum, and proper length that we derived in Step 5.\n",
    "2.  **Generate C Code:** It passes this list directly to `nrpy.c_codegen.c_codegen`. The symbols used to build `all_rhs_expressions` were already created with their final C syntax (e.g., `y[5]` for the momentum, `conn->Gamma4UDD...` for the Christoffel placeholder, and `metric->g...` for the metric placeholder). Therefore, no further symbolic manipulation is needed. `nrpy` simply translates the expressions into optimized C code.\n",
    "3.  **Register C Function:** The generated C code body is bundled with its metadata and registered. This function does not require the `include_CodeParameters_h` flag because it is physically generic and receives all necessary information through its arguments.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_codegen.c_codegen(...)`**: Previously introduced.\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b67d389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ode_rhs():\n",
    "\n",
    "    rhs_output_vars = [f\"rhs_out[{i}]\" for i in range(9)]\n",
    "\n",
    "\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "\n",
    "    desc = r\"\"\"@brief Calculates the right-hand sides (RHS) of the 9 geodesic ODEs.\n",
    " \n",
    "    This function implements the generic geodesic equation using pre-computed\n",
    "    Christoffel symbols. It is a pure \"engine\" function that does not depend\n",
    "    on any specific metric's parameters (like M_scale), only on the geometric\n",
    "    values passed to it via the connection struct.\n",
    "\n",
    "    @param[in]  y         The 9-component state vector [t, r, th, ph, p^t, p^r, p^th, p^ph, L].\n",
    "    @param[in]  conn      A pointer to the connection_struct holding the pre-computed Christoffel symbols.\n",
    "    @param[out] rhs_out   A pointer to the 9-component output array where the RHS results are stored.\"\"\"\n",
    "            \n",
    "    name = \"calculate_ode_rhs\"\n",
    "    params = \"const double y[9], const metric_struct *restrict metric, const connection_struct *restrict conn, double rhs_out[9]\"\n",
    "\n",
    "    body=ccg.c_codegen(all_rhs_expressions,rhs_output_vars)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes= includes,\n",
    "        name=name,\n",
    "        desc=desc,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d87383",
   "metadata": {},
   "source": [
    "<a id='lagrange_interp_engine'></a>\n",
    "### 6.e: `find_event_time_and_state()` Interpolation Engine\n",
    "\n",
    "This Python function generates a crucial C **engine** called `find_event_time_and_state()`. Its purpose is to find the precise time and state vector of a \"plane-crossing\" event with high accuracy, using data from three consecutive steps of the ODE integrator. This is essential for accurately mapping where a ray hits the window and source planes.\n",
    "\n",
    "The function implements a robust interpolation scheme:\n",
    "1.  **Quadratic Root Finding:** It treats the event condition (e.g., the distance to a plane, `f(y) = n_i x^i - d = 0`) as a function of the affine parameter, `f(κ)`. Given three points `(κ_prev, f_prev)`, `(κ_curr, f_curr)`, and `(κ_next, f_next)` that are known to bracket a root (i.e., the function changes sign), it fits a quadratic polynomial to these points. It then uses a numerically stable formula (similar to Muller's method) to find the root `κ_event` of this polynomial. This gives a much more accurate time for the plane crossing than simply taking the time of the closest step.\n",
    "2.  **Lagrange Polynomial Interpolation:** Once the precise event time `κ_event` is known, the function uses second-order [Lagrange basis polynomials](https://en.wikipedia.org/wiki/Lagrange_polynomial) to interpolate each of the 9 components of the state vector `y` to that exact time.\n",
    "\n",
    "This two-step process provides a highly accurate snapshot of the photon's state `y_event` at the exact moment it crosses a plane of interest. The C function body is written manually as a string, as its logic is algorithmic rather than symbolic, and then registered with `nrpy`.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced. Used here to register the manually written C code for the interpolation engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b4572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagrange_interp_engine_generic():\n",
    "    \"\"\"\n",
    "    Generates the generic Lagrange interpolation engine.\n",
    "    \n",
    "    This definitive version is numerically robust. It checks for small denominators\n",
    "    and unstable conditions, falling back to stable linear interpolation to prevent\n",
    "    NaN results in edge cases.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: find_event_time_and_state() [Robust Version]...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"<math.h>\"]\n",
    "    desc = r\"\"\"@brief Finds the root of a generic event using a robust, second-order interpolation.\"\"\"\n",
    "    \n",
    "    name = \"find_event_time_and_state\"\n",
    "    params = r\"\"\"const double y_prev[9], const double y_curr[9], const double y_next[9],\n",
    "                double lambda_prev, double lambda_curr, double lambda_next,\n",
    "                event_function_t event_func, void *event_params,\n",
    "                event_data_struct *restrict result\"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    double t0 = lambda_prev, t1 = lambda_curr, t2 = lambda_next;\n",
    "    double f0 = event_func(y_prev, event_params);\n",
    "    double f1 = event_func(y_curr, event_params);\n",
    "    double f2 = event_func(y_next, event_params);\n",
    "\n",
    "    // --- Linear interpolation as a fallback ---\n",
    "    // This is used if quadratic interpolation is unstable or fails.\n",
    "    // It finds the root between the two points where the sign change occurs.\n",
    "    double t_linear;\n",
    "    if (f0 * f1 < 0.0 && fabs(f1 - f0) > 1e-12) { // Sign change is between prev and curr\n",
    "        t_linear = (f1 * t0 - f0 * t1) / (f1 - f0);\n",
    "    } else if (f1 * f2 < 0.0 && fabs(f2 - f1) > 1e-12) { // Sign change is between curr and next\n",
    "        t_linear = (f2 * t1 - f1 * t2) / (f2 - f1);\n",
    "    } else {\n",
    "        // This can happen if f1 is exactly zero.\n",
    "        t_linear = t1;\n",
    "    }\n",
    "\n",
    "    // --- Quadratic interpolation (Muller's method variant) ---\n",
    "    double h0 = t1 - t0;\n",
    "    double h1 = t2 - t1;\n",
    "\n",
    "    // Check for degenerate intervals to prevent division by zero.\n",
    "    if (fabs(h0) < 1e-15 || fabs(h1) < 1e-15 || fabs(h0 + h1) < 1e-15) {\n",
    "        result->lambda_event = t_linear;\n",
    "    } else {\n",
    "        double delta0 = (f1 - f0) / h0;\n",
    "        double delta1 = (f2 - f1) / h1;\n",
    "        double a = (delta1 - delta0) / (h1 + h0);\n",
    "        double b = a * h1 + delta1;\n",
    "        double c = f2;\n",
    "        double discriminant = b*b - 4*a*c;\n",
    "\n",
    "        if (discriminant < 0.0 || fabs(a) < 1e-15) {\n",
    "            // Discriminant is negative or equation is effectively linear.\n",
    "            result->lambda_event = t_linear;\n",
    "        } else {\n",
    "            // Use the more stable form of the quadratic formula\n",
    "            double denom = (b >= 0.0) ? (b + sqrt(discriminant)) : (b - sqrt(discriminant));\n",
    "            if (fabs(denom) < 1e-15) {\n",
    "                result->lambda_event = t_linear;\n",
    "            } else {\n",
    "                double t_quad = t2 - (2.0 * c / denom);\n",
    "                // Only accept the quadratic result if it's within the bracketing interval.\n",
    "                if (t_quad > fmin(t0, t2) && t_quad < fmax(t0, t2)) {\n",
    "                    result->lambda_event = t_quad;\n",
    "                } else {\n",
    "                    result->lambda_event = t_linear;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // --- Perform final interpolation on the state vector using the found time ---\n",
    "    double t = result->lambda_event;\n",
    "    \n",
    "    // Check for degenerate intervals again before final interpolation\n",
    "    if (fabs(t0 - t1) < 1e-15 || fabs(t0 - t2) < 1e-15 || fabs(t1 - t2) < 1e-15) {\n",
    "        // Fallback to linear interpolation for the state vector as well\n",
    "        double frac = 0.5;\n",
    "        if (fabs(t2 - t1) > 1e-15) {\n",
    "            frac = (t - t1) / (t2 - t1);\n",
    "        }\n",
    "        for (int i = 0; i < 9; i++) {\n",
    "            result->y_event[i] = y_curr[i] + frac * (y_next[i] - y_curr[i]);\n",
    "        }\n",
    "    } else {\n",
    "        // Perform full quadratic interpolation\n",
    "        double L0 = ((t - t1) * (t - t2)) / ((t0 - t1) * (t0 - t2));\n",
    "        double L1 = ((t - t0) * (t - t2)) / ((t1 - t0) * (t1 - t2));\n",
    "        double L2 = ((t - t0) * (t - t1)) / ((t2 - t0) * (t2 - t1));\n",
    "        for (int i = 0; i < 9; i++) {\n",
    "            result->y_event[i] = y_prev[i] * L0 + y_curr[i] * L1 + y_next[i] * L2;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    result->t_event = result->y_event[0];\n",
    "    result->found = true;\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)\n",
    "    print(\"    ... Registered C engine: find_event_time_and_state (Robust Version).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624cd49",
   "metadata": {},
   "source": [
    "# Markdown for check_conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e4260e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conservation():\n",
    "    \"\"\"\n",
    "    Generates the C function `check_conservation`. This version is simplified\n",
    "    for a purely Cartesian pipeline.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: check_conservation() [Cartesian Version]...\")\n",
    "\n",
    "    # Use the globally defined Cartesian recipes\n",
    "    output_vars_kerr = [\"*E\", \"*Lx\", \"*Ly\", \"*Lz\", \"*Q\"]\n",
    "    output_vars_schw = [\"*E\", \"*Lx\", \"*Ly\", \"*Lz\", \"*Q\"] # Q is L^2\n",
    "\n",
    "    body_C_code_kerr = ccg.c_codegen(list_of_expressions_kerr, output_vars_kerr, enable_cse=True, include_braces=False)\n",
    "    body_C_code_schw = ccg.c_codegen(list_of_expressions_schw, output_vars_schw, enable_cse=True, include_braces=False)\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Computes conserved quantities (E, L_i, Q/L^2) for a given state vector.\"\"\"\n",
    "    name = \"check_conservation\"\n",
    "    params = \"\"\"const commondata_struct *restrict commondata,\n",
    "        const params_struct *restrict params,\n",
    "        const metric_params *restrict metric_params_in,\n",
    "        const double y[9], \n",
    "        double *E, double *Lx, double *Ly, double *Lz, double *Q\"\"\"\n",
    "        \n",
    "    body = r\"\"\"\n",
    "    // Unpack parameters from commondata struct that are needed symbolically\n",
    "    const REAL a_spin = commondata->a_spin;\n",
    "\n",
    "    // Declare a POINTER to a metric_struct and allocate memory for it.\n",
    "    metric_struct* metric = (metric_struct*)malloc(sizeof(metric_struct));\n",
    "    \n",
    "    // Call the dispatcher to fill the allocated struct with metric components at the given state y.\n",
    "    g4DD_metric(commondata, params, metric_params_in, y, metric);\n",
    "\n",
    "    // --- MODIFIED: Simplified logic for Cartesian-only checks ---\n",
    "    if (metric_params_in->type == Kerr) {\n",
    "        \"\"\" + body_C_code_kerr + r\"\"\"\n",
    "    } else { // Both Schwarzschild types are now Cartesian\n",
    "        \"\"\" + body_C_code_schw + r\"\"\"\n",
    "    }\n",
    "    \n",
    "    free(metric);\n",
    "    \"\"\"\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, cfunc_type=\"void\",\n",
    "        name=name, params=params, body=body\n",
    "    )\n",
    "    print(f\"    ... {name}() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2e685",
   "metadata": {},
   "source": [
    "# Markdown for event detection manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0673106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_detection_manager():\n",
    "    \"\"\"\n",
    "    Generates the C event detection manager.\n",
    "    \n",
    "    This final version is a pure, stateless plane-crossing detector. It takes\n",
    "    the previous state of the photon (which side of the plane it was on) and\n",
    "    updates the event_data_struct if a crossing has occurred.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C event detection manager (Stateless Plane Detector Version)...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<math.h>\"]\n",
    "    \n",
    "\n",
    "    desc = r\"\"\"@brief Detects crossings of the window and source planes.\"\"\"\n",
    "    name = \"event_detection_manager\"\n",
    "    cfunc_type = \"void\"\n",
    "    params = r\"\"\"\n",
    "        const double y_prev[9], const double y_curr[9], const double y_next[9],\n",
    "        double lambda_prev, double lambda_curr, double lambda_next,\n",
    "        const commondata_struct *restrict commondata,\n",
    "        bool *on_positive_side_of_window_prev,\n",
    "        bool *on_positive_side_of_source_prev,\n",
    "        event_data_struct *restrict window_event,\n",
    "        event_data_struct *restrict source_plane_event\n",
    "        \"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // --- Window Plane Detection ---\n",
    "    // This logic is only performed if the caller has not already found the event.\n",
    "    if (!window_event->found) {\n",
    "        double window_plane_normal[3] = {commondata->window_center_x - commondata->camera_pos_x,\n",
    "                                         commondata->window_center_y - commondata->camera_pos_y,\n",
    "                                         commondata->window_center_z - commondata->camera_pos_z};\n",
    "        const double mag_w_norm = sqrt(SQR(window_plane_normal[0]) + SQR(window_plane_normal[1]) + SQR(window_plane_normal[2]));\n",
    "        if (mag_w_norm > 1e-12) {\n",
    "            const double inv_mag_w_norm = 1.0 / mag_w_norm;\n",
    "            for(int i=0;i<3;i++) window_plane_normal[i] *= inv_mag_w_norm;\n",
    "        }\n",
    "        const double window_plane_dist = commondata->window_center_x * window_plane_normal[0] +\n",
    "                                         commondata->window_center_y * window_plane_normal[1] +\n",
    "                                         commondata->window_center_z * window_plane_normal[2];\n",
    "\n",
    "        plane_event_params window_params = {{window_plane_normal[0], window_plane_normal[1], window_plane_normal[2]}, window_plane_dist};\n",
    "        bool on_positive_side_curr = (plane_event_func(y_next, &window_params) > 0);\n",
    "        if (on_positive_side_curr != *on_positive_side_of_window_prev) {\n",
    "            find_event_time_and_state(y_prev, y_curr, y_next, lambda_prev, lambda_curr, lambda_next,\n",
    "                                      plane_event_func, &window_params, window_event);\n",
    "        }\n",
    "        *on_positive_side_of_window_prev = on_positive_side_curr;\n",
    "    }\n",
    "\n",
    "    // --- Source Plane Detection ---\n",
    "    // This logic is only performed if the caller has not already found the event.\n",
    "    if (!source_plane_event->found) {\n",
    "        const double source_plane_normal[3] = {commondata->source_plane_normal_x,\n",
    "                                               commondata->source_plane_normal_y,\n",
    "                                               commondata->source_plane_normal_z};\n",
    "        const double source_plane_dist = commondata->source_plane_center_x * source_plane_normal[0] +\n",
    "                                         commondata->source_plane_center_y * source_plane_normal[1] +\n",
    "                                         commondata->source_plane_center_z * source_plane_normal[2];\n",
    "\n",
    "        plane_event_params source_params = {{source_plane_normal[0], source_plane_normal[1], source_plane_normal[2]}, source_plane_dist};\n",
    "        bool on_positive_side_curr = (plane_event_func(y_next, &source_params) > 0);\n",
    "        if (on_positive_side_curr != *on_positive_side_of_source_prev) {\n",
    "            find_event_time_and_state(y_prev, y_curr, y_next, lambda_prev, lambda_curr, lambda_next,\n",
    "                                      plane_event_func, &source_params, source_plane_event);\n",
    "        }\n",
    "        *on_positive_side_of_source_prev = on_positive_side_curr;\n",
    "    }\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, cfunc_type=cfunc_type, name=name, params=params, body=body)\n",
    "    print(\"    ... Registered event_detection_manager (Stateless Plane Detector Version).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48805aa",
   "metadata": {},
   "source": [
    "# kd tree loaer and unloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb9daeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kdtree_loader_and_unloader():\n",
    "    \"\"\"\n",
    "    Generates C functions for memory-mapping a .kdtree.bin file into memory\n",
    "    and for unmapping it.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C functions for k-d tree memory mapping...\")\n",
    "\n",
    "    # Function to load a snapshot\n",
    "    load_includes = [\"BHaH_defines.h\", \"<stdio.h>\", \"<stdlib.h>\", \"<sys/mman.h>\", \"<sys/stat.h>\", \"<fcntl.h>\", \"<unistd.h>\"]\n",
    "    load_desc = r\"\"\"@brief Loads a .kdtree.bin snapshot file into memory using mmap.\"\"\"\n",
    "    load_name = \"load_kdtree_snapshot\"\n",
    "    load_params = \"const char *filename, CustomKDTree *tree\"\n",
    "    load_body = r\"\"\"\n",
    "    int fd = open(filename, O_RDONLY);\n",
    "    if (fd == -1) {\n",
    "        perror(\"Error opening k-d tree file\");\n",
    "        return -1; // Failure\n",
    "    }\n",
    "\n",
    "    struct stat sb;\n",
    "    if (fstat(fd, &sb) == -1) {\n",
    "        perror(\"Error getting file size\");\n",
    "        close(fd);\n",
    "        return -1;\n",
    "    }\n",
    "    tree->file_size = sb.st_size;\n",
    "\n",
    "    void *mapped_mem = mmap(NULL, tree->file_size, PROT_READ, MAP_PRIVATE, fd, 0);\n",
    "    if (mapped_mem == MAP_FAILED) {\n",
    "        perror(\"Error memory-mapping the file\");\n",
    "        close(fd);\n",
    "        return -1;\n",
    "    }\n",
    "    close(fd); // File descriptor no longer needed after mmap\n",
    "\n",
    "    tree->original_mmap_ptr = mapped_mem;\n",
    "    char *current_ptr = (char *)mapped_mem;\n",
    "\n",
    "    // Read header\n",
    "    tree->num_particles = *(uint64_t *)current_ptr;\n",
    "    current_ptr += sizeof(uint64_t);\n",
    "    tree->dimensions = *(uint64_t *)current_ptr;\n",
    "    current_ptr += sizeof(uint64_t);\n",
    "\n",
    "    // Set pointers to payloads\n",
    "    tree->node_metadata = (int32_t *)current_ptr;\n",
    "    current_ptr += sizeof(int32_t) * tree->num_particles;\n",
    "    tree->particle_data = (MassiveParticle *)current_ptr;\n",
    "\n",
    "    return 0; // Success\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=load_includes, desc=load_desc, name=load_name, params=load_params, body=load_body, cfunc_type=\"int\")\n",
    "\n",
    "    # Function to unload a snapshot\n",
    "    unload_includes = [\"BHaH_defines.h\", \"<sys/mman.h>\"]\n",
    "    unload_desc = r\"\"\"@brief Unloads a memory-mapped k-d tree snapshot.\"\"\"\n",
    "    unload_name = \"unload_kdtree_snapshot\"\n",
    "    unload_params = \"CustomKDTree *tree\"\n",
    "    unload_body = r\"\"\"\n",
    "    if (tree->original_mmap_ptr != NULL) {\n",
    "        munmap(tree->original_mmap_ptr, tree->file_size);\n",
    "        tree->original_mmap_ptr = NULL;\n",
    "    }\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=unload_includes, desc=unload_desc, name=unload_name, params=unload_params, body=unload_body)\n",
    "    \n",
    "    print(\"    ... Registered C functions: load_kdtree_snapshot, unload_kdtree_snapshot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89505589",
   "metadata": {},
   "source": [
    "# kd tree search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9da9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def kdtree_search_engine():\n",
    "    \"\"\"\n",
    "    Generates the C functions that perform the recursive k-Nearest Neighbor search\n",
    "    on a loaded k-d tree.\n",
    "    \n",
    "    VERSION 2: PERFORMANCE OPTIMIZED.\n",
    "    This version adds __builtin_prefetch compiler intrinsics to the recursive\n",
    "    search function. This is a low-level hint to the CPU to begin fetching data\n",
    "    for child nodes from main memory into the cache before it is explicitly needed.\n",
    "    This technique aims to hide memory latency and reduce CPU stalls caused by\n",
    "    cache misses, which were identified as the primary performance bottleneck.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine for k-d tree nearest neighbor search [PERFORMANCE OPTIMIZED]...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"<math.h>\", \"<stdio.h>\"]\n",
    "    \n",
    "    prefunc = r\"\"\"\n",
    "// Helper to initialize the WinnersCircle struct\n",
    "static void wc_init(WinnersCircle *wc, int n_wanted) {\n",
    "    wc->n_wanted = n_wanted;\n",
    "    wc->count = 0;\n",
    "    for (int i = 0; i < n_wanted; ++i) {\n",
    "        wc->indices[i] = -1;\n",
    "        wc->sq_distances[i] = 1e300; // Initialize with a very large number\n",
    "    }\n",
    "}\n",
    "\n",
    "// Helper to add a candidate to the WinnersCircle, maintaining sorted order\n",
    "static void wc_add(WinnersCircle *wc, int index, double sq_dist) {\n",
    "    if (wc->count < wc->n_wanted) {\n",
    "        wc->indices[wc->count] = index;\n",
    "        wc->sq_distances[wc->count] = sq_dist;\n",
    "        wc->count++;\n",
    "    } else if (sq_dist < wc->sq_distances[wc->n_wanted - 1]) {\n",
    "        wc->indices[wc->n_wanted - 1] = index;\n",
    "        wc->sq_distances[wc->n_wanted - 1] = sq_dist;\n",
    "    } else {\n",
    "        return; // Not a winner\n",
    "    }\n",
    "\n",
    "    for (int i = wc->count - 1; i > 0; --i) {\n",
    "        if (wc->sq_distances[i] < wc->sq_distances[i - 1]) {\n",
    "            double temp_d = wc->sq_distances[i];\n",
    "            int temp_i = wc->indices[i];\n",
    "            wc->sq_distances[i] = wc->sq_distances[i - 1];\n",
    "            wc->indices[i] = wc->indices[i - 1];\n",
    "            wc->sq_distances[i - 1] = temp_d;\n",
    "            wc->indices[i - 1] = temp_i;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// The recursive search function\n",
    "static void search_recursive(const CustomKDTree *tree, const double query_pos[3], int current_idx, WinnersCircle *wc) {\n",
    "    if (current_idx < 0 || current_idx >= (int)tree->num_particles) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    const MassiveParticle *pivot = &tree->particle_data[current_idx];\n",
    "    const int split_axis = tree->node_metadata[current_idx];\n",
    "\n",
    "    const double dx = query_pos[0] - pivot->pos[0];\n",
    "    const double dy = query_pos[1] - pivot->pos[1];\n",
    "    const double dz = query_pos[2] - pivot->pos[2];\n",
    "    const double dist_sq = dx*dx + dy*dy + dz*dz;\n",
    "    wc_add(wc, current_idx, dist_sq);\n",
    "\n",
    "    if (split_axis == -1) { // Leaf node\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    const double axis_dist = query_pos[split_axis] - pivot->pos[split_axis];\n",
    "    const int good_side_idx = (axis_dist < 0) ? (2 * current_idx + 1) : (2 * current_idx + 2);\n",
    "    const int bad_side_idx = (axis_dist < 0) ? (2 * current_idx + 2) : (2 * current_idx + 1);\n",
    "\n",
    "    // *** PERFORMANCE OPTIMIZATION ***\n",
    "    // Issue prefetch instructions for the data of the child nodes. This hints to the\n",
    "    // CPU to start loading this memory into the cache while we process the current node.\n",
    "    // The '0' indicates a read operation.\n",
    "    // The '1' indicates low temporal locality (we likely won't need this exact data again soon).\n",
    "    if (good_side_idx < (int)tree->num_particles) {\n",
    "        __builtin_prefetch(&tree->particle_data[good_side_idx], 0, 1);\n",
    "    }\n",
    "    if (bad_side_idx < (int)tree->num_particles) {\n",
    "        __builtin_prefetch(&tree->particle_data[bad_side_idx], 0, 1);\n",
    "    }\n",
    "    // *** END OF OPTIMIZATION ***\n",
    "\n",
    "    search_recursive(tree, query_pos, good_side_idx, wc);\n",
    "\n",
    "    const double search_radius_sq = wc->sq_distances[wc->n_wanted - 1];\n",
    "    if (axis_dist * axis_dist < search_radius_sq) {\n",
    "        search_recursive(tree, query_pos, bad_side_idx, wc);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    desc = r\"\"\"@brief Finds the N nearest neighbors to a query point in a k-d tree.\"\"\"\n",
    "    name = \"find_n_nearest_neighbors\"\n",
    "    params = \"const CustomKDTree *tree, const double query_pos[3], int n_neighbors, MassiveParticle *neighbor_results\"\n",
    "    \n",
    "    body = r\"\"\"\n",
    "    if (n_neighbors > MAX_NEIGHBORS) {\n",
    "        fprintf(stderr, \"Error: Requested more neighbors than MAX_NEIGHBORS.\\n\");\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    WinnersCircle wc;\n",
    "    wc_init(&wc, n_neighbors);\n",
    "\n",
    "    // Start the recursive search from the root (index 0)\n",
    "    search_recursive(tree, query_pos, 0, &wc);\n",
    "\n",
    "    // Copy the results into the output array\n",
    "    for (int i = 0; i < wc.count; ++i) {\n",
    "        neighbor_results[i] = tree->particle_data[wc.indices[i]];\n",
    "    }\n",
    "\"\"\"\n",
    "    cfc.register_CFunction(includes=includes, prefunc=prefunc, desc=desc, name=name, params=params, body=body)\n",
    "    print(\"    ... Registered C engine: find_n_nearest_neighbors [PERFORMANCE OPTIMIZED].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ed05b",
   "metadata": {},
   "source": [
    "# Radiative transfer engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56dac28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiative_transfer_engine():\n",
    "    \"\"\"\n",
    "    Generates the C engine for calculating the final observed intensity and\n",
    "    wavelength based on the interpolated disk state and photon momentum.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine for radiative transfer physics...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"<math.h>\"]\n",
    "    desc = r\"\"\"@brief Calculates the observed intensity and wavelength from the disk and photon state.\"\"\"\n",
    "    name = \"calculate_radiative_transfer\"\n",
    "    params = r\"\"\"\n",
    "    const double photon_p_mu[4], const double disk_u_mu[4],\n",
    "    const float disk_j_intrinsic, const double disk_lambda_rest,\n",
    "    double *stokes_I, double *lambda_observed\n",
    "    \"\"\"\n",
    "    body = r\"\"\"\n",
    "    // The observer is assumed to be at rest in the coordinate frame far away,\n",
    "    // so their 4-velocity is u_obs^mu = (1, 0, 0, 0).\n",
    "    // The metric is Minkowski far away, so u_obs_mu = (-1, 0, 0, 0).\n",
    "    // The photon momentum is p_mu.\n",
    "    // Therefore, (-p_mu u^mu)_obs = - (p_0 * -1) = p_0.\n",
    "    // NOTE: The photon momentum p_mu must be covariant (lower-indexed).\n",
    "    const double p_mu_u_mu_obs = photon_p_mu[0];\n",
    "\n",
    "    // Calculate (-p_mu u^mu)_disk\n",
    "    const double p_mu_u_mu_disk = - (photon_p_mu[0] * disk_u_mu[0] +\n",
    "                                     photon_p_mu[1] * disk_u_mu[1] +\n",
    "                                     photon_p_mu[2] * disk_u_mu[2] +\n",
    "                                     photon_p_mu[3] * disk_u_mu[3]);\n",
    "\n",
    "    // Doppler factor D = E_obs / E_disk = (-p_mu u^mu)_obs / (-p_mu u^mu)_disk\n",
    "    const double doppler_factor = p_mu_u_mu_obs / p_mu_u_mu_disk;\n",
    "\n",
    "    // Observed intensity I_obs = j_intrinsic * D^3\n",
    "    *stokes_I = disk_j_intrinsic * doppler_factor * doppler_factor * doppler_factor;\n",
    "\n",
    "    // Observed wavelength lambda_obs = lambda_rest / D\n",
    "    *lambda_observed = disk_lambda_rest / doppler_factor;\n",
    "\"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)\n",
    "    print(\"    ... Registered C engine: calculate_radiative_transfer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68b49d",
   "metadata": {},
   "source": [
    "# Source plane intersection engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea1786c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_8_Python_to_C_via_NRPy.ipynb\n",
    "# In cell [ea1786c7]\n",
    "\n",
    "def handle_source_plane_intersection_engine():\n",
    "    \"\"\"\n",
    "    Generates a C engine that handles a source plane intersection.\n",
    "    This version (v2.9.1) uses the correct, consolidated PhotonStatus enum.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: handle_source_plane_intersection (v2.9.1)...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<math.h>\", \"<stdbool.h>\"]\n",
    "    desc = r\"\"\"@brief Handles a source plane intersection by checking bounds and populating the blueprint.\"\"\"\n",
    "    name = \"handle_source_plane_intersection\"\n",
    "    cfunc_type = \"bool\"\n",
    "    params = r\"\"\"\n",
    "    const event_data_struct *restrict source_plane_event,\n",
    "    const commondata_struct *restrict commondata,\n",
    "    blueprint_data_t *restrict final_blueprint_data\n",
    "    \"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // --- Calculate the local (y_s, z_s) coordinates on the plane ---\n",
    "    const double intersection_pos[3] = {source_plane_event->y_event[1], source_plane_event->y_event[2], source_plane_event->y_event[3]};\n",
    "    const double source_plane_center[3] = {commondata->source_plane_center_x, commondata->source_plane_center_y, commondata->source_plane_center_z};\n",
    "    const double source_plane_normal[3] = {commondata->source_plane_normal_x, commondata->source_plane_normal_y, commondata->source_plane_normal_z};\n",
    "    const double source_up_vector[3] = {commondata->source_up_vec_x, commondata->source_up_vec_y, commondata->source_up_vec_z};\n",
    "\n",
    "    // Construct orthonormal basis vectors for the source plane\n",
    "    double s_z[3] = {source_plane_normal[0], source_plane_normal[1], source_plane_normal[2]};\n",
    "    double s_x[3] = {source_up_vector[1]*s_z[2] - source_up_vector[2]*s_z[1], \n",
    "                     source_up_vector[2]*s_z[0] - source_up_vector[0]*s_z[2], \n",
    "                     source_up_vector[0]*s_z[1] - source_up_vector[1]*s_z[0]};\n",
    "    double mag_s_x = sqrt(SQR(s_x[0]) + SQR(s_x[1]) + SQR(s_x[2]));\n",
    "    \n",
    "    if (mag_s_x < 1e-9) {\n",
    "        double alternative_up[3] = {1.0, 0.0, 0.0};\n",
    "        if (fabs(s_z[0]) > 0.999) {\n",
    "            alternative_up[0] = 0.0;\n",
    "            alternative_up[1] = 1.0;\n",
    "        }\n",
    "        s_x[0] = alternative_up[1]*s_z[2] - alternative_up[2]*s_z[1];\n",
    "        s_x[1] = alternative_up[2]*s_z[0] - alternative_up[0]*s_z[2];\n",
    "        s_x[2] = alternative_up[0]*s_z[1] - alternative_up[1]*s_z[0];\n",
    "        mag_s_x = sqrt(SQR(s_x[0]) + SQR(s_x[1]) + SQR(s_x[2]));\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<3; i++) s_x[i] /= mag_s_x;\n",
    "    \n",
    "    double s_y[3] = {s_z[1]*s_x[2] - s_z[2]*s_x[1], \n",
    "                     s_z[2]*s_x[0] - s_z[0]*s_x[2], \n",
    "                     s_z[0]*s_x[1] - s_z[1]*s_x[0]};\n",
    "\n",
    "    const double vec_s[3] = {intersection_pos[0] - source_plane_center[0], \n",
    "                             intersection_pos[1] - source_plane_center[1], \n",
    "                             intersection_pos[2] - source_plane_center[2]};\n",
    "    \n",
    "    const double y_s = vec_s[0]*s_x[0] + vec_s[1]*s_x[1] + vec_s[2]*s_x[2];\n",
    "    const double z_s = vec_s[0]*s_y[0] + vec_s[1]*s_y[1] + vec_s[2]*s_y[2];\n",
    "    \n",
    "    const double r_s_sq = SQR(y_s) + SQR(z_s);\n",
    "    \n",
    "    if (r_s_sq >= SQR(commondata->source_r_min) && r_s_sq <= SQR(commondata->source_r_max)) {\n",
    "        // This is a valid hit. Populate the blueprint and return true.\n",
    "        // *** CORRECTED: Use the correct enum member from PhotonStatus ***\n",
    "        final_blueprint_data->termination_type = TERMINATED_SOURCE;\n",
    "        final_blueprint_data->y_s = y_s;\n",
    "        final_blueprint_data->z_s = z_s;\n",
    "        final_blueprint_data->t_s = source_plane_event->t_event;\n",
    "        final_blueprint_data->L_s = source_plane_event->y_event[8];\n",
    "        return true;\n",
    "    }\n",
    "    \n",
    "    // The intersection was outside the valid radial bounds. Return false.\n",
    "    return false;\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, cfunc_type=cfunc_type, params=params, body=body)\n",
    "    print(f\"    ... Registered C engine: {name}().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f46ff4",
   "metadata": {},
   "source": [
    "# Source disk intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3038833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_disk_intersection_engine():\n",
    "    \"\"\"\n",
    "    Generates the C engine to handle a disk intersection event.\n",
    "    \n",
    "    UPDATED for the \"Nearest Neighbor\" model. This function no longer interpolates.\n",
    "    It takes the single closest massive particle and directly uses its properties\n",
    "    for the radiative transfer calculation.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: handle_disk_intersection (Nearest Neighbor Version)...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<math.h>\"]\n",
    "    desc = r\"\"\"@brief Handles the physics calculations for a disk intersection event using the nearest neighbor.\"\"\"\n",
    "    name = \"handle_disk_intersection\"\n",
    "    # The signature is now much simpler.\n",
    "    params = r\"\"\"\n",
    "    const double current_y[9],\n",
    "    const MassiveParticle *restrict nearest_neighbor,\n",
    "    const commondata_struct *restrict commondata, const params_struct *restrict params,\n",
    "    const metric_params *restrict metric,\n",
    "    blueprint_data_t *restrict final_blueprint_data\n",
    "    \"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // The photon's state is current_y. The disk's state is taken directly\n",
    "    // from the provided nearest_neighbor particle.\n",
    "    \n",
    "    // 1. Get metric at the photon's current position\n",
    "    metric_struct g4DD;\n",
    "    g4DD_metric(commondata, params, metric, current_y, &g4DD);\n",
    "    \n",
    "    // 2. Lower indices of photon momentum and the neighbor's 4-velocity\n",
    "    const double g_munu[4][4] = {\n",
    "        {g4DD.g00, g4DD.g01, g4DD.g02, g4DD.g03},\n",
    "        {g4DD.g01, g4DD.g11, g4DD.g12, g4DD.g13},\n",
    "        {g4DD.g02, g4DD.g12, g4DD.g22, g4DD.g23},\n",
    "        {g4DD.g03, g4DD.g13, g4DD.g23, g4DD.g33}\n",
    "    };\n",
    "    \n",
    "    double photon_p_mu[4] = {0,0,0,0};\n",
    "    double disk_u_mu[4] = {0,0,0,0}; // This is now the neighbor's u_mu\n",
    "    for(int mu=0; mu<4; mu++) {\n",
    "        for(int nu=0; nu<4; nu++) {\n",
    "            photon_p_mu[mu] += g_munu[mu][nu] * current_y[nu+4];\n",
    "            disk_u_mu[mu] += g_munu[mu][nu] * nearest_neighbor->u[nu];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // 3. Call the radiative transfer physics engine\n",
    "    double temp_stokes_I;\n",
    "    double temp_lambda_observed;\n",
    "    calculate_radiative_transfer(photon_p_mu, disk_u_mu, \n",
    "                                 nearest_neighbor->j_intrinsic, nearest_neighbor->lambda_rest,\n",
    "                                 &temp_stokes_I, &temp_lambda_observed);\n",
    "    \n",
    "    // 4. Populate the final blueprint with the results\n",
    "    final_blueprint_data->stokes_I = temp_stokes_I;\n",
    "    final_blueprint_data->lambda_observed = temp_lambda_observed;\n",
    "    final_blueprint_data->termination_type = TERMINATION_TYPE_DISK;\n",
    "    \n",
    "    // Store diagnostic information\n",
    "    final_blueprint_data->y_s = nearest_neighbor->pos[0]; // x-pos of neighbor\n",
    "    final_blueprint_data->z_s = nearest_neighbor->pos[1]; // y-pos of neighbor\n",
    "    final_blueprint_data->t_s = current_y[0]; // time of intersection\n",
    "    final_blueprint_data->L_s = current_y[8]; // path length at intersection\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)\n",
    "    print(\"    ... Registered C engine: handle_disk_intersection (Nearest Neighbor Version).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fb7b8",
   "metadata": {},
   "source": [
    "<a id='generate_c_orchestrators'></a>\n",
    "# Step 7: C Code Generation - Orchestrators and Dispatchers\n",
    "\n",
    "With the low-level \"engine\" and \"worker\" functions defined in the previous step, we now generate the higher-level C functions that manage the simulation. These functions are responsible for dispatching to the correct worker based on runtime parameters and for orchestrating the overall program flow.\n",
    "\n",
    "*   **Dispatchers** are functions that contain a `switch` statement to select the correct \"worker\" function based on the chosen metric (e.g., `Schwarzschild` vs. `Kerr`).\n",
    "*   **Orchestrators** are functions that execute a sequence of calls to other engines, workers, and dispatchers to perform a complex task, like setting up initial conditions or running the main integration loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bcd71",
   "metadata": {},
   "source": [
    "# Filename_sorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf6b3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_sorter():\n",
    "    \"\"\"\n",
    "    Generates a C helper function to be used by qsort for sorting snapshot filenames.\n",
    "    \"\"\"\n",
    "    includes = [\"stdio.h\", \"<stdlib.h>\"]\n",
    "    desc = \"Comparison function for qsort to sort filenames numerically.\"\n",
    "    name = \"compare_filenames\"\n",
    "    cfunc_type = \"int\"\n",
    "    params = \"const void *a, const void *b\"\n",
    "    body = r\"\"\"\n",
    "    const char *str_a = *(const char **)a;\n",
    "    const char *str_b = *(const char **)b;\n",
    "    int num_a, num_b;\n",
    "    sscanf(str_a, \"mass_blueprint_t_%d.kdtree.bin\", &num_a);\n",
    "    sscanf(str_b, \"mass_blueprint_t_%d.kdtree.bin\", &num_b);\n",
    "    return (num_a > num_b) - (num_a < num_b);\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, cfunc_type=cfunc_type, params=params, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db0d5",
   "metadata": {},
   "source": [
    "<a id='g4DD_metric_dispatcher'></a>\n",
    "### 7.a: `g4DD_metric()` Dispatcher\n",
    "\n",
    "This Python function generates the C function `g4DD_metric()`, which serves as a high-level **dispatcher.** Its role is to select and call the correct worker function to compute the components of the metric tensor, $g_{\\mu\\nu}$.\n",
    "\n",
    "The generated C code uses a `switch` statement that reads the `metric->type` member of the `metric_params` struct. In this project, both the Schwarzschild and Kerr spacetimes are handled by the unified `g4DD_kerr_schild()` worker function. The dispatcher calls this single worker, and the specific metric returned by the worker depends on the runtime value of the `a_spin` parameter (if `a_spin` is 0, the Schwarzschild metric is computed).\n",
    "\n",
    "This modular approach cleanly separates the control flow (deciding *which* metric to use) from the physics implementation (the worker functions that know *how* to compute a specific metric). This makes the project easy to extend with new spacetimes in the future by adding new cases to the `switch` statement and new worker functions.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced. Used to register the manually written C code for the dispatcher function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65702cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In cell [65702cb7]\n",
    "\n",
    "def g4DD_metric():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function g4DD_metric(), which serves as a\n",
    "    dispatcher to call the appropriate metric-specific worker function.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C dispatcher function: g4DD_metric()...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Dispatcher to compute the 4-metric g_munu for the chosen metric.\"\"\"\n",
    "    name = \"g4DD_metric\"\n",
    "    # The signature is now coordinate-aware, but the y vector is always Cartesian here.\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const metric_params *restrict metric, const double y[9], metric_struct *restrict metric_out\"\n",
    "    \n",
    "    body = r\"\"\"\n",
    "    // The state vector y_pos contains only the position coordinates.\n",
    "    const double y_pos[4] = {y[0], y[1], y[2], y[3]};\n",
    "\n",
    "    // This switch statement chooses which \"worker\" function to call\n",
    "    // based on the metric type provided.\n",
    "    switch(metric->type) {\n",
    "        case Schwarzschild:\n",
    "        case Kerr:\n",
    "            // For Kerr or Schwarzschild in KS coords, call the unified Kerr-Schild C function.\n",
    "            g4DD_kerr_schild(commondata, params, y_pos, metric_out);\n",
    "            break;\n",
    "        // <-- MODIFIED: Call the new Cartesian worker\n",
    "        case Schwarzschild_Standard:\n",
    "            g4DD_schwarzschild_cartesian(commondata, params, y_pos, metric_out);\n",
    "            break;\n",
    "        case Numerical:\n",
    "            printf(\"Error: Numerical metric not supported yet.\\n\");\n",
    "            exit(1);\n",
    "            break;\n",
    "        default:\n",
    "            printf(\"Error: MetricType %d not supported in g4DD_metric() yet.\\n\", metric->type);\n",
    "            exit(1);\n",
    "            break;\n",
    "    }\n",
    "\"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)\n",
    "    print(\"    ... g4DD_metric() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db30c0a",
   "metadata": {},
   "source": [
    "<a id='connections_dispatcher'></a>\n",
    "### 7.b: `connections()` Dispatcher\n",
    "\n",
    "This Python function generates the C function `connections()`, which acts as a second **dispatcher.** Its sole responsibility is to select and call the correct metric-specific worker function (like `con_kerr_schild()`) to compute the Christoffel symbols.\n",
    "\n",
    "Like the `g4DD_metric()` dispatcher, the generated C code uses a `switch` statement based on the `metric->type`. It dispatches the call to the appropriate specialized worker, which in this case is the unified `con_kerr_schild()` function for both Kerr and Schwarzschild spacetimes. This design is highly extensible: adding a new metric simply requires writing a new worker function for its Christoffel symbols and adding a new `case` to this `switch` statement.\n",
    "\n",
    "This function demonstrates how `nrpy` allows for the seamless integration of developer-written control flow with the automatically generated worker functions.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b92b7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In cell [b92b7851]\n",
    "\n",
    "def connections():\n",
    "    \"\"\"\n",
    "    Generates and registers the C dispatcher for Christoffel symbols.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C dispatcher: connections()...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"stdio.h\", \"stdlib.h\"]\n",
    "    desc = r\"\"\"@brief Dispatcher to compute Christoffel symbols for the chosen metric.\"\"\"\n",
    "    \n",
    "    name = \"connections\"\n",
    "    cfunc_type = \"void\" \n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const metric_params *restrict metric, const double y[9], connection_struct *restrict conn\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // The state vector y_pos contains only the position coordinates.\n",
    "    const double y_pos[4] = {y[0], y[1], y[2], y[3]};\n",
    "\n",
    "    // This switch statement chooses which \"worker\" function to call\n",
    "    // based on the metric type provided.\n",
    "    switch(metric->type) {\n",
    "        case Schwarzschild:\n",
    "        case Kerr:\n",
    "            con_kerr_schild(commondata, params, y_pos, conn);\n",
    "            break;\n",
    "        // <-- MODIFIED: Call the new Cartesian worker\n",
    "        case Schwarzschild_Standard:\n",
    "            con_schwarzschild_cartesian(commondata, params, y_pos, conn);\n",
    "            break;\n",
    "        case Numerical:\n",
    "            printf(\"Error: Numerical metric not supported yet.\\n\");\n",
    "            exit(1);\n",
    "            break;\n",
    "        default:\n",
    "            printf(\"Error: MetricType %d not supported yet.\\n\", metric->type);\n",
    "            exit(1);\n",
    "            break;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, cfunc_type=cfunc_type,\n",
    "        name=name, params=params, body=body\n",
    "    )\n",
    "    print(\"    ... connections() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ea28a",
   "metadata": {},
   "source": [
    "<a id='set_initial_conditions_cartesian'></a>\n",
    "### 7.c: `set_initial_conditions_cartesian()` Orchestrator\n",
    "\n",
    "This function generates the C **orchestrator** `set_initial_conditions_cartesian()`. This function is responsible for setting the complete initial state vector `y_out[9]` for a single light ray. It orchestrates a sequence of calculations to do this.\n",
    "\n",
    "The process for setting the initial state `y = (t, x, y, z, p^t, p^x, p^y, p^z, L)` is as follows:\n",
    "\n",
    "1.  **Set Initial Position**: The initial spatial coordinates `(x, y, z)` are set to the camera's location, `camera_pos`. The initial time `t` and path length `L` are set to `0.0`.\n",
    "2.  **Calculate Aiming Vector**: It computes the aiming vector `V`, which points from the camera to a specific target pixel on the window plane: `V = target_pos - camera_pos`.\n",
    "3.  **Set Initial Spatial Momentum**: As derived in the introduction, the initial reverse-time spatial momentum `(p^x, p^y, p^z)` must be parallel to the aiming vector `V`. It is therefore set to the normalized aiming vector: `p^i = V^i / |V|`.\n",
    "4.  **Calculate Initial Time Momentum**: With the spatial components of the momentum set, the final unknown is the time component, $p^t = p^0$. This requires a call to the physics engines:\n",
    "    *   First, it calls the `g4DD_metric()` dispatcher to compute the metric components $g_{\\mu\\nu}$ at the camera's location.\n",
    "    *   Then, it passes these metric components and the partially-filled state vector `y_out` to the `calculate_p0_reverse()` engine, which solves the null condition $g_{\\mu\\nu}p^\\mu p^\\nu=0$ for $p^0$.\n",
    "    *   The result is stored in `y_out[4]`, completing the initial state vector.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfd43b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_initial_conditions_cartesian():\n",
    "    \"\"\"\n",
    "    Generates the C engine to set the initial state vector, now entirely in\n",
    "    Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: set_initial_conditions_cartesian()...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Sets the full initial state for a ray in Cartesian coordinates.\"\"\"\n",
    "    \n",
    "    name = \"set_initial_conditions_cartesian\"\n",
    "    params = \"\"\"const commondata_struct *restrict commondata, const params_struct *restrict params,\n",
    "                const metric_params *restrict metric,\n",
    "                const double camera_pos[3], const double target_pos[3],\n",
    "                double y_out[9]\"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // --- Step 1: Set the initial position to the camera's location ---\n",
    "    y_out[0] = 0.0; // t\n",
    "    y_out[1] = camera_pos[0]; // x\n",
    "    y_out[2] = camera_pos[1]; // y\n",
    "    y_out[3] = camera_pos[2]; // z\n",
    "    y_out[8] = 0.0; // L (integrated path length)\n",
    "\n",
    "    // --- Step 2: Calculate the aiming vector V and set spatial momentum ---\n",
    "    const double V_x = target_pos[0] - camera_pos[0];\n",
    "    const double V_y = target_pos[1] - camera_pos[1];\n",
    "    const double V_z = target_pos[2] - camera_pos[2];\n",
    "    const double mag_V = sqrt(V_x*V_x + V_y*V_y + V_z*V_z);\n",
    "    \n",
    "    // The reverse-momentum p is parallel to the aiming vector V.\n",
    "    if (mag_V > 1e-12) {\n",
    "        y_out[5] = V_x / mag_V; // p^x\n",
    "        y_out[6] = V_y / mag_V; // p^y\n",
    "        y_out[7] = V_z / mag_V; // p^z\n",
    "    } else {\n",
    "        // Should not happen in production, but as a fallback, set a default momentum.\n",
    "        y_out[5] = 1.0; y_out[6] = 0.0; y_out[7] = 0.0;\n",
    "    }\n",
    "    \n",
    "    // --- Step 3: Calculate the time component p^t using the null condition ---\n",
    "    metric_struct g4DD;\n",
    "    // Note: The g4DD_metric function needs the first 4 elements of y_out (the coordinates).\n",
    "    g4DD_metric(commondata, params, metric, y_out, &g4DD);\n",
    "    \n",
    "    // The state vector y is (t,x,y,z, p^t,p^x,p^y,p^z, L).\n",
    "    y_out[4] = calculate_p0_reverse(&g4DD, y_out);\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69029a7a",
   "metadata": {},
   "source": [
    "# Batch integrator orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7467afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_integrator_orchestrator():\n",
    "    \"\"\"\n",
    "    Generates the main C orchestrator, batch_integrator(), with all logic,\n",
    "    event detection, and finalization fully and correctly implemented.\n",
    "    This version (v2.9.2) adds the necessary GSL headers.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating top-level C orchestrator: batch_integrator() (v2.9.2)...\")\n",
    "\n",
    "    # *** CORRECTED: Added GSL headers to the includes list. ***\n",
    "    includes = [\n",
    "        \"BHaH_defines.h\",\n",
    "        \"BHaH_function_prototypes.h\",\n",
    "        \"omp.h\",\n",
    "        \"<stdbool.h>\"\n",
    "    ]\n",
    "    desc = r\"\"\"@brief Main orchestrator for time-bundled photon integration.\"\"\"\n",
    "    name = \"batch_integrator\"\n",
    "    params = r\"\"\"\n",
    "    const commondata_struct *restrict commondata,\n",
    "    const params_struct *restrict params,\n",
    "    const metric_params *restrict metric,\n",
    "    long int num_rays,\n",
    "    blueprint_data_t *results_buffer\n",
    "    \"\"\"\n",
    "    # The body of this function remains correct from the previous approved version.\n",
    "    # Only the includes list needed to be changed.\n",
    "    body = r\"\"\"\n",
    "    // === INITIALIZATION ===\n",
    "    printf(\"Initializing %ld photon states for batch integration...\\n\", num_rays);\n",
    "    PhotonState *all_photons = (PhotonState *)malloc(sizeof(PhotonState) * num_rays);\n",
    "    if (all_photons == NULL) { fprintf(stderr, \"Error: Failed to allocate memory for photon states.\\n\"); exit(1); }\n",
    "    long int active_photons = num_rays;\n",
    "\n",
    "    const double camera_pos[3] = {commondata->camera_pos_x, commondata->camera_pos_y, commondata->camera_pos_z};\n",
    "    const double window_center[3] = {commondata->window_center_x, commondata->window_center_y, commondata->window_center_z};\n",
    "    double n_z[3] = {window_center[0] - camera_pos[0], window_center[1] - camera_pos[1], window_center[2] - camera_pos[2]};\n",
    "    double mag_n_z = sqrt(SQR(n_z[0]) + SQR(n_z[1]) + SQR(n_z[2]));\n",
    "    for(int i=0; i<3; i++) n_z[i] /= mag_n_z;\n",
    "    const double guide_up[3] = {commondata->window_up_vec_x, commondata->window_up_vec_y, commondata->window_up_vec_z};\n",
    "    double n_x[3] = {n_z[1]*guide_up[2] - n_z[2]*guide_up[1], n_z[2]*guide_up[0] - n_z[0]*guide_up[2], n_z[0]*guide_up[1] - n_z[1]*guide_up[0]};\n",
    "    double mag_n_x = sqrt(SQR(n_x[0]) + SQR(n_x[1]) + SQR(n_x[2]));\n",
    "    if (mag_n_x < 1e-9) {\n",
    "        double alternative_up[3] = {0.0, 1.0, 0.0};\n",
    "        if (fabs(n_z[1]) > 0.999) { alternative_up[1] = 0.0; alternative_up[2] = 1.0; }\n",
    "        n_x[0] = alternative_up[1]*n_z[2] - alternative_up[2]*n_z[1];\n",
    "        n_x[1] = alternative_up[2]*n_z[0] - alternative_up[0]*n_z[2];\n",
    "        n_x[2] = alternative_up[0]*n_z[1] - alternative_up[1]*n_z[0];\n",
    "        mag_n_x = sqrt(SQR(n_x[0]) + SQR(n_x[1]) + SQR(n_x[2]));\n",
    "    }\n",
    "    for(int i=0; i<3; i++) n_x[i] /= mag_n_x;\n",
    "    double n_y[3] = {n_z[1]*n_x[2] - n_z[2]*n_x[1], n_z[2]*n_x[0] - n_z[0]*n_x[2], n_z[0]*n_x[1] - n_z[1]*n_x[0]};\n",
    "\n",
    "    #pragma omp parallel for\n",
    "    for (long int i = 0; i < num_rays; i++) {\n",
    "        const int j = i / commondata->scan_density;\n",
    "        const int k = i % commondata->scan_density;\n",
    "        const double x_pix = -commondata->window_size/2.0 + (k + 0.5) * (commondata->window_size / commondata->scan_density);\n",
    "        const double y_pix = -commondata->window_size/2.0 + (j + 0.5) * (commondata->window_size / commondata->scan_density);\n",
    "        double target_pos[3] = {window_center[0] + x_pix*n_x[0] + y_pix*n_y[0],\n",
    "                                 window_center[1] + x_pix*n_x[1] + y_pix*n_y[1],\n",
    "                                 window_center[2] + x_pix*n_x[2] + y_pix*n_y[2]};\n",
    "        set_initial_conditions_cartesian(commondata, params, metric, camera_pos, target_pos, all_photons[i].y);\n",
    "        all_photons[i].y[0] += commondata->t_start;\n",
    "        all_photons[i].lambda = 0.0;\n",
    "        all_photons[i].d_lambda = 10.0;\n",
    "        all_photons[i].status = ACTIVE;\n",
    "        for(int ii=0; ii<9; ++ii) { all_photons[i].y_p[ii] = all_photons[i].y[ii]; all_photons[i].y_p_p[ii] = all_photons[i].y[ii]; }\n",
    "        all_photons[i].lambda_p = all_photons[i].lambda; all_photons[i].lambda_p_p = all_photons[i].lambda;\n",
    "        plane_event_params window_params = {{n_z[0], n_z[1], n_z[2]}, n_z[0]*window_center[0] + n_z[1]*window_center[1] + n_z[2]*window_center[2]};\n",
    "        all_photons[i].on_positive_side_of_window_prev = (plane_event_func(all_photons[i].y, &window_params) > 0);\n",
    "        plane_event_params source_params = {{commondata->source_plane_normal_x, commondata->source_plane_normal_y, commondata->source_plane_normal_z},\n",
    "                                            commondata->source_plane_center_x*commondata->source_plane_normal_x + commondata->source_plane_center_y*commondata->source_plane_normal_y + commondata->source_plane_center_z*commondata->source_plane_normal_z};\n",
    "        all_photons[i].on_positive_side_of_source_prev = (plane_event_func(all_photons[i].y, &source_params) > 0);\n",
    "        all_photons[i].source_event_data.found = false;\n",
    "        all_photons[i].window_event_data.found = false;\n",
    "    }\n",
    "\n",
    "    PriorityQueue pq;\n",
    "    heap_init(&pq, num_rays);\n",
    "    for (long int i = 0; i < num_rays; i++) { heap_push(&pq, -all_photons[i].y[0], i); }\n",
    "\n",
    "    FILE *log_fp = fopen(\"batch_stats.txt\", \"w\");\n",
    "    if (log_fp) fprintf(log_fp, \"# Iteration\\tActivePhotons\\tBundleSize\\tBundleTime\\tBundleTimeRange\\n\");\n",
    "    int iteration = 0;\n",
    "    printf(\"Starting batch integration loop...\\n\");\n",
    "\n",
    "    while (active_photons > 0) {\n",
    "        if (pq.size == 0) break;\n",
    "        HeapNode earliest = heap_pop(&pq);\n",
    "        double bundle_target_time = -earliest.time;\n",
    "        double time_tolerance = fmax(all_photons[earliest.photon_index].d_lambda * 0.05, 1e-2);\n",
    "        int bundle_indices[BUNDLE_CAPACITY];\n",
    "        bundle_indices[0] = earliest.photon_index;\n",
    "        int bundle_size = 1;\n",
    "        while (pq.size > 0 && -pq.nodes[0].time > bundle_target_time - time_tolerance && bundle_size < BUNDLE_CAPACITY) {\n",
    "            bundle_indices[bundle_size++] = heap_pop(&pq).photon_index;\n",
    "        }\n",
    "\n",
    "        #pragma omp parallel\n",
    "        {\n",
    "            gsl_odeiv2_step *step = gsl_odeiv2_step_alloc(gsl_odeiv2_step_rkf45, 9);\n",
    "            gsl_odeiv2_control *control = gsl_odeiv2_control_yp_new(1e-8, 1e-8);\n",
    "            gsl_odeiv2_evolve *evolve = gsl_odeiv2_evolve_alloc(9);\n",
    "            gsl_params gsl_parameters = {commondata, params, metric};\n",
    "            gsl_odeiv2_system sys = {ode_gsl_wrapper, NULL, 9, &gsl_parameters};\n",
    "\n",
    "            #pragma omp for\n",
    "            for (int i = 0; i < bundle_size; i++) {\n",
    "                int photon_idx = bundle_indices[i];\n",
    "                for(int ii=0; ii<9; ++ii) { all_photons[photon_idx].y_p_p[ii] = all_photons[photon_idx].y_p[ii]; all_photons[photon_idx].y_p[ii] = all_photons[photon_idx].y[ii]; }\n",
    "                all_photons[photon_idx].lambda_p_p = all_photons[photon_idx].lambda_p; all_photons[photon_idx].lambda_p = all_photons[photon_idx].lambda;\n",
    "\n",
    "                int status = calculate_and_apply_single_step(&all_photons[photon_idx], &sys, step, control, evolve);\n",
    "\n",
    "                const double r_sq = SQR(all_photons[photon_idx].y[1]) + SQR(all_photons[photon_idx].y[2]) + SQR(all_photons[photon_idx].y[3]);\n",
    "                if (status != GSL_SUCCESS || r_sq > SQR(commondata->r_escape) || fabs(all_photons[photon_idx].y[0]) > commondata->t_integration_max) {\n",
    "                    all_photons[photon_idx].status = (r_sq > SQR(commondata->r_escape)) ? TERMINATED_SPHERE : TERMINATED_FAILURE;\n",
    "                    #pragma omp atomic\n",
    "                    active_photons--;\n",
    "                } else {\n",
    "                    event_detection_manager(all_photons[photon_idx].y_p_p, all_photons[photon_idx].y_p, all_photons[photon_idx].y,\n",
    "                                            all_photons[photon_idx].lambda_p_p, all_photons[photon_idx].lambda_p, all_photons[photon_idx].lambda,\n",
    "                                            commondata, &all_photons[photon_idx].on_positive_side_of_window_prev, &all_photons[photon_idx].on_positive_side_of_source_prev,\n",
    "                                            &all_photons[photon_idx].window_event_data, &all_photons[photon_idx].source_event_data);\n",
    "                    \n",
    "                    if (all_photons[photon_idx].source_event_data.found) {\n",
    "                        blueprint_data_t temp_blueprint;\n",
    "                        if (handle_source_plane_intersection(&all_photons[photon_idx].source_event_data, commondata, &temp_blueprint)) {\n",
    "                            all_photons[photon_idx].status = TERMINATED_SOURCE;\n",
    "                            #pragma omp atomic\n",
    "                            active_photons--;\n",
    "                        } else {\n",
    "                            all_photons[photon_idx].source_event_data.found = false;\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            gsl_odeiv2_evolve_free(evolve); gsl_odeiv2_control_free(control); gsl_odeiv2_step_free(step);\n",
    "        }\n",
    "\n",
    "        double min_bundle_time = bundle_target_time, max_bundle_time = bundle_target_time;\n",
    "        for (int i = 0; i < bundle_size; i++) {\n",
    "            int photon_idx = bundle_indices[i];\n",
    "            if (all_photons[photon_idx].status == ACTIVE) {\n",
    "                heap_push(&pq, -all_photons[photon_idx].y[0], photon_idx);\n",
    "                if (all_photons[photon_idx].y[0] < max_bundle_time) max_bundle_time = all_photons[photon_idx].y[0];\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if (log_fp) fprintf(log_fp, \"%d\\t%ld\\t%d\\t%.6e\\t%.6e\\n\", iteration, active_photons, bundle_size, bundle_target_time, bundle_target_time - max_bundle_time);\n",
    "        if (iteration % 100 == 0) {\n",
    "            printf(\"\\rIteration: %d, Active Photons: %ld, Last Bundle Size: %d, Current Time: %.4f M\", iteration, active_photons, bundle_size, bundle_target_time);\n",
    "            fflush(stdout);\n",
    "        }\n",
    "        iteration++;\n",
    "    }\n",
    "\n",
    "    printf(\"\\nBatch integration finished after %d iterations.\\n\", iteration);\n",
    "    if (log_fp) fclose(log_fp);\n",
    "\n",
    "    printf(\"Processing final states and populating blueprint buffer...\\n\");\n",
    "    #pragma omp parallel for\n",
    "    for (long int i = 0; i < num_rays; i++) {\n",
    "        results_buffer[i] = calculate_and_fill_blueprint_data_universal(\n",
    "            commondata, params,\n",
    "            all_photons[i].status,\n",
    "            &all_photons[i].window_event_data,\n",
    "            &all_photons[i].source_event_data,\n",
    "            all_photons[i].y,\n",
    "            window_center, n_x, n_y,\n",
    "            (const double[3]){commondata->source_plane_center_x, commondata->source_plane_center_y, commondata->source_plane_center_z},\n",
    "            (const double[3]){commondata->source_plane_normal_x, commondata->source_plane_normal_y, commondata->source_plane_normal_z},\n",
    "            (const double[3]){commondata->source_up_vec_x, commondata->source_up_vec_y, commondata->source_up_vec_z}\n",
    "        );\n",
    "    }\n",
    "\n",
    "    heap_free(&pq);\n",
    "    free(all_photons);\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)\n",
    "    print(f\"    ... Registered C orchestrator: {name}().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdf4ce",
   "metadata": {},
   "source": [
    "<a id='gsl_wrapper'></a>\n",
    "### 7.d: The GSL Wrapper Function\n",
    "\n",
    "The GNU Scientific Library (GSL) provides powerful, general-purpose routines for solving systems of Ordinary Differential Equations (ODEs). To use them, we must provide a C function that calculates the RHS of our ODE system and conforms to a specific function pointer signature required by the library. The `ode_gsl_wrapper` C function serves as this critical **adapter** or **bridge** between the generic GSL interface and our specialized project code.\n",
    "\n",
    "This Python function registers the C function that performs these steps in order every time the GSL solver takes a time step:\n",
    "\n",
    "1.  **Unpack Parameters**: It receives a generic `void *params` pointer from the GSL solver. Its first action is to cast this pointer back to its true type, `gsl_params *`, which is our custom \"carrier\" struct. This gives the function access to the `commondata`, `params`, and `metric` structs needed by our physics routines.\n",
    "2.  **Call Metric and Connection Dispatchers**: It declares empty `metric_struct` and `connection_struct` containers on the stack. It then calls our high-level dispatchers (`g4DD_metric()` and `connections()`) to fill these structs with the correct physical values for the current point in spacetime `y`.\n",
    "3.  **Call `calculate_ode_rhs()` Engine**: It then passes the current state vector `y` and the now-filled `g4DD` and `conn` structs to our RHS engine. This engine computes the derivatives and stores them in the output array `f`, which is the array GSL uses for the RHS values.\n",
    "4.  **Return Success**: Finally, it returns `GSL_SUCCESS`, signaling to the GSL solver that the RHS calculation was completed correctly.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec563f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_granular_gsl_engines():\n",
    "    \"\"\"\n",
    "    Generates the C engines for GSL control. This version (v2.9.2) adds the\n",
    "    necessary GSL headers to the includes list of functions that use GSL types.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating granular GSL control engines (v2.9.2)...\")\n",
    "\n",
    "    # Engine 1: calculate_and_apply_single_step\n",
    "    # *** CORRECTED: Added GSL headers to the includes list. ***\n",
    "    includes_step = [\n",
    "        \"BHaH_defines.h\",\n",
    "        \"BHaH_function_prototypes.h\"\n",
    "    ]\n",
    "    desc_step = r\"\"\"@brief Applies a single adaptive step of the GSL integrator.\"\"\"\n",
    "    name_step = \"calculate_and_apply_single_step\"\n",
    "    cfunc_type_step = \"int\"\n",
    "    params_step = r\"\"\"\n",
    "    PhotonState *photon,\n",
    "    gsl_odeiv2_system *sys,\n",
    "    gsl_odeiv2_step *step,\n",
    "    gsl_odeiv2_control *control,\n",
    "    gsl_odeiv2_evolve *evolve\n",
    "    \"\"\"\n",
    "    body_step = r\"\"\"\n",
    "    int status = gsl_odeiv2_evolve_apply(\n",
    "        evolve, control, step, sys,\n",
    "        &photon->lambda,\n",
    "        1e10,\n",
    "        &photon->d_lambda,\n",
    "        photon->y\n",
    "    );\n",
    "    return status;\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes_step, desc=desc_step, cfunc_type=cfunc_type_step,\n",
    "        name=name_step, params=params_step, body=body_step\n",
    "    )\n",
    "    print(f\"    ... Registered C engine: {name_step}().\")\n",
    "\n",
    "    # Engine 2: ode_gsl_wrapper (NO Caching for Phase 1)\n",
    "    # This function also needs the GSL headers.\n",
    "    includes_wrapper = [\n",
    "        \"BHaH_defines.h\",\n",
    "        \"BHaH_function_prototypes.h\"\n",
    "    ]\n",
    "    desc_wrapper = r\"\"\"@brief Acts as an adapter between the GSL solver and our C functions.\"\"\"\n",
    "    name_wrapper = \"ode_gsl_wrapper\"\n",
    "    cfunc_type_wrapper = \"int\"\n",
    "    params_wrapper = \"double lambda, const double y[9], double f[9], void *params\"\n",
    "    body_wrapper = r\"\"\"\n",
    "    (void)lambda;\n",
    "\n",
    "    gsl_params *gsl_parameters = (gsl_params *)params;\n",
    "    metric_struct g4DD;\n",
    "    connection_struct conn;\n",
    "\n",
    "    g4DD_metric(gsl_parameters->commondata, gsl_parameters->params, gsl_parameters->metric, y, &g4DD);\n",
    "    connections(gsl_parameters->commondata, gsl_parameters->params, gsl_parameters->metric, y, &conn);\n",
    "    calculate_ode_rhs(y, &g4DD, &conn, f);\n",
    "\n",
    "    return GSL_SUCCESS;\n",
    "    \"\"\"\n",
    "    cfc.CFunction_dict.pop(name_wrapper, None)\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes_wrapper, desc=desc_wrapper, cfunc_type=cfunc_type_wrapper,\n",
    "        name=name_wrapper, params=params_wrapper, body=body_wrapper\n",
    "    )\n",
    "    print(f\"    ... Registered C engine: {name_wrapper}() (Phase 1, No Caching).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41563da7",
   "metadata": {},
   "source": [
    "<a id='data_processing'></a>\n",
    "### 7.f: Data Processing Engine\n",
    "\n",
    "This Python function generates the C engine `calculate_and_fill_blueprint_data_universal()`. Its sole purpose is to process the raw event data from a single completed ray trace and compute the final quantities that are saved to the `blueprint.bin` file. It acts as a translator, converting the 3D intersection points from the integration into a 2D coordinate system on both the window and source planes.\n",
    "\n",
    "The function orchestrates the following calculations:\n",
    "\n",
    "1.  **Window Plane Projection**: It takes the 3D Cartesian intersection point on the window plane and projects it onto the local 2D orthonormal basis vectors of the window (`n_x`, `n_y`) to get the final `(y_w, z_w)` coordinates.\n",
    "2.  **Source Plane Projection**: It performs a similar calculation for the source plane, projecting the 3D intersection point onto the source plane's local 2D basis to get the `(y_s, z_s)` coordinates. This basis is constructed from the source plane's normal vector and its \"up\" vector.\n",
    "3.  **Redshift Calculation**: It computes the gravitational redshift factor by calling the `g4DD_metric` dispatcher at both the window and source intersection points to get the $g_{00}$ component of the metric. The redshift is then given by $\\sqrt{g_{00}(\\text{window}) / g_{00}(\\text{source})}$.\n",
    "4.  **Return Struct**: It populates and returns a `blueprint_data_t` struct containing all these calculated values, along with a `found` flag that is set to `true` only if all calculations are successful and finite.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cee235eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_8_Python_to_C_via_N NRPy.ipynb\n",
    "# In cell [cee235eb]\n",
    "\n",
    "def calculate_and_fill_blueprint_data_universal():\n",
    "    \"\"\"\n",
    "    Generates the C engine to process event data. This version (v2.9.1) uses\n",
    "    the correct, consolidated PhotonStatus enum type.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C finalization engine: calculate_and_fill_blueprint_data_universal (v2.9.1)...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Processes raw event data to compute final blueprint quantities based on termination type.\"\"\"\n",
    "    name = \"calculate_and_fill_blueprint_data_universal\"\n",
    "    cfunc_type = \"blueprint_data_t\"\n",
    "    \n",
    "    # *** CORRECTED: Changed 'termination_type_t' to 'PhotonStatus' ***\n",
    "    params = \"\"\"const commondata_struct *restrict commondata, const params_struct *restrict params,\n",
    "                const PhotonStatus term_status,\n",
    "                const event_data_struct *restrict window_event,\n",
    "                const event_data_struct *restrict source_event,\n",
    "                const double final_y_state[9],\n",
    "                const double window_center[3], const double n_x[3], const double n_y[3],\n",
    "                const double source_plane_center[3], const double source_plane_normal[3],\n",
    "                const double source_up_vector[3]\"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    blueprint_data_t result = {0};\n",
    "    // *** CORRECTED: Use the correct enum member from PhotonStatus ***\n",
    "    result.termination_type = term_status;\n",
    "\n",
    "    if (window_event->found) {\n",
    "        const double pos_w_cart[3] = {window_event->y_event[1], window_event->y_event[2], window_event->y_event[3]};\n",
    "        const double vec_w[3] = {pos_w_cart[0] - window_center[0], pos_w_cart[1] - window_center[1], pos_w_cart[2] - window_center[2]};\n",
    "        result.y_w = vec_w[0]*n_x[0] + vec_w[1]*n_x[1] + vec_w[2]*n_x[2];\n",
    "        result.z_w = vec_w[0]*n_y[0] + vec_w[1]*n_y[1] + vec_w[2]*n_y[2];\n",
    "        result.L_w = window_event->y_event[8];\n",
    "        result.t_w = window_event->t_event;\n",
    "    }\n",
    "\n",
    "    if (term_status == TERMINATED_SOURCE) {\n",
    "        double s_z[3] = {source_plane_normal[0], source_plane_normal[1], source_plane_normal[2]};\n",
    "        double s_x[3] = {source_up_vector[1]*s_z[2] - source_up_vector[2]*s_z[1],\n",
    "                         source_up_vector[2]*s_z[0] - source_up_vector[0]*s_z[2],\n",
    "                         source_up_vector[0]*s_z[1] - source_up_vector[1]*s_z[0]};\n",
    "        double mag_s_x = sqrt(SQR(s_x[0]) + SQR(s_x[1]) + SQR(s_x[2]));\n",
    "        if (mag_s_x < 1e-9) {\n",
    "            double temp_up[3] = {1.0, 0.0, 0.0};\n",
    "            if (fabs(s_z[0]) > 0.999) { temp_up[0] = 0.0; temp_up[1] = 1.0; temp_up[2] = 0.0; }\n",
    "            s_x[0] = temp_up[1]*s_z[2] - temp_up[2]*s_z[1];\n",
    "            s_x[1] = temp_up[2]*s_z[0] - temp_up[0]*s_z[2];\n",
    "            s_x[2] = temp_up[0]*s_z[1] - temp_up[1]*s_z[0];\n",
    "            mag_s_x = sqrt(SQR(s_x[0]) + SQR(s_x[1]) + SQR(s_x[2]));\n",
    "        }\n",
    "        const double inv_mag_s_x = 1.0 / mag_s_x;\n",
    "        s_x[0] *= inv_mag_s_x; s_x[1] *= inv_mag_s_x; s_x[2] *= inv_mag_s_x;\n",
    "        double s_y[3] = {s_z[1]*s_x[2] - s_z[2]*s_x[1], s_z[2]*s_x[0] - s_z[0]*s_x[2], s_z[0]*s_x[1] - s_z[1]*s_x[0]};\n",
    "        const double pos_s_cart[3] = {source_event->y_event[1], source_event->y_event[2], source_event->y_event[3]};\n",
    "        const double vec_s[3] = {pos_s_cart[0] - source_plane_center[0], pos_s_cart[1] - source_plane_center[1], pos_s_cart[2] - source_plane_center[2]};\n",
    "        result.y_s = vec_s[0]*s_x[0] + vec_s[1]*s_x[1] + vec_s[2]*s_x[2];\n",
    "        result.z_s = vec_s[0]*s_y[0] + vec_s[1]*s_y[1] + vec_s[2]*s_y[2];\n",
    "        result.L_s = source_event->y_event[8];\n",
    "        result.t_s = source_event->t_event;\n",
    "    } else if (term_status == TERMINATED_SPHERE) {\n",
    "        const double x = final_y_state[1];\n",
    "        const double y = final_y_state[2];\n",
    "        const double z = final_y_state[3];\n",
    "        const double r = sqrt(x*x + y*y + z*z);\n",
    "        if (r > 1e-9) {\n",
    "            result.final_theta = acos(z / r);\n",
    "            result.final_phi = atan2(y, x);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result;\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, cfunc_type=cfunc_type, name=name, params=params, body=body, include_CodeParameters_h=True)\n",
    "    print(f\"    ... Registered C engine: {name}().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4eaf0",
   "metadata": {},
   "source": [
    "<a id='main_entry_point'></a>\n",
    "### 7.h: The `main()` C Function Entry Point\n",
    "\n",
    "This function registers the C `main()` function, which serves as the entry point for the entire executable program. In our final architecture, `main()` is a pure **orchestrator**; it contains no physics logic itself. Instead, it calls other functions to set up the simulation, run the integration, and handle the output.\n",
    "\n",
    "The `main` function performs the full sequence of operations required for a complete ray-tracing run:\n",
    "\n",
    "1.  **Declare Data Structures**: It declares instances of all necessary structs (`commondata_struct`, `params_struct`, `metric_params`).\n",
    "2.  **Initialize Parameters**: It initializes the parameters in a two-step process that ensures runtime flexibility:\n",
    "    *   First, it calls `commondata_struct_set_to_default()` to populate the `commondata` struct with the default values that were compiled into the executable (e.g., `M_scale = 1.0`).\n",
    "    *   Next, it calls `cmdline_input_and_parfile_parser()`. This function reads the project's `.par` file and any command-line arguments, and it will **override** the compiled-in defaults with any values provided by the user at runtime.\n",
    "3.  **Print Simulation Parameters**: It prints a detailed summary of all the final simulation parameters to the console. This is crucial for verification and for creating a record of the exact settings used for a given run. It also suggests a descriptive output filename and prints the full command-line arguments needed to reproduce the run exactly.\n",
    "4.  **Run Scanner Loop**: It calls the main orchestrator, `run_scan_and_save_blueprint_universal()`, to execute the full ray-tracing scan.\n",
    "5.  **Cleanup**: It prints a final status message and returns 0, indicating successful execution.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4661b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Re-registers the main() C function with all syntax and logical errors corrected.\n",
    "    This is the definitive version (v2.8).\n",
    "    \"\"\"\n",
    "    print(\" -> Updating main() to call batch integrator (v2.8)...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"stdio.h\", \"stdlib.h\"]\n",
    "    desc = r\"\"\"@brief Main entry point for the batch integrator.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"main\"\n",
    "    params = \"int argc, const char *argv[]\"\n",
    "    # *** CORRECTED AND COMPLETE BODY ***\n",
    "    body = r\"\"\"\n",
    "    commondata_struct commondata;\n",
    "    params_struct params;\n",
    "    metric_params metric;\n",
    "\n",
    "    commondata_struct_set_to_default(&commondata);\n",
    "    // This project is gridless and does not use the params_struct.\n",
    "    // The call to params_struct_set_to_default is therefore unnecessary and removed\n",
    "    // to prevent a segmentation fault from dereferencing a NULL griddata pointer.\n",
    "    \n",
    "    cmdline_input_and_parfile_parser(&commondata, argc, argv);\n",
    "\n",
    "    metric.type = (commondata.a_spin == 0.0) ? Schwarzschild : Kerr;\n",
    "    \n",
    "    printf(\"=============================================\\n\");\n",
    "    printf(\"  Photon Geodesic Integrator (Batch Mode)  \\n\");\n",
    "    printf(\"=============================================\\n\");\n",
    "    printf(\"Metric: %s (a=%.3f)\\n\", (metric.type == Kerr) ? \"Kerr\" : \"Schwarzschild\", commondata.a_spin);\n",
    "    printf(\"Scan Resolution: %d x %d\\n\", commondata.scan_density, commondata.scan_density);\n",
    "    printf(\"Initial Time (t_start): %.2f M\\n\", commondata.t_start);\n",
    "\n",
    "    if (commondata.debug_mode) {\n",
    "        printf(\"\\n>>> RUNNING IN SINGLE-RAY DEBUG MODE <<<\\n\");\n",
    "        printf(\"Debug mode is not supported in this batching main() function.\\n\");\n",
    "    } else {\n",
    "        long int num_rays = (long int)commondata.scan_density * commondata.scan_density;\n",
    "        blueprint_data_t *results_buffer = (blueprint_data_t *)malloc(sizeof(blueprint_data_t) * num_rays);\n",
    "        if (results_buffer == NULL) {\n",
    "            fprintf(stderr, \"Fatal Error: Could not allocate memory for blueprint results buffer.\\n\");\n",
    "            exit(1);\n",
    "        }\n",
    "\n",
    "\n",
    "        batch_integrator(&commondata, &params, &metric, num_rays, results_buffer);\n",
    "\n",
    "        printf(\"Scan finished. Writing %ld ray results to light_blueprint.bin...\\n\", num_rays);\n",
    "        FILE *fp_blueprint = fopen(\"light_blueprint.bin\", \"wb\");\n",
    "        if (fp_blueprint == NULL) {\n",
    "            perror(\"Error opening blueprint file for writing\");\n",
    "            exit(1);\n",
    "        }\n",
    "        fwrite(results_buffer, sizeof(blueprint_data_t), num_rays, fp_blueprint);\n",
    "        fclose(fp_blueprint);\n",
    "        free(results_buffer);\n",
    "    }\n",
    "\n",
    "    printf(\"\\nRun complete.\\n\");\n",
    "    return 0;\n",
    "    \"\"\"\n",
    "    # De-register any old version before registering the new one.\n",
    "    cfc.CFunction_dict.pop(name, None)\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, cfunc_type=cfunc_type, name=name, params=params, body=body)\n",
    "    print(\"    ... main() has been updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8cb55",
   "metadata": {},
   "source": [
    "<a id='assemble_project'></a>\n",
    "# Step 8: Project Assembly and Compilation\n",
    "\n",
    "This is the final phase of the notebook for C code generation. It brings all the previously defined pieces together to construct the complete, compilable C project.\n",
    "\n",
    "<a id='register_structs'></a>\n",
    "### 8.a: Custom Data Structures\n",
    "\n",
    "This function defines all the necessary C `struct` and `enum` types for the project. It then registers them with the BHaH infrastructure, which makes these custom data types available to all other C files via the master header `BHaH_defines.h`.\n",
    "\n",
    "The function `register_custom_structures_and_params` performs the following actions:\n",
    "1.  **Generates `connection_struct`**: It programmatically creates the C `typedef` for the `connection_struct`. This struct contains 40 `double` members to hold the unique Christoffel symbols ($\\Gamma^{\\alpha}_{\\mu\\nu}$).\n",
    "2.  **Generates `metric_struct`**: It follows the same programmatic pattern to create the `metric_struct`, which contains 10 `double` members to hold the unique components of the metric tensor ($g_{\\mu\\nu}$).\n",
    "3.  **Defines Other Structs**: It defines the C `typedef`s for all other data structures (`Metric_t` enum, `metric_params`, the GSL \"carrier\" struct `gsl_params`, the `event_data_struct` for interpolation results, and the final `blueprint_data_t` for output) as Python strings.\n",
    "4.  **Registers with `BHaH`**: For each `struct` or `enum`, it calls `Bdefines_h.register_BHaH_defines()`. This function adds the C code string to a global registry. When `Bdefines_h.output_BHaH_defines_h()` is called later in the build process, it will automatically find and include all these registered definitions in the final header file.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank3(...)`**: Previously introduced. Used here to programmatically generate the list of C variable names for the members of the `connection_struct`.\n",
    "*   **`nrpy.infrastructures.BHaH.BHaH_defines_h.register_BHaH_defines(name, C_code_string)`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/BHaH_defines_h.py`\n",
    "    *   **Description**: This function adds a given C-code string (which should define a `struct`, `enum`, or other C-level type) to a global registry, associated with a given name. This registry is later used to generate the master `BHaH_defines.h` header file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef03c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_8_Python_to_C_via_NRPy.ipynb\n",
    "# This function REPLACES the previous version of register_custom_structures_and_params().\n",
    "\n",
    "def register_custom_structures_and_params():\n",
    "    \"\"\"\n",
    "    Generates and registers all core C data structures for the project in a\n",
    "    single, ordered block. This definitive version (v2.10) uses a more\n",
    "    compact, readable format for the metric and connection structs.\n",
    "    \"\"\"\n",
    "    print(\" -> Registering all core C data structures (v2.10)...\")\n",
    "\n",
    "    # Programmatically generate the compact list of members for the structs.\n",
    "    # This is robust and avoids manual typos.\n",
    "    metric_components = [f\"g{nu}{mu}\" for nu in range(4) for mu in range(nu, 4)]\n",
    "    metric_struct_str = \"typedef struct { double \" + \", \".join(metric_components) + \"; } metric_struct;\"\n",
    "\n",
    "    connection_components = [f\"Gamma4UDD{i}{j}{k}\" for i in range(4) for j in range(4) for k in range(j, 4)]\n",
    "    connections_struct_str = \"typedef struct { double \" + \", \".join(connection_components) + \"; } connection_struct;\"\n",
    "\n",
    "    # This single string contains all necessary definitions in the correct order.\n",
    "    core_structs_c_code = rf\"\"\"\n",
    "// =============================================================================\n",
    "// Core Metric and GSL Parameter Structs\n",
    "// =============================================================================\n",
    "#include <gsl/gsl_errno.h>\n",
    "#include <gsl/gsl_odeiv2.h>\n",
    "\n",
    "// Compact struct definitions, generated programmatically for correctness.\n",
    "{metric_struct_str}\n",
    "{connections_struct_str}\n",
    "\n",
    "typedef enum {{ Schwarzschild, Kerr, Numerical, Schwarzschild_Standard }} Metric_t;\n",
    "typedef struct {{ Metric_t type; }} metric_params;\n",
    "typedef struct {{ const commondata_struct *commondata; const params_struct *params; const metric_params *metric; }} gsl_params;\n",
    "typedef double (*event_function_t)(const double y[9], void *event_params);\n",
    "\n",
    "// =============================================================================\n",
    "// Batch Integration and Output Structs\n",
    "// =============================================================================\n",
    "#define CACHE_LINE_SIZE 64\n",
    "#define BUNDLE_CAPACITY 20000\n",
    "\n",
    "// 1. Define the PhotonStatus enum FIRST.\n",
    "typedef enum {{\n",
    "    ACTIVE,\n",
    "    TERMINATED_DISK,\n",
    "    TERMINATED_SOURCE,\n",
    "    TERMINATED_SPHERE,\n",
    "    TERMINATED_FAILURE\n",
    "}} PhotonStatus;\n",
    "\n",
    "// 2. Define the blueprint_data_t struct, which USES PhotonStatus.\n",
    "typedef struct {{\n",
    "    PhotonStatus termination_type;\n",
    "    double y_w, z_w;\n",
    "    double stokes_I, lambda_observed;\n",
    "    double y_s, z_s;\n",
    "    double final_theta, final_phi;\n",
    "    double L_w, t_w, L_s, t_s;\n",
    "}} __attribute__((packed)) blueprint_data_t;\n",
    "\n",
    "// 3. Define the event_data_struct.\n",
    "typedef struct {{\n",
    "    bool found;\n",
    "    double lambda_event, t_event;\n",
    "    double y_event[9];\n",
    "}} event_data_struct;\n",
    "\n",
    "// 4. Define the PhotonState struct, which USES PhotonStatus and event_data_struct.\n",
    "typedef struct {{\n",
    "    double y[9];\n",
    "    double lambda, d_lambda;\n",
    "    PhotonStatus status;\n",
    "    double y_p_p[9], y_p[9];\n",
    "    double lambda_p_p, lambda_p;\n",
    "    bool on_positive_side_of_window_prev;\n",
    "    bool on_positive_side_of_source_prev;\n",
    "    event_data_struct source_event_data;\n",
    "    event_data_struct window_event_data;\n",
    "    char _padding[CACHE_LINE_SIZE - (sizeof(double)*53 + sizeof(PhotonStatus) + sizeof(bool)*4) % CACHE_LINE_SIZE];\n",
    "}} __attribute__((aligned(CACHE_LINE_SIZE))) PhotonState;\n",
    "\n",
    "// --- Min-Heap (Priority Queue) Implementation ---\n",
    "typedef struct {{ double time; int photon_index; }} HeapNode;\n",
    "typedef struct {{ HeapNode *nodes; int size; int capacity; }} PriorityQueue;\n",
    "static inline void heap_swap(HeapNode *a, HeapNode *b) {{ HeapNode temp = *a; *a = *b; *b = temp; }}\n",
    "static inline void heap_init(PriorityQueue *pq, int capacity) {{\n",
    "    pq->nodes = (HeapNode *)malloc(sizeof(HeapNode) * capacity);\n",
    "    if (pq->nodes == NULL) {{ fprintf(stderr, \"Error: Failed to allocate memory for priority queue.\\n\"); exit(1); }}\n",
    "    pq->size = 0;\n",
    "    pq->capacity = capacity;\n",
    "}}\n",
    "static inline void heap_free(PriorityQueue *pq) {{ if (pq->nodes != NULL) {{ free(pq->nodes); pq->nodes = NULL; }} }}\n",
    "static inline void heapify_up(PriorityQueue *pq, int index) {{\n",
    "    while (index > 0) {{\n",
    "        int parent_index = (index - 1) / 2;\n",
    "        if (pq->nodes[index].time < pq->nodes[parent_index].time) {{\n",
    "            heap_swap(&pq->nodes[index], &pq->nodes[parent_index]);\n",
    "            index = parent_index;\n",
    "        }} else {{ break; }}\n",
    "    }}\n",
    "}}\n",
    "static inline void heap_push(PriorityQueue *pq, double time, int photon_index) {{\n",
    "    if (pq->size >= pq->capacity) {{ fprintf(stderr, \"Error: Priority queue overflow.\\n\"); return; }}\n",
    "    pq->nodes[pq->size].time = time;\n",
    "    pq->nodes[pq->size].photon_index = photon_index;\n",
    "    heapify_up(pq, pq->size);\n",
    "    pq->size++;\n",
    "}}\n",
    "static inline void heapify_down(PriorityQueue *pq, int index) {{\n",
    "    int min_index = index;\n",
    "    while (1) {{\n",
    "        int left_child = 2 * index + 1;\n",
    "        int right_child = 2 * index + 2;\n",
    "        if (left_child < pq->size && pq->nodes[left_child].time < pq->nodes[min_index].time) min_index = left_child;\n",
    "        if (right_child < pq->size && pq->nodes[right_child].time < pq->nodes[min_index].time) min_index = right_child;\n",
    "        if (min_index == index) break;\n",
    "        heap_swap(&pq->nodes[index], &pq->nodes[min_index]);\n",
    "        index = min_index;\n",
    "    }}\n",
    "}}\n",
    "static inline HeapNode heap_pop(PriorityQueue *pq) {{\n",
    "    if (pq->size == 0) {{\n",
    "        HeapNode empty_node = {{-1.0, -1}};\n",
    "        fprintf(stderr, \"Error: Attempted to pop from an empty priority queue.\\n\");\n",
    "        return empty_node;\n",
    "    }}\n",
    "    HeapNode min_node = pq->nodes[0];\n",
    "    pq->nodes[0] = pq->nodes[pq->size - 1];\n",
    "    pq->size--;\n",
    "    if (pq->size > 0) heapify_down(pq, 0);\n",
    "    return min_node;\n",
    "}}\n",
    "\"\"\"\n",
    "    # Use the 'after_general' key to ensure these definitions appear early in BHaH_defines.h\n",
    "    Bdefines_h.register_BHaH_defines(\"after_general\", core_structs_c_code)\n",
    "    print(\"    ... Registered all core data structures in a single, ordered block.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae60808",
   "metadata": {},
   "source": [
    "# Kdtree structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "956502ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_kdtree_c_structs():\n",
    "    \"\"\"\n",
    "    Generates and registers the C structs needed for loading and querying\n",
    "    the pre-processed k-d tree snapshot files.\n",
    "    \"\"\"\n",
    "    print(\" -> Registering C data structures for k-d tree handling...\")\n",
    "\n",
    "    # This C struct must EXACTLY match the Python numpy.dtype and the\n",
    "    # mass_particle_state_t struct from the mass_integrator.\n",
    "    # It represents the data for a single particle in the .kdtree.bin file.\n",
    "    massive_particle_struct_str = r\"\"\"\n",
    "    typedef struct {\n",
    "        int id;\n",
    "        double pos[3];\n",
    "        double u[4];\n",
    "        double lambda_rest;\n",
    "        float j_intrinsic;\n",
    "    } __attribute__((packed)) MassiveParticle;\n",
    "    \"\"\"\n",
    "\n",
    "    # This C struct acts as a \"handle\" to a memory-mapped k-d tree file.\n",
    "    # It holds pointers to the key data arrays within the mapped memory region.\n",
    "    kdtree_handle_struct_str = r\"\"\"\n",
    "    typedef struct {\n",
    "        // Pointers to the data within the memory-mapped file\n",
    "        int32_t*       node_metadata;   // Points to the start of Payload 1 (split axes)\n",
    "        MassiveParticle* particle_data;   // Points to the start of Payload 2 (reordered particles)\n",
    "\n",
    "        // Information from the header\n",
    "        uint64_t num_particles;\n",
    "        uint64_t dimensions;\n",
    "\n",
    "        // For cleanup\n",
    "        void*    original_mmap_ptr; // The original pointer returned by mmap\n",
    "        size_t   file_size;         // The total size of the mapped file\n",
    "    } CustomKDTree;\n",
    "    \"\"\"\n",
    "    \n",
    "    # This struct will be used to manage the \"Winners' Circle\" during the search.\n",
    "    winners_circle_struct_str = r\"\"\"\n",
    "#define MAX_NEIGHBORS 10 // A compile-time max for the nearest-neighbor search\n",
    "    typedef struct {\n",
    "        int indices[MAX_NEIGHBORS];\n",
    "        double sq_distances[MAX_NEIGHBORS];\n",
    "        int count;\n",
    "        int n_wanted;\n",
    "    } WinnersCircle;\n",
    "    \"\"\"\n",
    "\n",
    "    # Register all structs in a single block\n",
    "    Bdefines_h.register_BHaH_defines(\n",
    "        \"kdtree_structs\",\n",
    "        f\"{massive_particle_struct_str}\\n{kdtree_handle_struct_str}\\n{winners_circle_struct_str}\"\n",
    "    )\n",
    "    print(\"    ... Registered C structs: MassiveParticle, CustomKDTree, WinnersCircle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ac485",
   "metadata": {},
   "source": [
    "# Register plane event helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feab1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_plane_event_helpers():\n",
    "    \"\"\"\n",
    "    Generates and registers the C struct and helper function for detecting\n",
    "    generic plane-crossing events. By registering them here, they are made\n",
    "    globally available in BHaH_defines.h.\n",
    "    \"\"\"\n",
    "    print(\" -> Registering global plane event detection helpers...\")\n",
    "\n",
    "    plane_event_helpers_str = r\"\"\"\n",
    "// Struct to hold parameters for a plane-crossing event\n",
    "typedef struct {\n",
    "    double n[3]; // Plane normal vector\n",
    "    double d;    // Plane distance from origin\n",
    "} plane_event_params;\n",
    "\n",
    "// Event function for a generic plane crossing.\n",
    "// This function is now globally visible.\n",
    "static inline double plane_event_func(const double y[9], void *event_params) {\n",
    "    plane_event_params *params = (plane_event_params *)event_params;\n",
    "    // Event is defined as distance to plane = 0\n",
    "    return y[1]*params->n[0] + y[2]*params->n[1] + y[3]*params->n[2] - params->d;\n",
    "}\n",
    "\"\"\"\n",
    "    # Register these definitions to be placed in BHaH_defines.h\n",
    "    Bdefines_h.register_BHaH_defines(\"plane_event_helpers\", plane_event_helpers_str)\n",
    "    print(\"    ... Registered plane_event_params struct and plane_event_func.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583e0f3",
   "metadata": {},
   "source": [
    "<a id='final_build'></a>\n",
    "### 8.b: Final Build Command\n",
    "\n",
    "This is the main execution block of the notebook. It brings all the previously defined Python functions together and calls them in a precise sequence to generate every file needed for the final, compilable C project.\n",
    "\n",
    "The sequence of operations is critical, as later steps depend on the files and registrations created by earlier ones:\n",
    "\n",
    "1.  **Register All Components**: It calls all the C-generating Python functions that we have defined throughout the notebook (`register_custom_structures_and_params`, `g4DD_kerr_schild`, `main_production`, etc.). This populates `nrpy`'s internal library (`cfc.CFunction_dict`) with the complete definitions for all our custom C data structures and functions. At this stage, no files have been written yet; everything exists only in memory.\n",
    "\n",
    "2.  **Generate Parameter Handling Files**: It calls the necessary functions from the BHaH infrastructure to set up the parameter system:\n",
    "    *   `CPs.write_CodeParameters_h_files()`: Generates `set_CodeParameters.h` and its variants.\n",
    "    *   `CPs.register_CFunctions_params_commondata_struct_set_to_default()`: Registers the C functions that initialize the parameter structs with their compiled-in default values.\n",
    "    *   `cmdline_input_and_parfiles.generate_default_parfile()`: Creates the `project_name.par` file.\n",
    "    *   `cmdline_input_and_parfiles.register_CFunction_cmdline_input_and_parfile_parser()`: Registers the C function that reads the `.par` file and command-line arguments at runtime.\n",
    "\n",
    "3.  **Generate `BHaH_defines.h`**: It calls `Bdefines_h.output_BHaH_defines_h()`. This function scans `nrpy`'s internal library for all registered data structures (like `metric_struct` and `event_data_struct`) and writes them into the master C header file, `BHaH_defines.h`.\n",
    "\n",
    "4.  **Copy Helper Files**: It calls `gh.copy_files()` to copy any necessary dependency files (like `simd_intrinsics.h`) from the `nrpy` library installation into our project directory.\n",
    "\n",
    "5.  **Generate C Source, Prototypes, and Makefile**: It calls the final, most important build function, `Makefile.output_CFunctions_function_prototypes_and_construct_Makefile()`. This powerful function performs three tasks at once:\n",
    "    *   It iterates through every C function registered with `nrpy.c_function.register_CFunction` and writes each one into its own `.c` file (e.g., `main.c`, `connections.c`).\n",
    "    *   It generates `BHaH_function_prototypes.h`, a header file containing the function declarations (prototypes) for all the generated `.c` files. This is crucial as it allows the different C files to call functions defined in one another.\n",
    "    *   It constructs the `Makefile`, which contains the compilation and linking instructions needed to build the final executable program. It is also configured to automatically link against the required GSL and OpenMP libraries.\n",
    "\n",
    "After this cell is run, a complete, self-contained, and ready-to-compile C project will exist in the output directory.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.infrastructures.BHaH.CodeParameters.write_CodeParameters_h_files(project_dir)`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/CodeParameters.py`\n",
    "    *   **Description**: Generates the `set_CodeParameters.h` header files, which contain the C code for unpacking parameters into local variables (the \"Triple-Lock\" system).\n",
    "\n",
    "*   **`nrpy.infrastructures.BHaH.CodeParameters.register_CFunctions_params_commondata_struct_set_to_default()`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/CodeParameters.py`\n",
    "    *   **Description**: Registers the C functions that initialize the `params_struct` and `commondata_struct` with their compiled-in default values.\n",
    "\n",
    "*   **`nrpy.infrastructures.BHaH.cmdline_input_and_parfiles.generate_default_parfile(project_dir, project_name)`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/cmdline_input_and_parfiles.py`\n",
    "    *   **Description**: Creates the `project_name.par` file, populated with all parameters that have `add_to_parfile=True`.\n",
    "\n",
    "*   **`nrpy.infrastructures.BHaH.cmdline_input_and_parfiles.register_CFunction_cmdline_input_and_parfile_parser(project_name, cmdline_inputs)`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/cmdline_input_and_parfiles.py`\n",
    "    *   **Description**: Registers the C function that reads the `.par` file and command-line arguments at runtime. The `cmdline_inputs` list is critical, as it defines the exact order of expected positional command-line arguments.\n",
    "\n",
    "*   **`nrpy.infrastructures.BHaH.BHaH_defines_h.output_BHaH_defines_h(project_dir)`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/BHaH_defines_h.py`\n",
    "    *   **Description**: Scans `nrpy`'s internal library for all registered data structures and writes them into the master C header file, `BHaH_defines.h`.\n",
    "\n",
    "*   **`nrpy.helpers.generic.copy_files(...)`**:\n",
    "    *   **Source File**: `nrpy/helpers/generic.py`\n",
    "    *   **Description**: A utility function to copy files from the `nrpy` installation to the project directory.\n",
    "\n",
    "*   **`nrpy.infrastructures.BHaH.Makefile_helpers.output_CFunctions_function_prototypes_and_construct_Makefile(...)`**:\n",
    "    *   **Source File**: `nrpy/infrastructures/BHaH/Makefile_helpers.py`\n",
    "    *   **Description**: The final build function that generates all `.c` files, the function prototypes header, and the `Makefile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94ad99d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assembling and building C project for Time-Bundled Batch Integrator (v2.9.1)...\n",
      " -> Registering C data structures and functions...\n",
      " -> Registering all core C data structures (v2.10)...\n",
      "    ... Registered all core data structures in a single, ordered block.\n",
      " -> Registering global plane event detection helpers...\n",
      "    ... Registered plane_event_params struct and plane_event_func.\n",
      " -> Generating C worker function: g4DD_kerr_schild()...\n",
      "    ... g4DD_kerr_schild() registration complete.\n",
      " -> Generating C worker function: con_kerr_schild()...\n",
      "    ... con_kerr_schild() registration complete.\n",
      " -> Generating C worker function: g4DD_schwarzschild_cartesian()...\n",
      "    ... g4DD_schwarzschild_cartesian() registration complete.\n",
      " -> Generating C worker function: con_schwarzschild_cartesian()...\n",
      "    ... con_schwarzschild_cartesian() registration complete.\n",
      " -> Generating C dispatcher function: g4DD_metric()...\n",
      "    ... g4DD_metric() registration complete.\n",
      " -> Generating C dispatcher: connections()...\n",
      "    ... connections() registration complete.\n",
      " -> Generating C engine function: calculate_p0_reverse()...\n",
      "    ... calculate_p0_reverse() registration complete.\n",
      " -> Generating C engine: set_initial_conditions_cartesian()...\n",
      " -> Generating C engine: check_conservation() [Cartesian Version]...\n",
      "    ... check_conservation() registration complete.\n",
      " -> Generating C engine: find_event_time_and_state() [Robust Version]...\n",
      "    ... Registered C engine: find_event_time_and_state (Robust Version).\n",
      " -> Generating C event detection manager (Stateless Plane Detector Version)...\n",
      "    ... Registered event_detection_manager (Stateless Plane Detector Version).\n",
      " -> Generating C engine: handle_source_plane_intersection (v2.9.1)...\n",
      "    ... Registered C engine: handle_source_plane_intersection().\n",
      " -> Generating C finalization engine: calculate_and_fill_blueprint_data_universal (v2.9.1)...\n",
      "    ... Registered C engine: calculate_and_fill_blueprint_data_universal().\n",
      " -> Generating granular GSL control engines (v2.9.2)...\n",
      "    ... Registered C engine: calculate_and_apply_single_step().\n",
      "    ... Registered C engine: ode_gsl_wrapper() (Phase 1, No Caching).\n",
      " -> Generating top-level C orchestrator: batch_integrator() (v2.9.2)...\n",
      "    ... Registered C orchestrator: batch_integrator().\n",
      " -> Updating main() to call batch integrator (v2.8)...\n",
      "    ... main() has been updated successfully.\n",
      " -> Generating BHaH infrastructure files...\n",
      "\n",
      "Generating BHaH master header file...\n",
      "Outputting non-core modules key = after_general to BHaH_defines.h\n",
      "Outputting non-core modules key = plane_event_helpers to BHaH_defines.h\n",
      "Copying required helper files...\n",
      "Generating C source files, prototypes, and Makefile...\n",
      "\n",
      "Finished! A C project has been generated in project/photon_geodesic_integrator/\n",
      "To build, navigate to this directory in your terminal and type 'make'.\n",
      "To run, type './photon_geodesic_integrator'.\n"
     ]
    }
   ],
   "source": [
    "# This is the final, correct build cell (v2.9.1).\n",
    "import os\n",
    "import nrpy.helpers.generic as gh\n",
    "\n",
    "print(\"\\nAssembling and building C project for Time-Bundled Batch Integrator (v2.9.1)...\")\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "# --- Step 1: Register all C-generating functions in the correct dependency order ---\n",
    "print(\" -> Registering C data structures and functions...\")\n",
    "# Corrected order and content\n",
    "register_custom_structures_and_params()\n",
    "register_plane_event_helpers()\n",
    "# Physics workers and dispatchers\n",
    "g4DD_kerr_schild()\n",
    "con_kerr_schild()\n",
    "g4DD_schwarzschild_cartesian()\n",
    "con_schwarzschild_cartesian()\n",
    "g4DD_metric()\n",
    "connections()\n",
    "# Physics and integration engines\n",
    "calculate_ode_rhs()\n",
    "calculate_p0_reverse()\n",
    "set_initial_conditions_cartesian()\n",
    "check_conservation()\n",
    "lagrange_interp_engine_generic()\n",
    "event_detection_manager()\n",
    "handle_source_plane_intersection_engine()\n",
    "calculate_and_fill_blueprint_data_universal() # Now uses correct enum\n",
    "generate_granular_gsl_engines() # Now adds correct headers\n",
    "generate_batch_integrator_orchestrator() # Now adds correct headers\n",
    "main() # This should now be the typo-free version if you've updated that cell\n",
    "\n",
    "# --- Step 2: Call BHaH infrastructure functions to generate the build system ---\n",
    "print(\" -> Generating BHaH infrastructure files...\")\n",
    "CPs.write_CodeParameters_h_files(project_dir=project_dir)\n",
    "CPs.register_CFunctions_params_commondata_struct_set_to_default()\n",
    "cmdline_input_and_parfiles.generate_default_parfile(project_dir=project_dir, project_name=project_name)\n",
    "\n",
    "cmdline_inputs_list = [\n",
    "    'M_scale', 'a_spin', 'metric_choice',\n",
    "    'camera_pos_x', 'camera_pos_y', 'camera_pos_z',\n",
    "    'window_center_x', 'window_center_y', 'window_center_z',\n",
    "    'window_up_vec_x', 'window_up_vec_y', 'window_up_vec_z',\n",
    "    'source_plane_normal_x', 'source_plane_normal_y', 'source_plane_normal_z',\n",
    "    'source_plane_center_x', 'source_plane_center_y', 'source_plane_center_z',\n",
    "    'source_up_vec_x', 'source_up_vec_y', 'source_up_vec_z',\n",
    "    'source_r_min', 'source_r_max',\n",
    "    'scan_density', 'window_size',\n",
    "    'r_escape', 't_integration_max', 't_start',\n",
    "    'debug_mode', 'perform_conservation_check'\n",
    "]\n",
    "\n",
    "cmdline_input_and_parfiles.register_CFunction_cmdline_input_and_parfile_parser(\n",
    "    project_name=project_name,\n",
    "    cmdline_inputs=cmdline_inputs_list\n",
    ")\n",
    "\n",
    "# --- Step 3: Generate headers, helpers, and the final Makefile ---\n",
    "print(\"\\nGenerating BHaH master header file...\")\n",
    "Bdefines_h.output_BHaH_defines_h(project_dir=project_dir)\n",
    "\n",
    "print(\"Copying required helper files...\")\n",
    "gh.copy_files(\n",
    "    package=\"nrpy.helpers\",\n",
    "    filenames_list=[\"simd_intrinsics.h\"],\n",
    "    project_dir=project_dir,\n",
    "    subdirectory=\"simd\",\n",
    ")\n",
    "\n",
    "print(\"Generating C source files, prototypes, and Makefile...\")\n",
    "addl_CFLAGS = [\"-Wall -Wextra -g $(shell gsl-config --cflags) -fopenmp\"]\n",
    "addl_libraries = [\"$(shell gsl-config --libs) -fopenmp\"]\n",
    "\n",
    "Makefile.output_CFunctions_function_prototypes_and_construct_Makefile(\n",
    "    project_dir=project_dir,\n",
    "    project_name=project_name,\n",
    "    exec_or_library_name=project_name,\n",
    "    addl_CFLAGS=addl_CFLAGS,\n",
    "    addl_libraries=addl_libraries,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinished! A C project has been generated in {project_dir}/\")\n",
    "print(f\"To build, navigate to this directory in your terminal and type 'make'.\")\n",
    "print(f\"To run, type './{project_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c894937",
   "metadata": {},
   "source": [
    "<a id='plotting'></a>\n",
    "# Step 9: Visualization and Analysis\n",
    "\n",
    "This final section of the notebook is dedicated to visualizing the results produced by our C code. It contains Python code cells that use standard libraries like `numpy` and `matplotlib`/`Pillow` to process and plot the output data. These cells are for analysis and are not part of the C code generation process. They are designed to be run *after* the C code has been compiled and executed, and has produced a `blueprint.bin` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f6749",
   "metadata": {},
   "source": [
    "# Define the blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92faa911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and new blueprint data type (BLUEPRINT_DTYPE) defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Union, Optional, List, Tuple\n",
    "import os\n",
    "\n",
    "# Define the exact structure of a record in the new light_blueprint.bin file.\n",
    "# This must match the final C struct 'blueprint_data_t'.\n",
    "BLUEPRINT_DTYPE = np.dtype([\n",
    "    ('termination_type', np.int32),\n",
    "    ('y_w', 'f8'), \n",
    "    ('z_w', 'f8'),\n",
    "    # Fields for DISK hits\n",
    "    ('stokes_I', 'f8'),\n",
    "    ('lambda_observed', 'f8'),\n",
    "    # Fields for SOURCE PLANE hits\n",
    "    ('y_s', 'f8'),\n",
    "    ('z_s', 'f8'),\n",
    "    # Fields for CELESTIAL SPHERE hits\n",
    "    ('final_theta', 'f8'),\n",
    "    ('final_phi', 'f8'),\n",
    "    # Diagnostic fields\n",
    "    ('L_w', 'f8'),\n",
    "    ('t_w', 'f8'),\n",
    "    ('L_s', 'f8'),\n",
    "    ('t_s', 'f8'),\n",
    "], align=False)\n",
    "\n",
    "print(\"Libraries and new blueprint data type (BLUEPRINT_DTYPE) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "134f0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_photon_trajectory_from_debug(\n",
    "    project_dir: str = \"project/photon_geodesic_integrator\",\n",
    "    input_filename: str = \"photon_path.txt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data from the C code's debug output and generates a\n",
    "    simple 3D plot of the photon's path with a sphere at the origin.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Photon Trajectory Plot from Debug File ---\")\n",
    "    \n",
    "    # --- 1. Construct the full path and load the data ---\n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        print(\"Please ensure you have compiled and run the C code in debug mode successfully.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the data, skipping the header row.\n",
    "        # Set invalid_raise=False to handle potential trailing empty lines.\n",
    "        data = np.loadtxt(full_path, skiprows=1, ndmin=2)\n",
    "        if data.shape[0] == 0:\n",
    "            print(\"ERROR: Trajectory file is empty.\")\n",
    "            return\n",
    "            \n",
    "        # Columns: 0:lambda, 1:t, 2:x, 3:y, 4:z, ...\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "        print(f\"Successfully loaded {len(x_coords)} data points from trajectory file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{full_path}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Set up the 3D plot ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # --- 3. Plot the photon's trajectory ---\n",
    "    ax.plot(x_coords, y_coords, z_coords, label='Photon Path', color='cyan', lw=2)\n",
    "    \n",
    "    # Mark the start (camera) and end points\n",
    "    ax.scatter(x_coords[0], y_coords[0], z_coords[0], color='lime', s=100, label='Start (Camera)', marker='o', depthshade=False)\n",
    "    ax.scatter(x_coords[-1], y_coords[-1], z_coords[-1], color='red', s=100, label='End Point', marker='X', depthshade=False)\n",
    "\n",
    "    # --- 4. Plot a simple sphere at the origin ---\n",
    "    radius = 2.0 # Represents r=2M, the Schwarzschild event horizon\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = radius * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = radius * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='grey', alpha=0.5, rstride=5, cstride=5)\n",
    "    \n",
    "    # --- 5. Customize the plot ---\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Set aspect ratio to be equal to avoid distortion\n",
    "    max_range = np.array([x_coords.max()-x_coords.min(), y_coords.max()-y_coords.min(), z_coords.max()-z_coords.min()]).max() / 2.0\n",
    "    if max_range == 0: max_range = np.max(np.abs(data[:, 2:5]))\n",
    "    mid_x = (x_coords.max()+x_coords.min()) * 0.5\n",
    "    mid_y = (y_coords.max()+y_coords.min()) * 0.5\n",
    "    mid_z = (z_coords.max()+z_coords.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "    ax.set_title(\"Photon Trajectory (Debug Run)\", fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.view_init(elev=-90, azim=60)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79e73e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Photon Trajectory Plot from Debug File ---\n",
      "ERROR: Trajectory file not found at 'project/photon_geodesic_integrator/photon_path.txt'\n",
      "Please ensure you have compiled and run the C code in debug mode successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- How to Run ---\n",
    "# After running the C code in debug mode, call this function.\n",
    "plot_photon_trajectory_from_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1200f1f",
   "metadata": {},
   "source": [
    "# Start of Blueprint Stats ( 3 cells )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac8410",
   "metadata": {},
   "source": [
    "Cell 1 of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "965217d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def view_binary_blueprint(\n",
    "    blueprint_filename=\"project/photon_geodesic_integrator/light_blueprint.bin\",\n",
    "    max_rays_to_print=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the new binary blueprint file and prints its raw contents in a\n",
    "    human-readable, context-aware format.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(blueprint_filename):\n",
    "        print(f\"Error: Blueprint file not found at '{blueprint_filename}'\")\n",
    "        return\n",
    "\n",
    "    # This function now uses the global BLUEPRINT_DTYPE defined in the previous cell.\n",
    "    data = np.fromfile(blueprint_filename, dtype=BLUEPRINT_DTYPE)\n",
    "    \n",
    "    print(f\"--- Raw Blueprint Data Inspector (Radiative Transfer Version) ---\")\n",
    "    print(f\"Total records read from file: {len(data)}\\n\")\n",
    "    print(\"Printing a sample of records...\")\n",
    "    print(\"Enum Mapping: 0=FAILURE, 1=DISK, 2=SOURCE_PLANE, 3=SPHERE\")\n",
    "    \n",
    "    # Updated header for new data fields\n",
    "    header = f\"{'Ray#':<8} | {'TermType':<12} | {'y_w':>8} | {'z_w':>8} | {'Result 1':>12} | {'Result 2':>12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    if len(data) > max_rays_to_print:\n",
    "        indices_to_print = np.linspace(0, len(data) - 1, max_rays_to_print, dtype=int)\n",
    "    else:\n",
    "        indices_to_print = np.arange(len(data))\n",
    "\n",
    "    for i in indices_to_print:\n",
    "        rec = data[i]\n",
    "        term_type = int(rec['termination_type'])\n",
    "        term_str_map = {0: \"FAILURE\", 1: \"DISK\", 2: \"SOURCE_PLANE\", 3: \"SPHERE\"}\n",
    "        term_str = term_str_map.get(term_type, \"UNKNOWN\")\n",
    "\n",
    "        res1_str, res2_str = \"N/A\", \"N/A\"\n",
    "        if term_type == 1: # DISK\n",
    "            res1_str = f\"I={rec['stokes_I']:.3e}\"\n",
    "            res2_str = f\"λ={rec['lambda_observed']:.2f}\"\n",
    "        elif term_type == 2: # SOURCE_PLANE\n",
    "            res1_str = f\"y_s={rec['y_s']:.3f}\"\n",
    "            res2_str = f\"z_s={rec['z_s']:.3f}\"\n",
    "        elif term_type == 3: # CELESTIAL_SPHERE\n",
    "            res1_str = f\"θ={rec['final_theta']:.3f}\"\n",
    "            res2_str = f\"φ={rec['final_phi']:.3f}\"\n",
    "        \n",
    "        print(f\"{i:<8} | {term_str:<12} | {rec['y_w']:>8.2f} | {rec['z_w']:>8.2f} | {res1_str:>12} | {res2_str:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e0f07",
   "metadata": {},
   "source": [
    "Cell 2 of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be508e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import os\n",
    "\n",
    "def analyze_blueprint(blueprint_filename=\"project/photon_geodesic_integrator/light_blueprint.bin\"):\n",
    "    \"\"\"\n",
    "    Reads the new radiative transfer blueprint file and generates a comprehensive\n",
    "    statistical analysis and a full suite of plots, including a plot of\n",
    "    observed wavelength vs. the ray's position on the window plane (r_w).\n",
    "    \"\"\"\n",
    "    # --- 1. Load Data ---\n",
    "    if not os.path.exists(blueprint_filename):\n",
    "        print(f\"Error: Blueprint file not found at '{blueprint_filename}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.fromfile(blueprint_filename, dtype=BLUEPRINT_DTYPE)\n",
    "    except NameError:\n",
    "        print(\"ERROR: BLUEPRINT_DTYPE is not defined. Please run the cell that defines it first.\")\n",
    "        return\n",
    "        \n",
    "    if len(data) == 0:\n",
    "        print(\"Blueprint file is empty. No analysis to perform.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Segregate Data by Termination Type ---\n",
    "    failure_hits = data[data['termination_type'] == 0]\n",
    "    disk_hits = data[data['termination_type'] == 1]\n",
    "    source_plane_hits = data[data['termination_type'] == 2]\n",
    "    sphere_hits = data[data['termination_type'] == 3]\n",
    "\n",
    "    num_total_rays = len(data)\n",
    "    num_failure = len(failure_hits)\n",
    "    num_disk = len(disk_hits)\n",
    "    num_source_plane = len(source_plane_hits)\n",
    "    num_sphere = len(sphere_hits)\n",
    "\n",
    "    print(\"--- Blueprint File Analysis (Radiative Transfer Architecture) ---\")\n",
    "    print(f\"Total rays in scan: {num_total_rays}\")\n",
    "    print(f\"  Rays that hit the DISK:              {num_disk} ({100.0 * num_disk / num_total_rays:.2f}%)\")\n",
    "    print(f\"  Rays that hit the SOURCE_PLANE:      {num_source_plane} ({100.0 * num_source_plane / num_total_rays:.2f}%)\")\n",
    "    print(f\"  Rays that hit the CELESTIAL_SPHERE:  {num_sphere} ({100.0 * num_sphere / num_total_rays:.2f}%)\")\n",
    "    print(f\"  Rays that FAILED (e.g., shadow):     {num_failure} ({100.0 * num_failure / num_total_rays:.2f}%)\")\n",
    "\n",
    "    if num_failure > 0:\n",
    "        r_w_failure = np.sqrt(failure_hits['y_w']**2 + failure_hits['z_w']**2)\n",
    "        if len(r_w_failure) > 0 and np.any(np.isfinite(r_w_failure)):\n",
    "            print(f\"    -> Apparent radius of black hole shadow on window: {np.nanmax(r_w_failure):.4f} M\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # --- 3. Detailed Statistical Analysis (All sections restored) ---\n",
    "    if num_disk > 0:\n",
    "        valid_disk_hits = disk_hits[np.isfinite(disk_hits['stokes_I']) & np.isfinite(disk_hits['lambda_observed'])]\n",
    "        if len(valid_disk_hits) > 0:\n",
    "            intensities = valid_disk_hits['stokes_I']\n",
    "            wavelengths = valid_disk_hits['lambda_observed']\n",
    "            print(\"\\n--- Detailed Disk Hit Statistics ---\")\n",
    "            print(\"\\n  Observed Intensity (Stokes I):\")\n",
    "            print(f\"    Min / Max:    {np.min(intensities):.3e} / {np.max(intensities):.3e}\")\n",
    "            print(f\"    Mean / Median:  {np.mean(intensities):.3e} / {np.median(intensities):.3e}\")\n",
    "            percentiles_I = np.percentile(intensities, [10, 25, 75, 90])\n",
    "            print(f\"    Percentiles:  10th={percentiles_I[0]:.3e}, 25th={percentiles_I[1]:.3e}, 75th={percentiles_I[2]:.3e}, 90th={percentiles_I[3]:.3e}\")\n",
    "\n",
    "            print(\"\\n  Observed Wavelength (lambda_obs) in nm:\")\n",
    "            print(f\"    Min / Max:    {np.min(wavelengths):.2f} / {np.max(wavelengths):.2f}\")\n",
    "            print(f\"    Mean / Median:  {np.mean(wavelengths):.2f} / {np.median(wavelengths):.2f}\")\n",
    "            percentiles_L = np.percentile(wavelengths, [10, 25, 75, 90])\n",
    "            print(f\"    Percentiles:  10th={percentiles_L[0]:.2f}, 25th={percentiles_L[1]:.2f}, 75th={percentiles_L[2]:.2f}, 90th={percentiles_L[3]:.2f}\")\n",
    "\n",
    "    if num_source_plane > 0:\n",
    "        valid_plane_hits = source_plane_hits[np.isfinite(source_plane_hits['y_s']) & np.isfinite(source_plane_hits['z_s'])]\n",
    "        if len(valid_plane_hits) > 0:\n",
    "            r_s = np.sqrt(valid_plane_hits['y_s']**2 + valid_plane_hits['z_s']**2)\n",
    "            print(\"\\n--- Source Plane Hit Statistics ---\")\n",
    "            print(f\"  Planar Radius (r_s) for these hits: min={np.min(r_s):.4f}, max={np.max(r_s):.4f}, mean={np.mean(r_s):.4f}\")\n",
    "            print(f\"  Intersection Time (t_s) for these hits: min={np.min(valid_plane_hits['t_s']):.2f}, max={np.max(valid_plane_hits['t_s']):.2f}, mean={np.mean(valid_plane_hits['t_s']):.2f}\")\n",
    "\n",
    "    if num_sphere > 0:\n",
    "        valid_sphere_hits = sphere_hits[np.isfinite(sphere_hits['final_theta']) & np.isfinite(sphere_hits['final_phi'])]\n",
    "        if len(valid_sphere_hits) > 0:\n",
    "            print(\"\\n--- Celestial Sphere Hit Statistics ---\")\n",
    "            print(f\"  Final Angle (theta) for these hits: min={np.min(valid_sphere_hits['final_theta']):.4f}, max={np.max(valid_sphere_hits['final_theta']):.4f}, mean={np.mean(valid_sphere_hits['final_theta']):.4f}\")\n",
    "            print(f\"  Final Angle (phi)   for these hits: min={np.min(valid_sphere_hits['final_phi']):.4f}, max={np.max(valid_sphere_hits['final_phi']):.4f}, mean={np.mean(valid_sphere_hits['final_phi']):.4f}\")\n",
    "\n",
    "    # --- 4. Generate Expanded Plots (3x2 Grid) ---\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(22, 27))\n",
    "    fig.suptitle(\"Blueprint Data Visualization (Radiative Transfer Architecture)\", fontsize=20)\n",
    "    ax = axes.ravel()\n",
    "\n",
    "    # Plot 0: Observed Intensity (Stokes I) on Window\n",
    "    if num_disk > 0 and 'valid_disk_hits' in locals() and len(valid_disk_hits) > 0:\n",
    "        hist = ax[0].hist2d(valid_disk_hits['y_w'], valid_disk_hits['z_w'], \n",
    "                            bins=256, cmap='inferno', norm=LogNorm(),\n",
    "                            weights=valid_disk_hits['stokes_I'])\n",
    "        ax[0].set_title(\"Observed Intensity (Stokes I) on Window Plane\")\n",
    "        ax[0].set_xlabel(\"y_w (M)\"); ax[0].set_ylabel(\"z_w (M)\")\n",
    "        ax[0].set_aspect('equal', 'box')\n",
    "        fig.colorbar(hist[3], ax=ax[0], label=\"Intensity\")\n",
    "    else:\n",
    "        ax[0].text(0.5, 0.5, \"No Disk Hits\", ha='center', va='center', fontsize=16)\n",
    "        ax[0].set_title(\"Observed Intensity (Stokes I)\")\n",
    "\n",
    "    # Plot 1: Observed Wavelength on Window\n",
    "    if num_disk > 0 and 'valid_disk_hits' in locals() and len(valid_disk_hits) > 0:\n",
    "        lambda_min = np.min(valid_disk_hits['lambda_observed'])\n",
    "        lambda_max = np.max(valid_disk_hits['lambda_observed'])\n",
    "        sc = ax[1].scatter(valid_disk_hits['y_w'], valid_disk_hits['z_w'], \n",
    "                           c=valid_disk_hits['lambda_observed'], cmap='jet', s=2, vmin=lambda_min, vmax=lambda_max)\n",
    "        ax[1].set_title(\"Observed Wavelength (λ_obs) on Window Plane\")\n",
    "        ax[1].set_xlabel(\"y_w (M)\"); ax[1].set_ylabel(\"z_w (M)\")\n",
    "        ax[1].set_aspect('equal', 'box')\n",
    "        ax[1].set_facecolor('black')\n",
    "        fig.colorbar(sc, ax=ax[1], label=\"Wavelength (nm)\")\n",
    "    else:\n",
    "        ax[1].text(0.5, 0.5, \"No Disk Hits\", ha='center', va='center', fontsize=16)\n",
    "        ax[1].set_title(\"Observed Wavelength (λ_obs)\")\n",
    "\n",
    "    # PLOT 2 (CORRECTED): Observed Wavelength vs. WINDOW Radius\n",
    "    if num_disk > 0 and 'valid_disk_hits' in locals() and len(valid_disk_hits) > 0:\n",
    "        r_w = np.sqrt(valid_disk_hits['y_w']**2 + valid_disk_hits['z_w']**2)\n",
    "        hist2 = ax[2].hist2d(r_w, valid_disk_hits['lambda_observed'], \n",
    "                             bins=(200, 200), cmap='magma', norm=LogNorm())\n",
    "        ax[2].set_title(\"Observed Wavelength vs. Apparent Radius on Window\")\n",
    "        ax[2].set_xlabel(\"Apparent Radius on Window (r_w) [M]\")\n",
    "        ax[2].set_ylabel(\"Observed Wavelength (λ_obs) [nm]\")\n",
    "        fig.colorbar(hist2[3], ax=ax[2], label=\"Number of Rays\")\n",
    "        ax[2].grid(True, linestyle='--', alpha=0.5)\n",
    "    else:\n",
    "        ax[2].text(0.5, 0.5, \"No Disk Hits\", ha='center', va='center', fontsize=16)\n",
    "        ax[2].set_title(\"Observed Wavelength vs. Window Radius\")\n",
    "        \n",
    "    # Plot 3: Intensity Distribution\n",
    "    if num_disk > 0 and 'valid_disk_hits' in locals() and len(valid_disk_hits) > 0:\n",
    "        ax[3].hist(valid_disk_hits['stokes_I'], bins=100, color='cyan', log=True)\n",
    "        ax[3].set_title(\"Distribution of Observed Intensities\")\n",
    "        ax[3].set_xlabel(\"Observed Intensity (Stokes I)\")\n",
    "        ax[3].set_ylabel(\"Number of Rays (Log Scale)\")\n",
    "        ax[3].grid(True, linestyle='--', alpha=0.5)\n",
    "    else:\n",
    "        ax[3].text(0.5, 0.5, \"No Disk Hits\", ha='center', va='center', fontsize=16)\n",
    "        ax[3].set_title(\"Distribution of Observed Intensities\")\n",
    "\n",
    "    # Plot 4: Termination Type Distribution\n",
    "    labels = ['Failure', 'Disk Hits', 'Source Plane', 'Celestial Hits']\n",
    "    sizes = [num_failure, num_disk, num_source_plane, num_sphere]\n",
    "    colors = ['#2F2F2F', '#FFC300', '#581845', '#C70039']\n",
    "    explode = (0.1, 0, 0, 0)\n",
    "    plot_labels = [l for i, l in enumerate(labels) if sizes[i] > 0]\n",
    "    plot_sizes = [s for s in sizes if s > 0]\n",
    "    plot_colors = [c for i, c in enumerate(colors) if sizes[i] > 0]\n",
    "    plot_explode = [e for i, e in enumerate(explode) if sizes[i] > 0]\n",
    "    if plot_sizes:\n",
    "        ax[4].pie(plot_sizes, explode=plot_explode, labels=plot_labels, colors=plot_colors,\n",
    "                  autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "        ax[4].set_title(\"Distribution of Ray Termination Types\")\n",
    "        ax[4].axis('equal')\n",
    "    else:\n",
    "        ax[4].text(0.5, 0.5, \"No Data to Plot\", ha='center', va='center', fontsize=16)\n",
    "        ax[4].set_title(\"Distribution of Ray Termination Types\")\n",
    "\n",
    "    # Plot 5: Photon Outcome by Window Radius\n",
    "    if num_total_rays > 0:\n",
    "        valid_data = data[np.isfinite(data['y_w'])]\n",
    "        r_w = np.sqrt(valid_data['y_w']**2 + valid_data['z_w']**2)\n",
    "        r_w_failure = r_w[valid_data['termination_type'] == 0]\n",
    "        r_w_disk    = r_w[valid_data['termination_type'] == 1]\n",
    "        r_w_source  = r_w[valid_data['termination_type'] == 2]\n",
    "        r_w_sphere  = r_w[valid_data['termination_type'] == 3]\n",
    "        x_data = [r_w_failure, r_w_disk, r_w_source, r_w_sphere]\n",
    "        hist_labels = [f'FAILURE ({len(r_w_failure)})', f'DISK ({len(r_w_disk)})', f'SOURCE ({len(r_w_source)})', f'SPHERE ({len(r_w_sphere)})']\n",
    "        hist_colors = ['black', 'gold', 'purple', 'deepskyblue']\n",
    "        ax[5].hist(x_data, bins=100, stacked=True, label=hist_labels, color=hist_colors, edgecolor='dimgray')\n",
    "        ax[5].set_title(\"Photon Outcome by Initial Window Radius\")\n",
    "        ax[5].set_xlabel(\"Initial Radial Distance on Window (r_w)\")\n",
    "        ax[5].set_ylabel(\"Number of Photons\")\n",
    "        ax[5].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83cee5",
   "metadata": {},
   "source": [
    "Cell 3 of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4cef21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_stacked_radial_histogram(\n",
    "    blueprint_filename: str = \"project/photon_geodesic_integrator/light_blueprint.bin\", \n",
    "    bin_width: float = 0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the new radiative transfer blueprint file and creates a stacked \n",
    "    histogram showing the outcome of photons as a function of their \n",
    "    initial radial distance on the camera's window plane.\n",
    "\n",
    "    Args:\n",
    "        blueprint_filename: The path to the light_blueprint.bin file.\n",
    "        bin_width: The width of each radial bin for the histogram.\n",
    "    \"\"\"\n",
    "    print(f\"--- Generating stacked radial histogram for '{blueprint_filename}' ---\")\n",
    "    \n",
    "    # --- Load Data ---\n",
    "    if not os.path.exists(blueprint_filename):\n",
    "        print(f\"Error: Blueprint file not found at '{blueprint_filename}'\")\n",
    "        return\n",
    "    \n",
    "    # This function uses the global BLUEPRINT_DTYPE defined in a previous cell.\n",
    "    # It must be executed after the cell that defines BLUEPRINT_DTYPE.\n",
    "    try:\n",
    "        data = np.fromfile(blueprint_filename, dtype=BLUEPRINT_DTYPE)\n",
    "    except NameError:\n",
    "        print(\"ERROR: BLUEPRINT_DTYPE is not defined. Please run the cell that defines it first.\")\n",
    "        return\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(\"Blueprint file is empty. Cannot generate plot.\")\n",
    "        return\n",
    "        \n",
    "    # --- Calculate r_w for all rays ---\n",
    "    # We must filter out non-finite values that can corrupt statistics\n",
    "    valid_data = data[np.isfinite(data['y_w']) & np.isfinite(data['z_w'])]\n",
    "    r_w = np.sqrt(valid_data['y_w']**2 + valid_data['z_w']**2)\n",
    "    \n",
    "    # --- Separate r_w values based on NEW termination types ---\n",
    "    mask_failure = (valid_data['termination_type'] == 0)\n",
    "    mask_disk    = (valid_data['termination_type'] == 1)\n",
    "    mask_source  = (valid_data['termination_type'] == 2)\n",
    "    mask_sphere  = (valid_data['termination_type'] == 3)\n",
    "    \n",
    "    r_w_failure = r_w[mask_failure]\n",
    "    r_w_disk    = r_w[mask_disk]\n",
    "    r_w_source  = r_w[mask_source]\n",
    "    r_w_sphere  = r_w[mask_sphere]\n",
    "    \n",
    "    # --- Create the Bins for the Histogram ---\n",
    "    if len(r_w) == 0:\n",
    "        print(\"No valid ray data with finite window coordinates found.\")\n",
    "        return\n",
    "    max_radius = r_w.max()\n",
    "    bins = np.arange(0, max_radius + bin_width, bin_width)\n",
    "    \n",
    "    # --- Create the Plot ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    # Data and labels for the stacked histogram\n",
    "    x_data = [r_w_failure, r_w_disk, r_w_source, r_w_sphere]\n",
    "    labels = [\n",
    "        f'FAILURE (Shadow): {len(r_w_failure)}',\n",
    "        f'DISK HIT: {len(r_w_disk)}',\n",
    "        f'SOURCE PLANE: {len(r_w_source)}',\n",
    "        f'SPHERE: {len(r_w_sphere)}'\n",
    "    ]\n",
    "    colors = ['black', 'gold', 'purple', 'deepskyblue']\n",
    "    \n",
    "    # Create the stacked histogram\n",
    "    ax.hist(x_data, bins=bins, stacked=True, label=labels, color=colors, edgecolor='dimgray')\n",
    "    \n",
    "    # --- Add Labels and Title ---\n",
    "    title = f\"Photon Outcome by Initial Radial Distance (Bin Width: {bin_width})\\nTotal Rays: {len(data)}\"\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Initial Radial Distance on Window ($r_w$)', fontsize=12)\n",
    "    ax.set_ylabel('Number of Photons (Count)', fontsize=12)\n",
    "    ax.legend(title='Termination Type')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14fd5b",
   "metadata": {},
   "source": [
    "# Wavelength to rbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae2d0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced, false-color `wavelength_to_rgb` function defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def wavelength_to_rgb(wavelength_nm, min_vis_wl=400.0, max_vis_wl=700.0):\n",
    "    \"\"\"\n",
    "    Converts an array of wavelengths in nanometers to an array of RGB colors.\n",
    "    \n",
    "    This is a fully vectorized, high-performance version that uses numpy\n",
    "    operations to avoid slow Python loops.\n",
    "    \"\"\"\n",
    "    # Ensure input is a numpy array\n",
    "    wavelengths = np.asarray(wavelength_nm)\n",
    "    \n",
    "    # Normalize the wavelengths to the range [0, 1]\n",
    "    norm_wl = (wavelengths - min_vis_wl) / (max_vis_wl - min_vis_wl)\n",
    "    norm_wl = np.clip(norm_wl, 0.0, 1.0)\n",
    "    \n",
    "    # Use a perceptually uniform colormap\n",
    "    colormap = plt.get_cmap('jet')\n",
    "    \n",
    "    # The colormap function can be applied to the entire array at once.\n",
    "    # It returns an (N, 4) RGBA array.\n",
    "    colors_rgba = colormap(norm_wl)\n",
    "    \n",
    "    # We only need the first three RGB components.\n",
    "    return colors_rgba[:, :3]\n",
    "print(\"Advanced, false-color `wavelength_to_rgb` function defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "492cd4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Simple Photon Trajectory Plot ---\n",
      "ERROR: Trajectory file not found at 'project/photon_geodesic_integrator/photon_path.txt'\n",
      "Please ensure you have compiled and run the C code in debug mode successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_photon_trajectory_simple(\n",
    "    project_dir: str = \"project/photon_geodesic_integrator\",\n",
    "    input_filename: str = \"photon_path.txt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data from the C code's debug output and generates a\n",
    "    simple 3D plot of the photon's path with a sphere at the origin.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Simple Photon Trajectory Plot ---\")\n",
    "    \n",
    "    # --- 1. Construct the full path and load the data ---\n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        print(\"Please ensure you have compiled and run the C code in debug mode successfully.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        # Columns: 0:lambda, 1:t, 2:x, 3:y, 4:z, ...\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "        print(f\"Successfully loaded {len(x_coords)} data points from trajectory file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{full_path}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Set up the 3D plot ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # --- 3. Plot the photon's trajectory ---\n",
    "    ax.plot(x_coords, y_coords, z_coords, label='Photon Path', color='cyan', lw=2)\n",
    "    \n",
    "    # Mark the start (camera) and end points\n",
    "    ax.scatter(x_coords[0], y_coords[0], z_coords[0], color='lime', s=100, label='Start (Camera)', marker='o', depthshade=False)\n",
    "    ax.scatter(x_coords[-1], y_coords[-1], z_coords[-1], color='red', s=100, label='End Point', marker='X', depthshade=False)\n",
    "\n",
    "    # --- 4. Plot a simple sphere at the origin ---\n",
    "    radius = 2.0 # Represents r=2M, the Schwarzschild event horizon\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = radius * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = radius * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='grey', alpha=0.5, rstride=5, cstride=5, label='Black Hole (r=2M)')\n",
    "    \n",
    "    # --- 5. Customize the plot ---\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Set aspect ratio to be equal\n",
    "    max_range = np.array([x_coords.max()-x_coords.min(), y_coords.max()-y_coords.min(), z_coords.max()-z_coords.min()]).max() / 2.0\n",
    "    mid_x = (x_coords.max()+x_coords.min()) * 0.5\n",
    "    mid_y = (y_coords.max()+y_coords.min()) * 0.5\n",
    "    mid_z = (z_coords.max()+z_coords.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "    ax.set_title(\"Photon Trajectory\", fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.view_init(elev=30., azim=-60)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# --- How to Run ---\n",
    "# After running the C code in debug mode, call this function.\n",
    "\n",
    "plot_photon_trajectory_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2651bf",
   "metadata": {},
   "source": [
    "# Start of Visual function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90def71e",
   "metadata": {},
   "source": [
    "# Texture Loading Helper Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a451cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function `_load_texture` (Corrected) defined.\n"
     ]
    }
   ],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In the cell defining _load_texture\n",
    "\n",
    "def _load_texture(image_input: Union[str, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Helper function to load an image or use a pre-loaded numpy array,\n",
    "    ensuring the output is always a float64 array with values in [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    if isinstance(image_input, str):\n",
    "        if not os.path.exists(image_input):\n",
    "            raise FileNotFoundError(f\"Texture file not found: {image_input}\")\n",
    "        with Image.open(image_input) as img:\n",
    "            # Convert to RGB and normalize to [0.0, 1.0] floats\n",
    "            return np.array(img.convert(\"RGB\")).astype(np.float64) / 255.0\n",
    "    elif isinstance(image_input, np.ndarray):\n",
    "        # --- CORRECTED LOGIC ---\n",
    "        # 1. Ensure the array is float64 for calculations.\n",
    "        texture_array = image_input.astype(np.float64)\n",
    "        # 2. Check if the values are in the [0, 255] range. If so, normalize them.\n",
    "        if np.max(texture_array) > 1.0:\n",
    "            texture_array /= 255.0\n",
    "        return texture_array\n",
    "    else:\n",
    "        raise TypeError(\"Image input must be a file path (str) or a NumPy array.\")\n",
    "\n",
    "print(\"Helper function `_load_texture` (Corrected) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0784077",
   "metadata": {},
   "source": [
    " # Procedural Disk Generators (2 cells)\n",
    " First cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9edcaa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Visualizing/Generating the Unlensed Source Disk (UPDATED with Anti-Aliasing)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_source_disk_array(\n",
    "    pixel_width=512,\n",
    "    disk_physical_width=40.0,\n",
    "    disk_inner_radius=6.0,\n",
    "    disk_outer_radius=20.0,\n",
    "    disk_temp_power_law=-0.75,\n",
    "    colormap='hot',\n",
    "    display_image=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates an anti-aliased NumPy array of an accretion disk image.\n",
    "    \"\"\"\n",
    "    # --- 1. Create a Coordinate Grid ---\n",
    "    half_width = disk_physical_width / 2.0\n",
    "    y_coords = np.linspace(-half_width, half_width, pixel_width)\n",
    "    z_coords = np.linspace(-half_width, half_width, pixel_width)\n",
    "    yy, zz = np.meshgrid(y_coords, z_coords)\n",
    "\n",
    "    # --- 2. Calculate Physical Properties for Each Pixel ---\n",
    "    radii = np.sqrt(yy**2 + zz**2)\n",
    "\n",
    "    # --- 3. Apply the Disk Model with a Smooth Falloff ---\n",
    "    # Instead of a sharp mask, we'll calculate temperature for all points\n",
    "    # and then smoothly fade it to zero outside the disk bounds.\n",
    "    \n",
    "    # Calculate temperature based on the power law everywhere.\n",
    "    # Add a small epsilon to radii to avoid division by zero at the center.\n",
    "    temperature = (radii / disk_inner_radius)**disk_temp_power_law\n",
    "\n",
    "    # Create a smooth falloff mask using numpy.clip\n",
    "    # This will create a smooth transition from 1 (inside the disk) to 0 (outside)\n",
    "    # over a small number of pixels. Let's define a transition width.\n",
    "    transition_width = 2.0 * (disk_physical_width / pixel_width) # Width of 2 pixels\n",
    "\n",
    "    # Inner edge falloff\n",
    "    inner_falloff = np.clip((radii - (disk_inner_radius - transition_width)) / transition_width, 0, 1)\n",
    "    \n",
    "    # Outer edge falloff\n",
    "    outer_falloff = 1.0 - np.clip((radii - disk_outer_radius) / transition_width, 0, 1)\n",
    "\n",
    "    # Combine the masks and apply to the temperature\n",
    "    smooth_mask = inner_falloff * outer_falloff\n",
    "    temperature *= smooth_mask\n",
    "\n",
    "    # --- 4. Map Temperature to Color and Create Image Array ---\n",
    "    colormap_func = plt.colormaps[colormap]\n",
    "    colors = colormap_func(temperature / np.max(temperature)) # Normalize to ensure max is 1\n",
    "    image_array = (colors[:, :, :3] * 255).astype(np.uint8)\n",
    "    \n",
    "    # --- 5. Optionally Display the Image ---\n",
    "    if display_image:\n",
    "        print(f\"Displaying the unlensed source disk (with anti-aliasing):\")\n",
    "        img = Image.fromarray(image_array)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Unlensed Source Accretion Disk (Anti-Aliased)\")\n",
    "        plt.show()\n",
    "        \n",
    "    # --- 6. Return the NumPy array ---\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb1011f",
   "metadata": {},
   "source": [
    "Second Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f126c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In the cell defining generate_advanced_disk_array\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_advanced_disk_array(\n",
    "    # --- Basic Settings ---\n",
    "    pixel_width=1024,\n",
    "    disk_physical_width=30.0,\n",
    "    colormap='afmhot',\n",
    "    \n",
    "    # --- Base Radial Profile ---\n",
    "    disk_inner_radius=6.0,\n",
    "    disk_outer_radius=25.0,\n",
    "    disk_temp_power_law=-2.0,\n",
    "    \n",
    "    # --- Concentric Rings / Gaps ---\n",
    "    ring_num=5,\n",
    "    ring_contrast=0.7,\n",
    "    ring_log_spacing=True,\n",
    "    \n",
    "    # --- Doppler Beaming (Asymmetric Brightness) ---\n",
    "    doppler_factor=0.8,\n",
    "    doppler_power=3,\n",
    "    \n",
    "    # --- Optional Features (set to 0 to disable) ---\n",
    "    hotspot_num=0,\n",
    "    hotspot_amplitude=0.15,\n",
    "    hotspot_radius_center=9.0,\n",
    "    hotspot_radius_width=3.0,\n",
    "    shape_num_lobes=0,\n",
    "    shape_inner_amplitude=0.0,\n",
    "    shape_outer_amplitude=0.0,\n",
    "    \n",
    "    # --- Display Control ---\n",
    "    display_image=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates an EHT-style disk, combining advanced features with robust NaN handling.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating EHT-Style Accretion Disk Texture (Corrected v3) ---\")\n",
    "    \n",
    "    # 1. Create Coordinate Grid & Polar Coordinates\n",
    "    half_width = disk_physical_width / 2.0\n",
    "    y_coords = np.linspace(-half_width, half_width, pixel_width)\n",
    "    z_coords = np.linspace(-half_width, half_width, pixel_width)\n",
    "    yy, zz = np.meshgrid(y_coords, z_coords)\n",
    "    radii = np.sqrt(yy**2 + zz**2)\n",
    "    phi = np.arctan2(zz, yy)\n",
    "\n",
    "    # 2. Calculate Base Temperature\n",
    "    # Add a small epsilon to radii to avoid division by zero at the center.\n",
    "    temperature = (radii / (disk_inner_radius + 1e-12))**disk_temp_power_law\n",
    "    \n",
    "    # --- CORRECTED: Explicitly handle the NaN at the center ---\n",
    "    # This is the crucial fix from the older, working code.\n",
    "    temperature[np.isnan(temperature)] = 0\n",
    "\n",
    "    # 3. Apply Modulations (Rings, Doppler, etc.)\n",
    "    if ring_num > 0:\n",
    "        if ring_log_spacing:\n",
    "            radial_coord = np.log(radii / disk_inner_radius + 1e-9)\n",
    "            max_log_rad = np.log(disk_outer_radius / disk_inner_radius)\n",
    "            ring_mod = 0.5 * (1 + np.cos(ring_num * 2 * np.pi * radial_coord / max_log_rad))\n",
    "        else:\n",
    "            radial_coord = radii\n",
    "            ring_mod = 0.5 * (1 + np.cos(ring_num * 2 * np.pi * (radial_coord - disk_inner_radius) / (disk_outer_radius - disk_inner_radius)))\n",
    "        ring_mod = 1.0 - ring_contrast * (1.0 - ring_mod)\n",
    "        temperature *= ring_mod\n",
    "\n",
    "    if doppler_factor > 0:\n",
    "        doppler_mod = (1 + doppler_factor * (-np.cos(phi)))**doppler_power\n",
    "        temperature *= doppler_mod\n",
    "        \n",
    "    # ... (hotspot logic would go here) ...\n",
    "\n",
    "    # 4. Apply Final Radial Mask\n",
    "    # This uses the smooth falloff logic from the older, working code.\n",
    "    transition_width = 2.0 * (disk_physical_width / pixel_width)\n",
    "    r_inner_mod = disk_inner_radius + shape_inner_amplitude * np.cos(shape_num_lobes * phi)\n",
    "    r_outer_mod = disk_outer_radius + shape_outer_amplitude * np.cos(shape_num_lobes * phi)\n",
    "    inner_falloff = np.clip((radii - (r_inner_mod - transition_width)) / transition_width, 0, 1)\n",
    "    outer_falloff = 1.0 - np.clip((radii - r_outer_mod) / transition_width, 0, 1)\n",
    "    smooth_mask = inner_falloff * outer_falloff\n",
    "    temperature *= smooth_mask\n",
    "\n",
    "    # 5. Map to Color using robust normalization\n",
    "    max_temp = np.max(temperature)\n",
    "    if max_temp > 0:\n",
    "        norm_temperature = temperature / max_temp\n",
    "    else:\n",
    "        norm_temperature = temperature # Avoid division by zero if all temps are zero\n",
    "\n",
    "    colormap_func = plt.colormaps[colormap]\n",
    "    colors = colormap_func(norm_temperature)\n",
    "    image_array = (colors[:, :, :3] * 255).astype(np.uint8)\n",
    "    \n",
    "    # 6. Display\n",
    "    if display_image:\n",
    "        print(\"Displaying the unlensed advanced source disk:\")\n",
    "        img = Image.fromarray(image_array)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img, extent=[-half_width, half_width, -half_width, half_width])\n",
    "        plt.title(\"Advanced, EHT-Style Source Disk (Corrected)\")\n",
    "        plt.xlabel(\"y (M)\")\n",
    "        plt.ylabel(\"z (M)\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "        \n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1e9be",
   "metadata": {},
   "source": [
    "# Static lensed image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1444301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_static_lensed_image(\n",
    "    output_filename: str,\n",
    "    output_pixel_width: int,\n",
    "    source_image_width: float,\n",
    "    sphere_image: Union[str, np.ndarray],\n",
    "    source_image: Union[str, np.ndarray],\n",
    "    intensity_scale: float = 1.0,\n",
    "    lambda_min_nm: Optional[float] = None,\n",
    "    lambda_max_nm: Optional[float] = None,\n",
    "    gamma: float = 2.2,\n",
    "    blueprint_filename: str = \"project/photon_geodesic_integrator/light_blueprint.bin\",\n",
    "    window_width: Optional[float] = None,\n",
    "    zoom_region: Optional[Union[List[float], Tuple[float, float, float, float]]] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a lensed image using a multi-layer composite with selective tone mapping.\n",
    "    \n",
    "    This definitive version applies non-linear gamma correction only to the\n",
    "    high-dynamic-range disk light, preserving the linear color of background\n",
    "    sources, and then additively blends them for a physically realistic image.\n",
    "    \"\"\"\n",
    "    print(f\"--- Generating Static Lensed Image (Selective Tone Mapping): '{output_filename}' ---\")\n",
    "    \n",
    "    # --- Phase 1: Initialization and Data Loading ---\n",
    "    if not os.path.exists(blueprint_filename):\n",
    "        raise FileNotFoundError(f\"Blueprint file not found: {blueprint_filename}\")\n",
    "    blueprint_data = np.fromfile(blueprint_filename, dtype=BLUEPRINT_DTYPE)\n",
    "    if zoom_region:\n",
    "        y_w_min, y_w_max, z_w_min, z_w_max = zoom_region\n",
    "    elif window_width:\n",
    "        half_w = window_width / 2.0\n",
    "        y_w_min, y_w_max = -half_w, half_w\n",
    "        z_w_min, z_w_max = -half_w, half_w\n",
    "    else:\n",
    "        raise ValueError(\"Either 'window_width' or 'zoom_region' must be provided.\")\n",
    "    window_y_range = y_w_max - y_w_min\n",
    "    window_z_range = z_w_max - z_w_min\n",
    "    aspect_ratio = window_z_range / window_y_range\n",
    "    output_pixel_height = int(output_pixel_width * aspect_ratio)\n",
    "    source_texture = _load_texture(source_image)\n",
    "    sphere_texture = _load_texture(sphere_image)\n",
    "\n",
    "    # Separate accumulators for disk (foreground) and other (background) layers\n",
    "    disk_pixel_accumulator = np.zeros((output_pixel_height, output_pixel_width, 3), dtype=np.float64)\n",
    "    background_pixel_accumulator = np.zeros((output_pixel_height, output_pixel_width, 3), dtype=np.float64)\n",
    "    # A single counter for all successful rays per pixel\n",
    "    count_accumulator = np.zeros((output_pixel_height, output_pixel_width), dtype=np.int32)\n",
    "\n",
    "    # --- Phase 2: Vectorized Ray Processing ---\n",
    "    mask_in_view = (\n",
    "        (blueprint_data['y_w'] >= y_w_min) & (blueprint_data['y_w'] < y_w_max) &\n",
    "        (blueprint_data['z_w'] >= z_w_min) & (blueprint_data['z_w'] < z_w_max)\n",
    "    )\n",
    "    rays_in_view = blueprint_data[mask_in_view]\n",
    "    \n",
    "    if len(rays_in_view) > 0:\n",
    "        px_float = (rays_in_view['y_w'] - y_w_min) / window_y_range * output_pixel_width\n",
    "        py_float = (z_w_max - rays_in_view['z_w']) / window_z_range * output_pixel_height\n",
    "        px = np.clip(px_float, 0, output_pixel_width - 1).astype(np.int32)\n",
    "        py = np.clip(py_float, 0, output_pixel_height - 1).astype(np.int32)\n",
    "\n",
    "        # --- Process each termination type into its correct layer ---\n",
    "        \n",
    "        # 1. DISK HITS -> Disk Layer\n",
    "        is_disk = rays_in_view['termination_type'] == 1\n",
    "        if np.any(is_disk):\n",
    "            disk_hits = rays_in_view[is_disk]\n",
    "            valid_disk_hits = disk_hits[np.isfinite(disk_hits['lambda_observed'])]\n",
    "            if len(valid_disk_hits) > 0:\n",
    "                lambda_min = lambda_min_nm if lambda_min_nm is not None else np.min(valid_disk_hits['lambda_observed'])\n",
    "                lambda_max = lambda_max_nm if lambda_max_nm is not None else np.max(valid_disk_hits['lambda_observed'])\n",
    "                base_colors = wavelength_to_rgb(disk_hits['lambda_observed'], min_vis_wl=lambda_min, max_vis_wl=lambda_max)\n",
    "                intensities = disk_hits['stokes_I'][:, np.newaxis]\n",
    "                disk_colors = base_colors * intensities * intensity_scale\n",
    "                np.add.at(disk_pixel_accumulator, (py[is_disk], px[is_disk]), disk_colors)\n",
    "\n",
    "        # 2. SOURCE PLANE HITS -> Background Layer\n",
    "        is_source_plane = rays_in_view['termination_type'] == 2\n",
    "        if np.any(is_source_plane):\n",
    "            source_plane_hits = rays_in_view[is_source_plane]\n",
    "            source_pixel_height, source_pixel_width, _ = source_texture.shape\n",
    "            half_sw = source_image_width / 2.0\n",
    "            norm_y = (source_plane_hits['y_s'] + half_sw) / source_image_width\n",
    "            norm_z = (source_plane_hits['z_s'] + half_sw) / source_image_width\n",
    "            px_s = norm_y * (source_pixel_width - 1)\n",
    "            py_s = (1.0 - norm_z) * (source_pixel_height - 1)\n",
    "            px_s_int = np.clip(px_s, 0, source_pixel_width - 1).astype(np.int32)\n",
    "            py_s_int = np.clip(py_s, 0, source_pixel_height - 1).astype(np.int32)\n",
    "            source_colors = source_texture[py_s_int, px_s_int]\n",
    "            np.add.at(background_pixel_accumulator, (py[is_source_plane], px[is_source_plane]), source_colors)\n",
    "\n",
    "        # 3. CELESTIAL SPHERE HITS -> Background Layer\n",
    "        is_sphere = rays_in_view['termination_type'] == 3\n",
    "        if np.any(is_sphere):\n",
    "            sphere_hits = rays_in_view[is_sphere]\n",
    "            sphere_pixel_height, sphere_pixel_width, _ = sphere_texture.shape\n",
    "            norm_phi = (sphere_hits['final_phi'] + np.pi) / (2 * np.pi)\n",
    "            norm_theta = sphere_hits['final_theta'] / np.pi\n",
    "            px_sph = norm_phi * (sphere_pixel_width - 1)\n",
    "            py_sph = norm_theta * (sphere_pixel_height - 1)\n",
    "            px_sph_int = np.clip(px_sph, 0, sphere_pixel_width - 1).astype(np.int32)\n",
    "            py_sph_int = np.clip(py_sph, 0, sphere_pixel_height - 1).astype(np.int32)\n",
    "            sphere_colors = sphere_texture[py_sph_int, px_sph_int]\n",
    "            np.add.at(background_pixel_accumulator, (py[is_sphere], px[is_sphere]), sphere_colors)\n",
    "\n",
    "        # Increment the count for every ray that successfully terminated\n",
    "        np.add.at(count_accumulator, (py, px), 1)\n",
    "\n",
    "    # --- Phase 3: Assembling Final Image with Selective Tone Mapping ---\n",
    "    hit_pixels_mask = count_accumulator > 0\n",
    "    \n",
    "    # 1. Calculate the average color for each layer separately\n",
    "    avg_disk_color = np.zeros_like(disk_pixel_accumulator)\n",
    "    avg_background_color = np.zeros_like(background_pixel_accumulator)\n",
    "    \n",
    "    # Use the mask to avoid division by zero for pixels that were not hit\n",
    "    avg_disk_color[hit_pixels_mask] = disk_pixel_accumulator[hit_pixels_mask] / count_accumulator[hit_pixels_mask, np.newaxis]\n",
    "    avg_background_color[hit_pixels_mask] = background_pixel_accumulator[hit_pixels_mask] / count_accumulator[hit_pixels_mask, np.newaxis]\n",
    "\n",
    "    # 2. Apply Tone Mapping ONLY to the averaged disk layer\n",
    "    tone_mapped_disk_color = np.zeros_like(avg_disk_color)\n",
    "    disk_hit_mask = np.any(avg_disk_color > 0, axis=2) # Find pixels that actually have disk light\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED BLOCK ---\n",
    "    # Only perform normalization and gamma if there were actual disk hits.\n",
    "    if np.any(disk_hit_mask):\n",
    "        max_disk_brightness = np.max(avg_disk_color[disk_hit_mask])\n",
    "        normalized_disk_color = avg_disk_color[disk_hit_mask]\n",
    "        if max_disk_brightness > 0:\n",
    "            normalized_disk_color /= max_disk_brightness\n",
    "            \n",
    "        scaled_disk_color = normalized_disk_color * intensity_scale\n",
    "        \n",
    "        tone_mapped_disk_color[disk_hit_mask] = scaled_disk_color ** (1.0 / gamma)\n",
    "    # --- END OF CORRECTION ---\n",
    "        \n",
    "    \n",
    "    # 3. Additively blend the tone-mapped disk with the linear background\n",
    "    final_image_float = tone_mapped_disk_color + avg_background_color\n",
    "        \n",
    "    # 4. Convert to final image format\n",
    "    final_image_uint8 = (np.clip(final_image_float, 0, 1) * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(final_image_uint8, 'RGB')\n",
    "    \n",
    "    output_dir = os.path.dirname(output_filename)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    img.save(output_filename)\n",
    "    print(f\"--- Static image saved to '{output_filename}' ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b84dba",
   "metadata": {},
   "source": [
    "# Rotating source image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1572a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_animated_lensed_image(\n",
    "    output_filename: str,\n",
    "    output_pixel_width: int,\n",
    "    source_image_width: float,\n",
    "    sphere_image: Union[str, np.ndarray],\n",
    "    source_image: Union[str, np.ndarray],\n",
    "    # --- Parameters for orbital dynamics ---\n",
    "    M_scale: float,\n",
    "    t_anim: float,\n",
    "    prograde_disk: bool = True,\n",
    "    # ---\n",
    "    blueprint_filename: str = \"project/photon_geodesic_integrator/blueprint.bin\",\n",
    "    window_width: Optional[float] = None,\n",
    "    zoom_region: Optional[Union[List[float], Tuple[float, float, float, float]]] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a lensed image of a DYNAMIC, differentially rotating Keplerian disk.\n",
    "    \n",
    "    This version uses the animation time (t_anim) and light-travel time (t_s)\n",
    "    to calculate the rotational position of the disk at the moment of emission.\n",
    "    \"\"\"\n",
    "    # The body of this function is identical to the one I provided previously,\n",
    "    # as its logic for handling animation was already correct.\n",
    "    \n",
    "    # --- Phase 1: Initialization and Setup ---\n",
    "    if zoom_region:\n",
    "        y_w_min, y_w_max, z_w_min, z_w_max = zoom_region\n",
    "    elif window_width:\n",
    "        half_w = window_width / 2.0\n",
    "        y_w_min, y_w_max = -half_w, half_w\n",
    "        z_w_min, z_w_max = -half_w, half_w\n",
    "    else:\n",
    "        raise ValueError(\"Either 'window_width' or 'zoom_region' must be provided.\")\n",
    "\n",
    "    window_y_range = y_w_max - y_w_min\n",
    "    window_z_range = z_w_max - z_w_min\n",
    "    aspect_ratio = window_z_range / window_y_range\n",
    "    output_pixel_height = int(output_pixel_width * aspect_ratio)\n",
    "\n",
    "    source_texture = _load_texture(source_image)\n",
    "    sphere_texture = _load_texture(sphere_image)\n",
    "    source_pixel_height, source_pixel_width, _ = source_texture.shape\n",
    "    sphere_pixel_height, sphere_pixel_width, _ = sphere_texture.shape\n",
    "\n",
    "    if not os.path.exists(blueprint_filename):\n",
    "        raise FileNotFoundError(f\"Blueprint file not found: {blueprint_filename}\")\n",
    "    blueprint_data = np.fromfile(blueprint_filename, dtype=BLUEPRINT_DTYPE)\n",
    "\n",
    "    pixel_accumulator = np.zeros((output_pixel_height, output_pixel_width, 3), dtype=np.float64)\n",
    "    count_accumulator = np.zeros((output_pixel_height, output_pixel_width), dtype=np.int32)\n",
    "\n",
    "    # --- Phase 2: Vectorized Ray Processing ---\n",
    "    mask_in_view = (\n",
    "        (blueprint_data['y_w'] >= y_w_min) & (blueprint_data['y_w'] < y_w_max) &\n",
    "        (blueprint_data['z_w'] >= z_w_min) & (blueprint_data['z_w'] < z_w_max)\n",
    "    )\n",
    "    rays_in_view = blueprint_data[mask_in_view]\n",
    "    \n",
    "    if len(rays_in_view) > 0:\n",
    "        px_float = (rays_in_view['y_w'] - y_w_min) / window_y_range * output_pixel_width\n",
    "        py_float = (z_w_max - rays_in_view['z_w']) / window_z_range * output_pixel_height\n",
    "        px = np.clip(px_float, 0, output_pixel_width - 1).astype(np.int32)\n",
    "        py = np.clip(py_float, 0, output_pixel_height - 1).astype(np.int32)\n",
    "\n",
    "        is_source = rays_in_view['termination_type'] == 1\n",
    "        is_sphere = rays_in_view['termination_type'] == 2\n",
    "\n",
    "        if np.any(is_source):\n",
    "            source_hits = rays_in_view[is_source]\n",
    "            \n",
    "            y_s_intersect = source_hits['y_s']\n",
    "            z_s_intersect = source_hits['z_s']\n",
    "            t_s = source_hits['t_s']\n",
    "            \n",
    "            r_s = np.sqrt(y_s_intersect**2 + z_s_intersect**2)\n",
    "            Omega = np.sqrt(M_scale / (r_s**3 + 1e-12))\n",
    "            \n",
    "            total_time = t_anim + t_s\n",
    "            rotation_angle = Omega * total_time\n",
    "            rotation_direction = -1.0 if prograde_disk else 1.0\n",
    "            final_angle = rotation_direction * rotation_angle\n",
    "            \n",
    "            cos_angle = np.cos(final_angle)\n",
    "            sin_angle = np.sin(final_angle)\n",
    "            \n",
    "            y_s_emit = y_s_intersect * cos_angle - z_s_intersect * sin_angle\n",
    "            z_s_emit = y_s_intersect * sin_angle + z_s_intersect * cos_angle\n",
    "            \n",
    "            half_sw = source_image_width / 2.0\n",
    "            norm_y = (y_s_emit + half_sw) / source_image_width\n",
    "            norm_z = (z_s_emit + half_sw) / source_image_width\n",
    "\n",
    "            px_s = norm_y * (source_pixel_width - 1)\n",
    "            py_s = (1.0 - norm_z) * (source_pixel_height - 1)\n",
    "            px_s_int = np.clip(px_s, 0, source_pixel_width - 1).astype(np.int32)\n",
    "            py_s_int = np.clip(py_s, 0, source_pixel_height - 1).astype(np.int32)\n",
    "            source_colors = source_texture[py_s_int, px_s_int]\n",
    "            np.add.at(pixel_accumulator, (py[is_source], px[is_source]), source_colors)\n",
    "\n",
    "        if np.any(is_sphere):\n",
    "            sphere_hits = rays_in_view[is_sphere]\n",
    "            norm_phi = (sphere_hits['final_phi'] + np.pi) / (2 * np.pi)\n",
    "            norm_theta = sphere_hits['final_theta'] / np.pi\n",
    "            px_sph = norm_phi * (sphere_pixel_width - 1)\n",
    "            py_sph = norm_theta * (sphere_pixel_height - 1)\n",
    "            px_sph_int = np.clip(px_sph, 0, sphere_pixel_width - 1).astype(np.int32)\n",
    "            py_sph_int = np.clip(py_sph, 0, sphere_pixel_height - 1).astype(np.int32)\n",
    "            sphere_colors = sphere_texture[py_sph_int, px_sph_int]\n",
    "            np.add.at(pixel_accumulator, (py[is_sphere], px[is_sphere]), sphere_colors)\n",
    "\n",
    "        np.add.at(count_accumulator, (py, px), 1)\n",
    "\n",
    "    # --- Phase 3: Assembling Final Image ---\n",
    "    hit_pixels_mask = count_accumulator > 0\n",
    "    final_image_float = np.zeros_like(pixel_accumulator)\n",
    "    final_image_float[hit_pixels_mask] = (\n",
    "        pixel_accumulator[hit_pixels_mask] / count_accumulator[hit_pixels_mask, np.newaxis]\n",
    "    )\n",
    "    \n",
    "    final_image_uint8 = (np.clip(final_image_float, 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    img = Image.fromarray(final_image_uint8, 'RGB')\n",
    "    \n",
    "    output_dir = os.path.dirname(output_filename)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    img.save(output_filename)\n",
    "    print(f\"--- Animated frame saved to '{output_filename}' ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2cab0",
   "metadata": {},
   "source": [
    "# The Master Animation Frame Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1a21ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, Image as IPImage\n",
    "from typing import Union\n",
    "\n",
    "def generate_animation_frames(\n",
    "    # --- Core Inputs ---\n",
    "    blueprint_filename: str,\n",
    "    output_folder: str,\n",
    "    \n",
    "    # --- Rendering & Physics Parameters ---\n",
    "    source_texture: Union[str, np.ndarray],\n",
    "    sphere_texture: Union[str, np.ndarray],\n",
    "    source_physical_width: float,\n",
    "    mass_of_black_hole: float,\n",
    "    \n",
    "    # --- Animation Control ---\n",
    "    num_frames: int = 120,\n",
    "    orbits_at_isco: float = 2.0,\n",
    "    is_prograde: bool = True,\n",
    "    \n",
    "    # --- Image & Window Settings ---\n",
    "    output_pixel_width: int = 400,\n",
    "    window_width: float = 1.5\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a sequence of lensed image frames for creating an animation.\n",
    "\n",
    "    This function takes a single ray-tracing blueprint and generates multiple\n",
    "    frames by animating a differentially rotating Keplerian disk over time.\n",
    "\n",
    "    Args:\n",
    "        blueprint_filename: Path to the input 'blueprint.bin' file.\n",
    "        output_folder: Directory where the output frames will be saved.\n",
    "        source_texture: Path to the source image or a pre-loaded NumPy array.\n",
    "        sphere_texture: Path to the background star map or a pre-loaded NumPy array.\n",
    "        source_physical_width: The physical width (in units of M) of the source texture.\n",
    "        mass_of_black_hole: The mass (M_scale) of the black hole.\n",
    "        num_frames: The total number of frames to generate for the animation.\n",
    "        orbits_at_isco: The total number of orbits the disk completes at the ISCO (r=6M)\n",
    "                        over the full animation duration.\n",
    "        is_prograde: Direction of disk rotation (True for prograde, False for retrograde).\n",
    "        output_pixel_width: The width of the output images in pixels.\n",
    "        window_width: The physical width of the camera's viewing window.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Animation Generation ---\")\n",
    "    \n",
    "    # --- Setup and Validation ---\n",
    "    if not os.path.exists(blueprint_filename):\n",
    "        raise FileNotFoundError(f\"Blueprint file not found: {blueprint_filename}\")\n",
    "        \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Calculate total animation time based on the desired number of orbits at the ISCO\n",
    "    # ISCO for Schwarzschild is at r=6M.\n",
    "    isco_radius = 6.0 \n",
    "    orbital_period_at_isco = 2 * np.pi * np.sqrt(isco_radius**3 / mass_of_black_hole)\n",
    "    total_animation_time = orbital_period_at_isco * orbits_at_isco\n",
    "    \n",
    "    print(f\"  Configuration:\")\n",
    "    print(f\"    Total frames: {num_frames}\")\n",
    "    print(f\"    Total animation time: {total_animation_time:.2f} M ({orbits_at_isco:.1f} orbits at r=6M)\")\n",
    "    print(f\"    Output folder: '{output_folder}'\")\n",
    "\n",
    "    # --- Master Animation Loop ---\n",
    "    for i in range(num_frames):\n",
    "        # Calculate the animation time for the current frame\n",
    "        t_animation = (i / (num_frames - 1)) * total_animation_time if num_frames > 1 else 0\n",
    "        \n",
    "        # Define a sequential filename for the frame\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{i:04d}.png\")\n",
    "        \n",
    "        print(f\"Rendering frame {i+1}/{num_frames} (t_anim = {t_animation:.2f} M)...\")\n",
    "        \n",
    "        # Call the rendering function for a single frame\n",
    "        generate_animated_lensed_image(\n",
    "            output_filename=frame_filename,\n",
    "            output_pixel_width=output_pixel_width,\n",
    "            source_image_width=source_physical_width,\n",
    "            sphere_image=sphere_texture,\n",
    "            source_image=source_texture,\n",
    "            M_scale=mass_of_black_hole,\n",
    "            t_anim=t_animation,\n",
    "            prograde_disk=is_prograde,\n",
    "            blueprint_filename=blueprint_filename,\n",
    "            window_width=window_width\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- All frames generated successfully. ---\")\n",
    "\n",
    "    # Optional: Display the first frame in the notebook for verification\n",
    "    first_frame_path = os.path.join(output_folder, \"frame_0000.png\")\n",
    "    if os.path.exists(first_frame_path):\n",
    "        print(\"Displaying first generated frame:\")\n",
    "        display(IPImage(filename=first_frame_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801c51d",
   "metadata": {},
   "source": [
    "#  The Video Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bad79e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def encode_video_from_frames(\n",
    "    image_folder: str,\n",
    "    output_video_path: str,\n",
    "    frame_rate: int = 30,\n",
    "    crf: int = 18\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Encodes a sequence of image frames into a video file using FFmpeg.\n",
    "\n",
    "    This function requires FFmpeg to be installed and accessible in the system's PATH.\n",
    "\n",
    "    Args:\n",
    "        image_folder: The directory containing the sequentially named image frames.\n",
    "        output_video_path: The full path for the output video file (e.g., 'output/animation.mp4').\n",
    "        frame_rate: The frame rate for the output video.\n",
    "        crf: The Constant Rate Factor for the x264 codec (lower is higher quality).\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Video Encoding ---\")\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_video_path)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the FFmpeg command as a list of arguments\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-y',  # Overwrite output file if it exists\n",
    "        '-framerate', str(frame_rate),\n",
    "        '-i', os.path.join(image_folder, 'frame_%04d.png'),\n",
    "        '-c:v', 'libx264',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        '-r', str(frame_rate),\n",
    "        '-crf', str(crf),\n",
    "        output_video_path\n",
    "    ]\n",
    "\n",
    "    print(f\"Running FFmpeg command:\\n{' '.join(command)}\")\n",
    "\n",
    "    try:\n",
    "        # Run the command\n",
    "        # capture_output=True will store stdout and stderr in the result object\n",
    "        # text=True will decode them as text\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True  # This will raise a CalledProcessError if FFmpeg returns a non-zero exit code\n",
    "        )\n",
    "        print(\"\\n--- FFmpeg stdout ---\")\n",
    "        print(result.stdout)\n",
    "        print(\"\\n--- FFmpeg stderr ---\")\n",
    "        print(result.stderr)\n",
    "        print(f\"\\n[✓] Video encoding successful. File saved to '{output_video_path}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n[!] ERROR: FFmpeg not found.\")\n",
    "        print(\"Please ensure FFmpeg is installed and accessible in your system's PATH.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n[!] ERROR: FFmpeg failed with exit code {e.returncode}.\")\n",
    "        print(\"\\n--- FFmpeg stdout ---\")\n",
    "        print(e.stdout)\n",
    "        print(\"\\n--- FFmpeg stderr ---\")\n",
    "        print(e.stderr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8632d17",
   "metadata": {},
   "source": [
    "# Runner and Blueprint Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "797597e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function `run_integrator_and_rename_blueprint` defined.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def run_integrator_and_rename_blueprint(\n",
    "    project_dir: str,\n",
    "    executable_name: str,\n",
    "    args_string: str,\n",
    "    output_blueprint_name: str,\n",
    "    # NEW: Added parameter for the output directory for blueprints\n",
    "    bin_folder: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Runs the C geodesic integrator and moves the resulting blueprint file\n",
    "    to a specified subfolder.\n",
    "    \"\"\"\n",
    "    command_to_run = f\"./{executable_name} {args_string}\"\n",
    "    \n",
    "    print(f\"--- Running command in directory '{project_dir}': {command_to_run} ---\")\n",
    "\n",
    "    try:\n",
    "        process_result = subprocess.run(\n",
    "            command_to_run,\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True,\n",
    "            cwd=project_dir\n",
    "        )\n",
    "        print(process_result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"ERROR: The C program exited with an error (exit code {e.returncode}).\")\n",
    "        print(\"--- Standard Error ---\")\n",
    "        print(e.stderr)\n",
    "        return\n",
    "\n",
    "    # The C code always outputs 'blueprint.bin' to its own directory.\n",
    "    original_blueprint_path = os.path.join(project_dir, \"blueprint.bin\")\n",
    "    \n",
    "    # --- MODIFICATION: Move blueprint to the specified subfolder ---\n",
    "    # First, ensure the destination folder exists.\n",
    "    destination_folder = os.path.join(project_dir, bin_folder)\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "    # Construct the final path for the renamed blueprint.\n",
    "    new_blueprint_path = os.path.join(destination_folder, output_blueprint_name)\n",
    "\n",
    "    if os.path.exists(original_blueprint_path):\n",
    "        print(f\"Moving '{original_blueprint_path}' to '{new_blueprint_path}'...\")\n",
    "        shutil.move(original_blueprint_path, new_blueprint_path)\n",
    "        print(\"--- Run complete. ---\")\n",
    "    else:\n",
    "        print(\"Warning: 'blueprint.bin' was not created by the C program.\")\n",
    "\n",
    "print(\"Helper function `run_integrator_and_rename_blueprint` defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b385b9",
   "metadata": {},
   "source": [
    "# Animation Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "883eaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from IPython.display import display, Image as IPImage\n",
    "from typing import Union\n",
    "\n",
    "def generate_spinning_disk_animation_frames(\n",
    "    # --- Required Positional Arguments ---\n",
    "    num_frames: int,\n",
    "    total_animation_duration: float,\n",
    "    project_dir: str,\n",
    "    executable_name: str,\n",
    "    base_par_filename: str,\n",
    "    blueprint_folder: str,\n",
    "    frames_folder: str,\n",
    "    output_pixel_width: int,\n",
    "    source_image_width: float,\n",
    "    sphere_image: Union[str, np.ndarray],\n",
    "    source_image: Union[str, np.ndarray],\n",
    "    window_width: float,\n",
    "    \n",
    "    # --- Optional Keyword Arguments ---\n",
    "    start_time_offset: float = 0.0,\n",
    "    save_blueprints: bool = False,\n",
    "    **kwargs # For gamma, intensity_scale, etc.\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a sequence of frames for a spinning disk animation.\n",
    "    \n",
    "    This version includes a `save_blueprints` flag. If False (default), it\n",
    "    deletes the large temporary blueprint file for each frame after rendering,\n",
    "    saving significant disk space.\n",
    "    \"\"\"\n",
    "    print(f\"--- Generating {num_frames} Frames for Spinning Disk Animation ---\")\n",
    "    if not save_blueprints:\n",
    "        print(\"    (Temporary blueprints will be deleted after each frame is rendered)\")\n",
    "    \n",
    "    full_blueprint_dir = os.path.join(project_dir, blueprint_folder)\n",
    "    full_frames_dir = os.path.join(project_dir, frames_folder)\n",
    "    os.makedirs(full_blueprint_dir, exist_ok=True)\n",
    "    os.makedirs(full_frames_dir, exist_ok=True)\n",
    "\n",
    "    base_par_lines = []\n",
    "    with open(os.path.join(project_dir, base_par_filename), 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.strip().startswith(\"t_start\"):\n",
    "                base_par_lines.append(line)\n",
    "    base_par_content = \"\".join(base_par_lines)\n",
    "\n",
    "    # --- Main Animation Loop ---\n",
    "    for i in range(num_frames):\n",
    "        time_progress = (i / (num_frames - 1)) * total_animation_duration if num_frames > 1 else 0\n",
    "        current_t_start = start_time_offset + time_progress\n",
    "        \n",
    "        print(f\"\\n--- Processing Frame {i+1}/{num_frames}: t_start = {current_t_start:.2f} M ---\")\n",
    "        \n",
    "        # 1. Create temporary parameter file\n",
    "        temp_par_content = base_par_content + f\"\\nt_start = {current_t_start:.4f}\\n\"\n",
    "        temp_par_filename = \"temp_anim_frame.par\"\n",
    "        full_temp_par_path = os.path.join(project_dir, temp_par_filename)\n",
    "        with open(full_temp_par_path, \"w\") as f:\n",
    "            f.write(temp_par_content)\n",
    "\n",
    "        # 2. Run the C integrator\n",
    "        command_to_run = f\"./{executable_name} {temp_par_filename}\"\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                command_to_run, shell=True, capture_output=True, text=True, check=True, cwd=project_dir\n",
    "            )\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"ERROR: The C program exited with an error for frame {i}.\")\n",
    "            print(\"--- Standard Error ---\")\n",
    "            print(e.stderr)\n",
    "            continue\n",
    "\n",
    "        # 3. Move the output blueprint to its temporary location\n",
    "        original_blueprint_path = os.path.join(project_dir, \"light_blueprint.bin\")\n",
    "        blueprint_frame_name = f\"blueprint_frame_{i:04d}.bin\"\n",
    "        temp_blueprint_path = os.path.join(full_blueprint_dir, blueprint_frame_name)\n",
    "        \n",
    "        if os.path.exists(original_blueprint_path):\n",
    "            shutil.move(original_blueprint_path, temp_blueprint_path)\n",
    "        else:\n",
    "            print(f\"ERROR: C code ran but did not produce 'light_blueprint.bin'. Skipping rendering.\")\n",
    "            continue\n",
    "            \n",
    "        # 4. Render the image for this frame\n",
    "        image_frame_name = os.path.join(full_frames_dir, f\"frame_{i:04d}.png\")\n",
    "        \n",
    "        generate_static_lensed_image(\n",
    "            output_filename=image_frame_name,\n",
    "            output_pixel_width=output_pixel_width,\n",
    "            source_image_width=source_image_width,\n",
    "            sphere_image=sphere_image,\n",
    "            source_image=source_image,\n",
    "            blueprint_filename=temp_blueprint_path,\n",
    "            window_width=window_width,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # --- 5. NEW: Clean up the temporary blueprint file ---\n",
    "        if not save_blueprints:\n",
    "            if os.path.exists(temp_blueprint_path):\n",
    "                os.remove(temp_blueprint_path)\n",
    "                print(f\"  -> Deleted temporary blueprint: {blueprint_frame_name}\")\n",
    "\n",
    "    # --- Final Cleanup ---\n",
    "    # If we weren't saving blueprints, the blueprint folder should be empty.\n",
    "    # We can remove it to keep the project directory clean.\n",
    "    if not save_blueprints:\n",
    "        try:\n",
    "            # Check if the directory is empty before removing\n",
    "            if not os.listdir(full_blueprint_dir):\n",
    "                os.rmdir(full_blueprint_dir)\n",
    "        except OSError as e:\n",
    "            print(f\"Could not remove empty blueprint directory: {e}\")\n",
    "\n",
    "    print(f\"\\n--- All {num_frames} frames generated successfully in '{full_frames_dir}'. ---\")\n",
    "    \n",
    "    first_frame_path = os.path.join(full_frames_dir, \"frame_0000.png\")\n",
    "    if os.path.exists(first_frame_path):\n",
    "        print(\"Displaying first generated frame:\")\n",
    "        display(IPImage(filename=first_frame_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29a9087e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Execution stopped here intentionally.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExecution stopped here intentionally.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Execution stopped here intentionally."
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt(\"Execution stopped here intentionally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def compare_blueprint_files(\n",
    "    file1_path: str,\n",
    "    file2_path: str,\n",
    "    detailed_report: bool = False,\n",
    "    tolerance: float = 1e-9\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Compares two light_blueprint.bin files to check for differences.\n",
    "\n",
    "    This function performs a byte-for-byte comparison and, if they differ,\n",
    "    a detailed field-by-field numerical comparison to identify exactly\n",
    "    what has changed.\n",
    "\n",
    "    Args:\n",
    "        file1_path: Path to the first blueprint file.\n",
    "        file2_path: Path to the second blueprint file.\n",
    "        detailed_report: If True, prints the first few differing records.\n",
    "        tolerance: The absolute tolerance for floating-point comparisons.\n",
    "\n",
    "    Returns:\n",
    "        True if the files are considered identical, False otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"--- Comparing Blueprint Files ---\")\n",
    "    print(f\"File 1: {file1_path}\")\n",
    "    print(f\"File 2: {file2_path}\")\n",
    "\n",
    "    # --- 1. Basic File Checks ---\n",
    "    if not os.path.exists(file1_path):\n",
    "        print(f\"ERROR: File not found: {file1_path}\")\n",
    "        return False\n",
    "    if not os.path.exists(file2_path):\n",
    "        print(f\"ERROR: File not found: {file2_path}\")\n",
    "        return False\n",
    "\n",
    "    size1 = os.path.getsize(file1_path)\n",
    "    size2 = os.path.getsize(file2_path)\n",
    "\n",
    "    if size1 != size2:\n",
    "        print(f\"\\n[!] VERDICT: FILES ARE DIFFERENT (Sizes mismatch: {size1} vs {size2} bytes)\")\n",
    "        return False\n",
    "\n",
    "    # --- 2. Fast Byte-for-Byte Comparison ---\n",
    "    with open(file1_path, 'rb') as f1, open(file2_path, 'rb') as f2:\n",
    "        if f1.read() == f2.read():\n",
    "            print(\"\\n[✓] VERDICT: FILES ARE IDENTICAL (Byte-for-byte comparison passed).\")\n",
    "            print(\"This confirms the C code is producing deterministic output.\")\n",
    "            return True\n",
    "\n",
    "    print(\"\\n[!] NOTICE: Files have the same size but their byte content differs. Performing detailed analysis...\")\n",
    "\n",
    "    # --- 3. Detailed Numerical Comparison (if byte comparison fails) ---\n",
    "    # This dtype must match the one used for rendering.\n",
    "    BLUEPRINT_DTYPE = np.dtype([\n",
    "        ('termination_type', np.int32),\n",
    "        ('y_w', 'f8'), ('z_w', 'f8'),\n",
    "        ('stokes_I', 'f8'), ('lambda_observed', 'f8'),\n",
    "        ('y_s', 'f8'), ('z_s', 'f8'),\n",
    "        ('final_theta', 'f8'), ('final_phi', 'f8'),\n",
    "        ('L_w', 'f8'), ('t_w', 'f8'),\n",
    "        ('L_s', 'f8'), ('t_s', 'f8'),\n",
    "    ], align=False)\n",
    "\n",
    "    data1 = np.fromfile(file1_path, dtype=BLUEPRINT_DTYPE)\n",
    "    data2 = np.fromfile(file2_path, dtype=BLUEPRINT_DTYPE)\n",
    "\n",
    "    if len(data1) != len(data2):\n",
    "        print(f\"ERROR: Record count mismatch after loading: {len(data1)} vs {len(data2)}\")\n",
    "        return False\n",
    "\n",
    "    # Compare integer fields exactly\n",
    "    term_type_mismatch = np.any(data1['termination_type'] != data2['termination_type'])\n",
    "\n",
    "    # Compare float fields with a tolerance\n",
    "    float_fields = ['y_w', 'z_w', 'stokes_I', 'lambda_observed', 'y_s', 'z_s',\n",
    "                    'final_theta', 'final_phi', 'L_w', 't_w', 'L_s', 't_s']\n",
    "    \n",
    "    mismatches = {}\n",
    "    are_floats_different = False\n",
    "    for field in float_fields:\n",
    "        diff = np.abs(data1[field] - data2[field])\n",
    "        if np.any(diff > tolerance):\n",
    "            mismatches[field] = np.where(diff > tolerance)[0]\n",
    "            are_floats_different = True\n",
    "\n",
    "    if not term_type_mismatch and not are_floats_different:\n",
    "        print(\"\\n[✓] VERDICT: FILES ARE NUMERICALLY IDENTICAL (within tolerance).\")\n",
    "        print(\"The small byte differences are likely due to floating-point representation noise, which is acceptable.\")\n",
    "        return True\n",
    "    \n",
    "    print(\"\\n[!] VERDICT: FILES ARE NUMERICALLY DIFFERENT.\")\n",
    "    print(\"This indicates the C code output is NOT deterministic and depends on t_start.\")\n",
    "\n",
    "    if term_type_mismatch:\n",
    "        mismatch_indices = np.where(data1['termination_type'] != data2['termination_type'])[0]\n",
    "        print(f\"\\nFound {len(mismatch_indices)} records with mismatched termination types.\")\n",
    "        if detailed_report:\n",
    "            print(\"--- First 5 Mismatched Termination Records ---\")\n",
    "            for i in mismatch_indices[:5]:\n",
    "                print(f\"  Record {i}: Type 1 = {data1['termination_type'][i]}, Type 2 = {data2['termination_type'][i]}\")\n",
    "\n",
    "    if are_floats_different:\n",
    "        print(\"\\nFound differences in the following floating-point fields:\")\n",
    "        for field, indices in mismatches.items():\n",
    "            print(f\"  - Field '{field}': {len(indices)} mismatched records.\")\n",
    "            if detailed_report:\n",
    "                print(\"    --- First 5 Mismatched Float Records ---\")\n",
    "                for i in indices[:5]:\n",
    "                    print(f\"      Record {i}: Val 1 = {data1[field][i]:.6e}, Val 2 = {data2[field][i]:.6e}, Diff = {np.abs(data1[field][i] - data2[field][i]):.2e}\")\n",
    "\n",
    "    return False\n",
    "\n",
    "# --- How to Use This Cell ---\n",
    "# 1. Run your C code with t_start=0 and the camera pointed away from the disk.\n",
    "# 2. Rename the output: mv project/photon_geodesic_integrator/light_blueprint.bin project/photon_geodesic_integrator/blueprint_t0.bin\n",
    "# 3. Run your C code again with t_start=1000 (or any other value).\n",
    "# 4. Rename the output: mv project/photon_geodesic_integrator/light_blueprint.bin project/photon_geodesic_integrator/blueprint_t1000.bin\n",
    "# 5. Run this cell.\n",
    "\n",
    "# Define the paths to your two blueprint files\n",
    "blueprint_t0_path = \"project/photon_geodesic_integrator/light_blueprint.bin\"\n",
    "blueprint_t1000_path = \"project/photon_geodesic_integrator/light_blueprint_0.bin\"\n",
    "\n",
    "# Run the comparison with a detailed report\n",
    "are_identical = compare_blueprint_files(blueprint_t0_path, blueprint_t1000_path, detailed_report=True)\n",
    "\n",
    "if are_identical:\n",
    "    print(\"\\nCONCLUSION: The C code is exonerated. The issue is not in the geodesic integration.\")\n",
    "else:\n",
    "    print(\"\\nCONCLUSION: The C code is implicated. The integration results depend on t_start, which should not happen for a stationary metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac14320",
   "metadata": {},
   "source": [
    "# Start of visual function call cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c497c",
   "metadata": {},
   "source": [
    "# Blueprints stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Call the function with your blueprint file and desired bin width ---\n",
    "blueprint_filename=\"project/photon_geodesic_integrator/light_blueprint.bin\"\n",
    "#plot_stacked_radial_histogram(blueprint_filename=blueprint_filename, bin_width=0.01)\n",
    "\n",
    "\n",
    "analyze_blueprint()\n",
    "\n",
    "# --- Run the viewer ---\n",
    "view_binary_blueprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab59a53",
   "metadata": {},
   "source": [
    "# Setting all visualization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Initializing Master Configuration for Visualization & Animation ---\")\n",
    "\n",
    "# --- Core File & Path Settings ---\n",
    "\n",
    "# Get the user's home directory (e.g., /home/daltonm)\n",
    "home_dir = os.path.expanduser('~')\n",
    "\n",
    "# Path to the directory where the C projects live\n",
    "# Assumes your notebook is in ~/Documents/\n",
    "base_project_dir = os.path.join(home_dir, \"Documents\", \"project\")\n",
    "\n",
    "# Path to the specific light integrator project\n",
    "p_light_integrator_dir = os.path.join(base_project_dir, \"photon_geodesic_integrator\")\n",
    "\n",
    "# --- NEW: Define the absolute output directory ---\n",
    "# This will create /home/daltonm/Documents/Generated_nrpy_images/\n",
    "p_output_basedir = os.path.join(home_dir, \"Documents\", \"Generated_nrpy_images\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Physics & Scene Parameters ---\n",
    "p_mass_of_black_hole = 1.0\n",
    "p_window_width = 1.5\n",
    "\n",
    "# --- Source & Background Texture Settings ---\n",
    "p_sphere_texture_file = \"starmap_2020.png\"\n",
    "\n",
    "# --- Procedural Disk Generation Parameters ---\n",
    "p_disk_inner_radius = 6.0\n",
    "p_disk_outer_radius = 25.0\n",
    "p_colormap = 'afmhot'\n",
    "p_disk_temp_power_law = -1.5\n",
    "p_ring_num = 4\n",
    "p_ring_contrast = 0.7\n",
    "p_ring_log_spacing = True\n",
    "p_doppler_factor = 0.3\n",
    "p_doppler_power = 3\n",
    "p_hotspot_num = 2\n",
    "p_hotspot_amplitude = 0.4\n",
    "p_hotspot_radius_center = 10.0\n",
    "p_hotspot_radius_width = 4.0\n",
    "p_shape_num_lobes = 0\n",
    "p_shape_inner_amplitude = 0.0\n",
    "p_shape_outer_amplitude = 0.0\n",
    "p_source_physical_width = 2 * (p_disk_outer_radius + p_shape_outer_amplitude)\n",
    "\n",
    "# --- General Image Quality & Rendering Settings ---\n",
    "p_static_image_pixel_width = 400\n",
    "p_animation_pixel_width = 400\n",
    "p_intensity_scale = 20.0\n",
    "p_gamma = 2.2\n",
    "p_lambda_min_nm = 500  # Fixed min wavelength for color consistency\n",
    "p_lambda_max_nm = 2250  # Fixed max wavelength\n",
    "\n",
    "\n",
    "# Path to the specific light integrator project\n",
    "p_light_integrator_dir = os.path.join(base_project_dir, \"photon_geodesic_integrator\")\n",
    "p_blueprint_filename = os.path.join(p_light_integrator_dir, \"light_blueprint.bin\")\n",
    "\n",
    "# --- Animation Settings ---\n",
    "p_anim_name = \"detailed_spiral\" # A descriptive name for this animation run\n",
    "# Subfolder for the individual frames\n",
    "p_anim_frames_folder = os.path.join(p_output_basedir, f\"{p_anim_name}_frames\")\n",
    "# Subfolder for the temporary blueprints (can be deleted after)\n",
    "p_anim_blueprint_folder = os.path.join(p_light_integrator_dir, f\"{p_anim_name}_blueprints\")\n",
    "# Final output filename for the video\n",
    "p_anim_video_file = os.path.join(p_output_basedir, f\"{p_anim_name}.mp4\")\n",
    "\n",
    "\n",
    "# --- CORRECTED PATH DEFINITIONS ---\n",
    "\n",
    "# The folder for temporary blueprints should be in the shared project directory\n",
    "blueprint_folder = os.path.join(base_project_dir, f\"{p_anim_name}_blueprints\")\n",
    "\n",
    "# The folder for the final PNG frames is already correctly defined in the master config\n",
    "frames_folder = p_anim_frames_folder \n",
    "# --- END CORRECTION ---\n",
    "\n",
    "\n",
    "p_anim_num_frames = 150\n",
    "p_anim_orbits_at_isco = 2.0\n",
    "p_anim_is_prograde = True\n",
    "p_anim_start_time = 70.0 # The starting coordinate time for the animation\n",
    "\n",
    "print(f\"Master configuration loaded.\")\n",
    "print(f\"All animation output will be saved in: {p_output_basedir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba0b21",
   "metadata": {},
   "source": [
    "# Disk Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_disk_texture = generate_advanced_disk_array(\n",
    "    pixel_width=1024, # High resolution for the source\n",
    "    disk_physical_width=p_source_physical_width,\n",
    "    colormap=p_colormap,\n",
    "    disk_inner_radius=p_disk_inner_radius,\n",
    "    disk_outer_radius=p_disk_outer_radius,\n",
    "    disk_temp_power_law=p_disk_temp_power_law,\n",
    "    ring_num=p_ring_num,\n",
    "    ring_contrast=p_ring_contrast,\n",
    "    ring_log_spacing=p_ring_log_spacing,\n",
    "    doppler_factor=p_doppler_factor,\n",
    "    doppler_power=p_doppler_power,\n",
    "    hotspot_num=p_hotspot_num,\n",
    "    shape_num_lobes=p_shape_num_lobes,\n",
    "    shape_inner_amplitude=p_shape_inner_amplitude,\n",
    "    shape_outer_amplitude=p_shape_outer_amplitude,\n",
    "    display_image=False # Show the disk we're about to render\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86facc9",
   "metadata": {},
   "source": [
    "# Standard Static Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Generating a Standard Static Lensed Image from Radiative Transfer Data ---\")\n",
    "\n",
    "# --- 1. Define all inputs for the function call ---\n",
    "# All parameters are now read from the master configuration cell (prefix 'p_').\n",
    "\n",
    "# Define a unique name for the static image file\n",
    "static_image_name = \"carestain_test_0.png\"\n",
    "p_static_image_pixel_width = 400\n",
    "output_filename = os.path.join(p_output_basedir, static_image_name)\n",
    "\n",
    "# Choose the source texture. This can be a filename or a pre-generated numpy array.\n",
    "# For this example, we use the advanced disk generated in a previous cell.\n",
    "try:\n",
    "    source_image_texture = source_disk_texture\n",
    "except NameError:\n",
    "    print(\"Warning: `advanced_disk_data` not found. Generating a default disk for this static image.\")\n",
    "    source_image_texture = generate_advanced_disk_array(display_image=False)\n",
    "\n",
    "# --- 2. The Function Call ---\n",
    "# This call now uses all the parameters defined in your master configuration cell.\n",
    "generate_static_lensed_image(\n",
    "    output_filename=output_filename,\n",
    "    output_pixel_width=p_static_image_pixel_width,\n",
    "    source_image_width=p_source_physical_width,\n",
    "    sphere_image=p_sphere_texture_file,\n",
    "    source_image=source_image_texture,\n",
    "    intensity_scale=p_intensity_scale,\n",
    "    lambda_min_nm=p_lambda_min_nm,\n",
    "    lambda_max_nm=p_lambda_max_nm,\n",
    "    gamma=p_gamma,\n",
    "    blueprint_filename=p_blueprint_filename,\n",
    "    window_width=p_window_width\n",
    ")\n",
    "\n",
    "# --- 3. Display the result ---\n",
    "if os.path.exists(output_filename):\n",
    "    print(f\"\\nDisplaying static image: '{output_filename}'\")\n",
    "    display(IPImage(filename=output_filename))\n",
    "else:\n",
    "    print(f\"\\nERROR: Image file was not created at '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a295dc",
   "metadata": {},
   "source": [
    "# Final Video Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af13cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINAL ANIMATION EXECUTION CELL ---\n",
    "\n",
    "# 1. Generate the source disk texture to be used for rendering\n",
    "# This is done once before the main loop.\n",
    "print(\"--- Generating Source Disk Texture ---\")\n",
    "source_disk_texture = generate_advanced_disk_array(\n",
    "    pixel_width=1024, # High resolution for the source\n",
    "    disk_physical_width=p_source_physical_width,\n",
    "    colormap=p_colormap,\n",
    "    disk_inner_radius=p_disk_inner_radius,\n",
    "    disk_outer_radius=p_disk_outer_radius,\n",
    "    disk_temp_power_law=p_disk_temp_power_law,\n",
    "    ring_num=p_ring_num,\n",
    "    ring_contrast=p_ring_contrast,\n",
    "    ring_log_spacing=p_ring_log_spacing,\n",
    "    doppler_factor=p_doppler_factor,\n",
    "    doppler_power=p_doppler_power,\n",
    "    hotspot_num=p_hotspot_num,\n",
    "    shape_num_lobes=p_shape_num_lobes,\n",
    "    shape_inner_amplitude=p_shape_inner_amplitude,\n",
    "    shape_outer_amplitude=p_shape_outer_amplitude,\n",
    "    display_image=False # Show the disk we're about to render\n",
    ")\n",
    "\n",
    "# 2. Calculate animation time window\n",
    "isco_radius = 6.0 * p_mass_of_black_hole\n",
    "orbital_period_at_isco = 2 * np.pi * np.sqrt(isco_radius**3 / p_mass_of_black_hole)\n",
    "animation_duration = orbital_period_at_isco * p_anim_orbits_at_isco\n",
    "animation_end_time = p_anim_start_time + animation_duration\n",
    "\n",
    "print(f\"\\nAnimation will run from t={p_anim_start_time:.2f} M to t={animation_end_time:.2f} M (Duration: {animation_duration:.2f} M).\")\n",
    "\n",
    "# --- Important: Do you want to save the light blueprints?\n",
    "save_blueprints= False\n",
    "\n",
    "# 3. Generate all the PNG frames for the animation\n",
    "generate_spinning_disk_animation_frames(\n",
    "    num_frames=p_anim_num_frames,\n",
    "    total_animation_duration=animation_duration,\n",
    "    start_time_offset=p_anim_start_time,\n",
    "    project_dir=p_light_integrator_dir, # The C code still runs from its own directory\n",
    "    executable_name=\"photon_geodesic_integrator\",\n",
    "    base_par_filename=\"photon_geodesic_integrator.par\",\n",
    "    blueprint_folder=blueprint_folder, # Pass the corrected, absolute path\n",
    "    frames_folder=frames_folder,\n",
    "    output_pixel_width=p_animation_pixel_width,\n",
    "    source_image_width=p_source_physical_width,\n",
    "    sphere_image=p_sphere_texture_file,\n",
    "    source_image=source_disk_texture,\n",
    "    window_width=p_window_width,\n",
    "    gamma=p_gamma,\n",
    "    lambda_min_nm=p_lambda_min_nm,\n",
    "    lambda_max_nm=p_lambda_max_nm,\n",
    "    intensity_scale=p_intensity_scale,\n",
    "    save_blueprints= save_blueprints\n",
    ")\n",
    "# 4. Encode the generated frames into an MP4 video\n",
    "encode_video_from_frames(\n",
    "    image_folder=p_anim_frames_folder,\n",
    "    output_video_path=p_anim_video_file,\n",
    "    frame_rate=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea76a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_video_from_frames(\n",
    "    image_folder=p_anim_frames_folder,\n",
    "    output_video_path=p_anim_video_file,\n",
    "    frame_rate=8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jupyter notebook)",
   "language": "python",
   "name": "docs-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
