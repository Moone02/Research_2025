{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba92b65f",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bac0cb",
   "metadata": {},
   "source": [
    "# Table of Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c0d30",
   "metadata": {},
   "source": [
    "\n",
    "#  Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f07e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_5_mass_geodesic.ipynb, Cell ID 33f07e1c (UPDATED)\n",
    "import os\n",
    "import shutil\n",
    "import sympy as sp\n",
    "import nrpy.c_function as cfc\n",
    "import nrpy.c_codegen as ccg\n",
    "import nrpy.params as par\n",
    "import nrpy.indexedexp as ixp\n",
    "import nrpy.infrastructures.BHaH.BHaH_defines_h as Bdefines_h\n",
    "import nrpy.infrastructures.BHaH.Makefile_helpers as Makefile\n",
    "from nrpy.infrastructures.BHaH import cmdline_input_and_parfiles\n",
    "import nrpy.helpers.generic as gh\n",
    "import nrpy.infrastructures.BHaH.CodeParameters as CPs\n",
    "\n",
    "project_name = \"mass_integrator\"\n",
    "project_dir = os.path.join(\"project\", project_name)\n",
    "shutil.rmtree(project_dir, ignore_errors=True)\n",
    "\n",
    "par.set_parval_from_str(\"Infrastructure\", \"BHaH\")\n",
    "\n",
    "# --- Physical Parameters ---\n",
    "M_scale = par.CodeParameter(\"REAL\", __name__, \"M_scale\", 1.0, commondata=True, add_to_parfile=True, add_to_set_CodeParameters_h=True)\n",
    "a_spin = par.CodeParameter(\"REAL\", __name__, \"a_spin\", 0.9, commondata=True, add_to_parfile=True, add_to_set_CodeParameters_h=True)\n",
    "\n",
    "metric_choice = par.CodeParameter(\n",
    "    \"int\", __name__, \"metric_choice\", 0,\n",
    "    commondata=True, add_to_parfile=True\n",
    ")\n",
    "\n",
    "# --- Integration & Termination Parameters ---\n",
    "t_max_integration = par.CodeParameter(\"REAL\", __name__, \"t_max_integration\", 2000.0, commondata=True, add_to_parfile=True)\n",
    "flatness_threshold = par.CodeParameter(\"REAL\", __name__, \"flatness_threshold\", 1e-2, commondata=True, add_to_parfile=True)\n",
    "r_escape = par.CodeParameter(\"REAL\", __name__, \"r_escape\", 1500.0, commondata=True, add_to_parfile=True)\n",
    "ut_max = par.CodeParameter(\"REAL\", __name__, \"ut_max\", 1e3, commondata=True, add_to_parfile=True)\n",
    "\n",
    "# --- Debugging & Validation Parameters ---\n",
    "perform_conservation_check = par.CodeParameter(\"bool\", __name__, \"perform_conservation_check\", True, commondata=True, add_to_parfile=True)\n",
    "run_in_debug_mode = par.CodeParameter(\"bool\", __name__, \"run_in_debug_mode\", True, commondata=True, add_to_parfile=True)\n",
    "\n",
    "# --- Disk Parameters ---\n",
    "disk_lambda_rest_at_r_min = par.CodeParameter(\"REAL\", __name__, \"disk_lambda_rest_at_r_min\", 656.3, commondata=True, add_to_parfile=True)\n",
    "disk_num_r= par.CodeParameter(\"int\", __name__, \"disk_num_r\", 100, commondata=True, add_to_parfile=True)\n",
    "disk_num_phi= par.CodeParameter(\"int\", __name__, \"disk_num_phi\", 200, commondata=True, add_to_parfile=True)\n",
    "disk_r_min = par.CodeParameter(\"REAL\", __name__, \"disk_r_min\", 6.0, commondata=True, add_to_parfile=True)\n",
    "disk_r_max = par.CodeParameter(\"REAL\", __name__, \"disk_r_max\", 25.0, commondata=True, add_to_parfile=True)\n",
    "snapshot_every_t = par.CodeParameter(\"REAL\", __name__, \"snapshot_every_t\", 10.0, commondata=True, add_to_parfile=True)\n",
    "t_final = par.CodeParameter(\"REAL\", __name__, \"t_final\", 2000.0, commondata=True, add_to_parfile=True)\n",
    "output_folder = par.CodeParameter(\"char[100]\", __name__, \"output_folder\", \"output\", commondata=True, add_to_parfile=True)\n",
    "\n",
    "# --- : Barred Flocculent Spiral Galaxy Shape Parameters ---\n",
    "print(\"-> Registering CodeParameters for barred flocculent spiral galaxy geometry...\")\n",
    "bar_length = par.CodeParameter(\"REAL\", __name__, \"bar_length\", 5.0, commondata=True, add_to_parfile=True)\n",
    "bar_aspect_ratio = par.CodeParameter(\"REAL\", __name__, \"bar_aspect_ratio\", 0.25, commondata=True, add_to_parfile=True) # width / length\n",
    "bulge_radius = par.CodeParameter(\"REAL\", __name__, \"bulge_radius\", 1.5, commondata=True, add_to_parfile=True)\n",
    "arm_particle_density = par.CodeParameter(\"REAL\", __name__, \"arm_particle_density\", 0.3, commondata=True, add_to_parfile=True) # Base probability of a particle being in an arm\n",
    "arm_clumpiness_factor = par.CodeParameter(\"REAL\", __name__, \"arm_clumpiness_factor\", 8.0, commondata=True, add_to_parfile=True) # How many clumps per arm rotation\n",
    "arm_clump_size = par.CodeParameter(\"REAL\", __name__, \"arm_clump_size\", 0.5, commondata=True, add_to_parfile=True) # How tight the clumps are\n",
    "bar_density_factor = par.CodeParameter(\"REAL\", __name__, \"bar_density_factor\", 2.0, commondata=True, add_to_parfile=True) # Bar is 2x denser than arms\n",
    "bulge_density_factor = par.CodeParameter(\"REAL\", __name__, \"bulge_density_factor\", 3.0, commondata=True, add_to_parfile=True) # Bulge is 3x denser\n",
    "\n",
    "# --- : Spiral Galaxy Shape Parameters ---\n",
    "print(\"-> Registering CodeParameters for spiral galaxy geometry...\")\n",
    "spiral_galaxy_num_arms = par.CodeParameter(\n",
    "    \"int\", __name__, \"spiral_galaxy_num_arms\", 2,\n",
    "    commondata=True, add_to_parfile=True\n",
    ")\n",
    "spiral_galaxy_arm_tightness = par.CodeParameter(\n",
    "    \"REAL\", __name__, \"spiral_galaxy_arm_tightness\", 0.2,\n",
    "    commondata=True, add_to_parfile=True\n",
    ")\n",
    "\n",
    "# --- : Initial Conditions Type Selector ---\n",
    "print(\"-> Registering CodeParameter for selecting initial conditions type...\")\n",
    "initial_conditions_type = par.CodeParameter(\"char[100]\", __name__, \"initial_conditions_type\", \"KeplerianDisk\",commondata=True, add_to_parfile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84ac02",
   "metadata": {},
   "source": [
    "<a id='symbolic_core'></a>\n",
    "# Step 3: The Symbolic Core - Foundational Math\n",
    "\n",
    "This section defines the pure mathematical logic of our problem using Python's `sympy` library. Each function in this section is a \"blueprint\" for a physical calculation. These functions take symbolic `sympy` objects as input and return new symbolic expressions as output. They have no knowledge of C code; they are concerned only with mathematics and will be called later to generate the \"recipes\" for our C code engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b659c",
   "metadata": {},
   "source": [
    "<a id='deriv_g4DD'></a>\n",
    "### 3.a: Metric Tensor Derivatives\n",
    "\n",
    "The first step in calculating the Christoffel symbols is to compute the partial derivatives of the metric tensor, $g_{\\mu\\nu}$. This function, `derivative_g4DD`, takes the symbolic 4x4 metric tensor `g4DD` and a list of the four coordinate symbols `xx` as input.\n",
    "\n",
    "The function iterates through all components to symbolically calculate the partial derivative of each metric component with respect to each coordinate. The resulting quantity, which we can denote using comma notation as $g_{\\mu\\nu,\\alpha}$, is defined as:\n",
    "\n",
    "$$ g_{\\mu\\nu,\\alpha} \\equiv \\frac{\\partial g_{\\mu\\nu}}{\\partial x^{\\alpha}} $$\n",
    "\n",
    "The nested `for` loops in the code directly correspond to the spacetime indices `μ, ν, α` in the physics equation. `sympy`'s built-in `sp.diff()` function is used to perform the symbolic differentiation, and the final result is returned as a rank-3 symbolic tensor.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank3(dimension)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: This function creates a symbolic rank-3 tensor (a Python list of lists of lists) of a specified dimension, with all elements initialized to the `sympy` integer 0. It is used here to create a container for the derivative results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_g4DD(g4DD, xx):\n",
    "    \"\"\"Computes the symbolic first derivatives of the metric tensor.\"\"\"\n",
    "    g4DD_dD = ixp.zerorank3(dimension=4)\n",
    "    for nu in range(4):\n",
    "        for mu in range(4):\n",
    "            for alpha in range(4):\n",
    "                g4DD_dD[nu][mu][alpha] = sp.diff(g4DD[nu][mu], xx[alpha])\n",
    "    return g4DD_dD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79893b7",
   "metadata": {},
   "source": [
    "<a id='four_connections'></a>\n",
    "### 3.b: Christoffel Symbol Calculation\n",
    "\n",
    "This function implements the core formula for the Christoffel symbols of the second kind, $\\Gamma^{\\delta}_{\\mu\\nu}$. It takes the symbolic metric tensor `g4DD` ($g_{\\mu\\nu}$) and its derivatives `g4DD_dD` ($g_{\\mu\\nu,\\alpha}$) as input. The calculation requires the inverse metric, $g^{\\mu\\nu}$, which is computed using another `nrpy` helper function.\n",
    "\n",
    "The function then applies the well-known formula for the Christoffel symbols. Using the comma notation for partial derivatives, the formula is:\n",
    "\n",
    "$$ \\Gamma^{\\delta}_{\\mu\\nu} = \\frac{1}{2} g^{\\delta\\alpha} \\left( g_{\\nu\\alpha,\\mu} + g_{\\mu\\alpha,\\nu} - g_{\\mu\\nu,\\alpha} \\right) $$\n",
    "\n",
    "The Python `for` loops iterate over the spacetime indices `δ, μ, ν, α` to construct each component of the Christoffel symbol tensor. After the summation is complete, the `sp.trigsimp()` function is used to simplify the resulting expression. This trigonometric simplification is highly effective and much faster than a general `sp.simplify()` for the Kerr-Schild metric, which contains trigonometric functions of the coordinates.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank3(dimension)`**: Previously introduced. Used to initialize the Christoffel symbol tensor.\n",
    "*   **`nrpy.indexedexp.symm_matrix_inverter4x4(g4DD)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: This function takes a symbolic 4x4 symmetric matrix and analytically computes its inverse. It is highly optimized for this specific task, returning both the inverse matrix ($g^{\\mu\\nu}$) and its determinant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_connections(g4DD, g4DD_dD):\n",
    "    \"\"\"\n",
    "    Computes and simplifies Christoffel symbols from the metric and its derivatives.\n",
    "    \n",
    "    This version uses sp.trigsimp() which is highly effective and much faster\n",
    "    than sp.simplify() for the Kerr-Schild metric.\n",
    "    \"\"\"\n",
    "    Gamma4UDD = ixp.zerorank3(dimension=4)\n",
    "    g4UU, _ = ixp.symm_matrix_inverter4x4(g4DD)\n",
    "    \n",
    "    for mu in range(4):\n",
    "        for nu in range(4):\n",
    "            for delta in range(4):\n",
    "                # Calculate the Christoffel symbol component using the standard formula\n",
    "                for alpha in range(4):\n",
    "                    Gamma4UDD[delta][mu][nu] += sp.Rational(1, 2) * g4UU[delta][alpha] * \\\n",
    "                        (g4DD_dD[nu][alpha][mu] + g4DD_dD[mu][alpha][nu] - g4DD_dD[mu][nu][alpha])\n",
    "                \n",
    "                # Use sp.trigsimp() to simplify the resulting expression.\n",
    "                # This is the key to speeding up the symbolic calculation.\n",
    "                Gamma4UDD[delta][mu][nu] = sp.trigsimp(Gamma4UDD[delta][mu][nu])\n",
    "\n",
    "    return Gamma4UDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59295c0b",
   "metadata": {},
   "source": [
    "<a id='geodesic_mom_rhs'></a>\n",
    "### 3.c: Geodesic Momentum RHS\n",
    "\n",
    "This function defines the symbolic right-hand side (RHS) for the evolution of the **reverse-time momentum**, $p^{\\alpha}$. As established in the introduction, this is the second of our three first-order ODEs:\n",
    "$$ \\frac{dp^{\\alpha}}{d\\kappa} = -\\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "The function `geodesic_mom_rhs` takes the symbolic Christoffel symbols $\\Gamma^{\\alpha}_{\\mu\\nu}$ as its input. It then defines the symbolic momentum vector `pU` using `sympy`'s `sp.symbols()` function. A key `nrpy` technique is used here: the symbols are created with names that are already valid C array syntax (e.g., `\"y[4]\"`). This \"direct naming\" simplifies the final C code generation by eliminating the need for string substitutions.\n",
    "\n",
    "The core of this function constructs the symbolic expression for the RHS by performing the Einstein summation $-\\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu}$. A direct implementation would involve a double loop over both $\\mu$ and $\\nu$ from 0 to 3, resulting in $4 \\times 4 = 16$ terms for each component of $\\alpha$, which is computationally inefficient.\n",
    "\n",
    "However, we can significantly optimize this calculation by exploiting symmetry. The term $p^{\\mu} p^{\\nu}$ is symmetric with respect to the interchange of the indices $\\mu$ and $\\nu$. The Christoffel symbols $\\Gamma^{\\alpha}_{\\mu\\nu}$ are also symmetric in their lower two indices. Therefore, the full sum can be split into diagonal ($\\mu=\\nu$) and off-diagonal ($\\mu \\neq \\nu$) terms:\n",
    "$$ \\sum_{\\mu,\\nu} \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} = \\sum_{\\mu=0}^{3} \\Gamma^{\\alpha}_{\\mu\\mu} (p^{\\mu})^2 + \\sum_{\\mu \\neq \\nu} \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "The second sum over $\\mu \\neq \\nu$ contains pairs of identical terms (e.g., the $\\mu=1, \\nu=2$ term is the same as the $\\mu=2, \\nu=1$ term). We can combine all such pairs by summing over only one of the cases (e.g., $\\mu < \\nu$) and multiplying by two:\n",
    "$$ \\sum_{\\mu,\\nu} \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} = \\sum_{\\mu=0}^{3} \\Gamma^{\\alpha}_{\\mu\\mu} (p^{\\mu})^2 + 2 \\sum_{\\mu < \\nu} \\Gamma^{\\alpha}_{\\mu\\nu} p^{\\mu} p^{\\nu} $$\n",
    "The Python code implements this optimized version, ensuring that each component of the RHS is computed with the minimum number of floating point operations, leading to more efficient C code.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank1(dimension)`**:\n",
    "    *   **Source File**: `nrpy/indexedexp.py`\n",
    "    *   **Description**: Creates a symbolic rank-1 tensor (a Python list) of a specified dimension, with all elements initialized to the `sympy` integer 0. It is used here to create a container for the four components of the momentum RHS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic_vel_rhs_massive():\n",
    "    \"\"\"\n",
    "    Symbolic RHS for massive particle velocity ODE: du^a/dτ = -Γ^a_μν u^μ u^ν.\n",
    "    u is the 4-velocity, y[4]...y[7].\n",
    "    \"\"\"\n",
    "    Gamma4UDD = ixp.declarerank3(\"conn->Gamma4UDD\",dimension= 4,sym=\"sym12\")\n",
    "    ut,ux,uy,uz = sp.symbols(\"y[4] y[5] y[6] y[7]\", Real=True)\n",
    "    uU = [ut,ux,uy,uz]\n",
    "    geodesic_rhs = ixp.zerorank1(dimension=4)\n",
    "    for alpha in range(4):\n",
    "        for mu in range(4):\n",
    "            geodesic_rhs[alpha] += Gamma4UDD[alpha][mu][mu] * uU[mu] * uU[mu]\n",
    "            for nu in range(mu + 1, 4):\n",
    "                geodesic_rhs[alpha] += 2 * Gamma4UDD[alpha][mu][nu] * uU[mu] * uU[nu]\n",
    "        geodesic_rhs[alpha] = -geodesic_rhs[alpha]\n",
    "    return geodesic_rhs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02fcc6",
   "metadata": {},
   "source": [
    "<a id='geodesic_pos_rhs'></a>\n",
    "### 3.d: Geodesic Position RHS\n",
    "\n",
    "This function defines the symbolic right-hand side (RHS) for the evolution of the position coordinates, $x^{\\alpha}$. As derived in the introduction, this is the first of our three first-order ODEs:\n",
    "\n",
    "$$ \\frac{dx^{\\alpha}}{d\\kappa} = p^{\\alpha} $$\n",
    "\n",
    "The Python function `geodesic_pos_rhs` is straightforward. It defines the components of the reverse-time momentum vector, `pU`, using `sympy`'s `sp.symbols()` function with the \"direct naming\" convention (`y[4]`, `y[5]`, etc.). It then simply returns a list containing these momentum components. This list of four symbolic expressions will serve as the first four components of the complete 9-component RHS vector that our C code will solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic_pos_rhs_massive():\n",
    "    \"\"\"\n",
    "    Symbolic RHS for position ODE: dx^a/dτ = u^a.\n",
    "    u is the 4-velocity, y[4]...y[7].\n",
    "    \"\"\"\n",
    "    ut,ux,uy,uz = sp.symbols(\"y[4] y[5] y[6] y[7]\", Real=True)\n",
    "    uU = [ut,ux,uy,uz]\n",
    "    return uU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896fc9e6",
   "metadata": {},
   "source": [
    "<a id='geodesic_mom0_calc'></a>\n",
    "### 3.f: Symbolic Calculation of u^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518283e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ut_massive():\n",
    "    \"\"\"\n",
    "    Symbolically derives u^t for a MASSIVE particle from the 4-velocity.\n",
    "    The derivation comes from solving the timelike normalization condition g_μν u^μ u^ν = -1.\n",
    "    \"\"\"\n",
    "    # The symbolic recipe will use the standard variable names u0, u1, etc.\n",
    "    # The C-generating function will map y[4], y[5], etc. to these.\n",
    "    u0,u1,u2,u3 = sp.symbols(\"u0 u1 u2 u3\", Real=True)\n",
    "    uU=[u0,u1,u2,u3]\n",
    "    \n",
    "    # The recipe uses the standard name \"metric\" for the struct.\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "\n",
    "    # This is the quadratic equation for u^0, derived from g_μν u^μ u^ν = -1\n",
    "    # g_00(u^0)^2 + 2g_0i u^0 u^i + g_ij u^i u^j = -1\n",
    "    # We solve for u^0.\n",
    "    sum_g0i_ui = sp.sympify(0)\n",
    "    for i in range(1,4):\n",
    "        sum_g0i_ui += g4DD[0][i]*uU[i]\n",
    "        \n",
    "    sum_gij_ui_uj = sp.sympify(0)\n",
    "    for i in range(1,4):\n",
    "        sum_gij_ui_uj += g4DD[i][i]*uU[i]*uU[i]\n",
    "        for j in range(i+1,4):\n",
    "            sum_gij_ui_uj += 2*g4DD[i][j]*uU[i]*uU[j]\n",
    "            \n",
    "    # The discriminant of the quadratic formula for u^0\n",
    "    # CORRECTED: This now includes the \"+1\" term from g_μν u^μ u^ν = -1\n",
    "    discriminant = sum_g0i_ui**2 - g4DD[0][0]*(sum_gij_ui_uj + 1)\n",
    "    \n",
    "    # We choose the positive root for a forward-in-time particle outside the horizon.\n",
    "    # Note: Your choice of the minus sign was correct for the final expression.\n",
    "    answer = (-sum_g0i_ui - sp.sqrt(discriminant)) / g4DD[0][0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8d45d",
   "metadata": {},
   "source": [
    "# Symbolic g_tt,g_tphi,g_phiphi values for numerically stable u^t,u^x,u^y on xy plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_5_mass_geodesic.ipynb, add as a NEW CELL\n",
    "\n",
    "def symbolic_kerr_metric_equatorial_boyer_lindquist():\n",
    "    \"\"\"\n",
    "    Provides the symbolic components of the Kerr metric in Boyer-Lindquist\n",
    "    coordinates, specialized to the equatorial plane (theta = pi/2).\n",
    "    \n",
    "    This is a necessary helper for the numerically stable initial condition calculation.\n",
    "    \"\"\"\n",
    "    # Define symbolic variables for Boyer-Lindquist coordinates and parameters\n",
    "    r, M, a = sp.symbols(\"r M_scale a_spin\", real=True)\n",
    "    \n",
    "    # In the equatorial plane, sin(theta) = 1 and cos(theta) = 0.\n",
    "    # The Kerr metric components simplify considerably.\n",
    "    \n",
    "    # g_tt = -(1 - 2M/r)\n",
    "    g_tt = -(1 - 2*M/r)\n",
    "    \n",
    "    # g_tphi = -2*a*M/r\n",
    "    g_tphi = -2*a*M/r\n",
    "    \n",
    "    # g_phiphi = (r^2 + a^2 + 2*M*a^2/r)\n",
    "    g_phiphi = r**2 + a**2 + (2*M*a**2)/r\n",
    "    \n",
    "    return g_tt, g_tphi, g_phiphi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7faa6c4",
   "metadata": {},
   "source": [
    "# Symbolic ut,uphi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_5_mass_geodesic.ipynb, replace cells [ceff5579] and [cbc7fdf2]\n",
    "# with this single cell.\n",
    "\n",
    "def symbolic_ut_uphi_from_r_stable():\n",
    "    \"\"\"\n",
    "    Symbolically derives u^t and u^phi for a circular, equatorial orbit\n",
    "    using a NUMERICALLY STABLE three-step method.\n",
    "\n",
    "    This definitive version constructs all expressions using the final C-level\n",
    "    parameter names directly, avoiding the need for .subs() and adhering to\n",
    "    the nrpy design pattern.\n",
    "    \n",
    "    Returns a list containing the symbolic expressions for [u^t, u^phi].\n",
    "    \"\"\"\n",
    "    # --- Step 0: Define symbolic variables with final C names ---\n",
    "    # These symbols directly correspond to the variables that will be\n",
    "    # available in the C function that uses this recipe.\n",
    "    r, M, a = sp.symbols(\"r_initial M_scale a_spin\", real=True)\n",
    "    \n",
    "    # --- Step 1: Calculate the stable angular velocity Omega = d(phi)/dt ---\n",
    "    Omega = sp.sqrt(M) / (r**sp.Rational(3, 2) + a * sp.sqrt(M))\n",
    "    \n",
    "    # --- Step 2: Define the required metric components using the same symbols ---\n",
    "    # These are the Kerr metric components in Boyer-Lindquist coordinates,\n",
    "    # specialized to the equatorial plane (theta = pi/2).\n",
    "    g_tt = -(1 - 2*M/r)\n",
    "    g_tphi = -2*a*M/r\n",
    "    g_phiphi = r**2 + a**2 + (2*M*a**2)/r\n",
    "\n",
    "    # --- Step 3: Solve for u^t using the 4-velocity normalization condition ---\n",
    "    # (u^t)^2 = -1 / (g_tt + 2*g_tphi*Omega + g_phiphi*Omega^2)\n",
    "    ut_squared_inv_denom = g_tt + 2*g_tphi*Omega + g_phiphi*Omega**2\n",
    "    ut_squared = -1 / ut_squared_inv_denom\n",
    "    ut = sp.sqrt(ut_squared)\n",
    "    \n",
    "    # --- Step 4: Calculate u^phi ---\n",
    "    # u^phi = Omega * u^t\n",
    "    uphi = Omega * ut\n",
    "    \n",
    "    return [ut, uphi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9bc84",
   "metadata": {},
   "source": [
    "# Markdown for conserved Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ef962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolic_energy():\n",
    "    \"\"\"\n",
    "    Computes the symbolic expression for conserved energy E = -p_t.\n",
    "    E = -g_{t,mu} p^mu\n",
    "    \"\"\"\n",
    "    # Define the 4-momentum components using the y[4]...y[7] convention\n",
    "    pt, px, py, pz = sp.symbols(\"y[4] y[5] y[6] y[7]\", real=True)\n",
    "    pU = [pt, px, py, pz]\n",
    "    \n",
    "    # Define an abstract metric tensor to be filled by a C struct at runtime\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "    \n",
    "    # Calculate p_t = g_{t,mu} p^mu\n",
    "    p_t = sp.sympify(0)\n",
    "    for mu in range(4):\n",
    "        p_t += g4DD[0][mu] * pU[mu]\n",
    "        \n",
    "    return -p_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52801f14",
   "metadata": {},
   "source": [
    "# Markdown for conserved L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolic_L_components_cart():\n",
    "    \"\"\"\n",
    "    Computes the symbolic expressions for the three components of angular momentum,\n",
    "    correctly accounting for the symmetry of the metric tensor.\n",
    "    \"\"\"\n",
    "    # Define coordinate and 4-momentum components\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    pt, px, py, pz = sp.symbols(\"y[4] y[5] y[6] y[7]\", real=True)\n",
    "    pU = [pt, px, py, pz]\n",
    "    \n",
    "    # Define an abstract metric tensor\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "    \n",
    "    # --- THIS IS THE CORE FIX ---\n",
    "    # Calculate covariant momentum components p_k = g_{k,mu} p^mu,\n",
    "    # correctly exploiting the metric's symmetry g_mu,nu = g_nu,mu.\n",
    "    p_down = ixp.zerorank1(dimension=4)\n",
    "    for k in range(1, 4): # We only need p_x, p_y, p_z for L_i\n",
    "        # Sum over mu\n",
    "        for mu in range(4):\n",
    "            # Use g4DD[k][mu] if k <= mu, otherwise use g4DD[mu][k]\n",
    "            if k <= mu:\n",
    "                p_down[k] += g4DD[k][mu] * pU[mu]\n",
    "            else: # k > mu\n",
    "                p_down[k] += g4DD[mu][k] * pU[mu]\n",
    "            \n",
    "    p_x, p_y, p_z = p_down[1], p_down[2], p_down[3]\n",
    "\n",
    "    # Calculate angular momentum components \n",
    "    L_x = y*p_z - z*p_y\n",
    "    L_y = z*p_x - x*p_z\n",
    "    L_z = x*p_y - y*p_x\n",
    "    \n",
    "    return [L_x, L_y, L_z]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b68249",
   "metadata": {},
   "source": [
    "# Markdown for Carter Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V10_Python_to_C_via_NRPy.ipynb\n",
    "# In the \"Symbolic Recipes\" cell (Final, Corrected symbolic_carter_constant_Q_final)\n",
    "\n",
    "# symbolic_energy() and symbolic_L_components_cart() remain correct.\n",
    "\n",
    "def symbolic_carter_constant_Q():\n",
    "    \"\"\"\n",
    "    Computes the symbolic expression for the Carter Constant Q using a\n",
    "    verified formula, robustly handling the axial singularity.\n",
    "    \"\"\"\n",
    "    # Define all necessary symbolic variables\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    pt, px, py, pz = sp.symbols(\"y[4] y[5] y[6] y[7]\", real=True)\n",
    "    pU = [pt, px, py, pz]\n",
    "    a = sp.Symbol(\"a_spin\", real=True)\n",
    "    g4DD = ixp.declarerank2(\"metric->g\", sym=\"sym01\", dimension=4)\n",
    "\n",
    "    # --- Step 1: Compute intermediate quantities E, Lz, and p_i ---\n",
    "    E = symbolic_energy()\n",
    "    _, _, Lz = symbolic_L_components_cart()\n",
    "    \n",
    "    p_down = ixp.zerorank1(dimension=4)\n",
    "    for k in range(1, 4):\n",
    "        for mu in range(4):\n",
    "            if k <= mu: p_down[k] += g4DD[k][mu] * pU[mu]\n",
    "            else: p_down[k] += g4DD[mu][k] * pU[mu]\n",
    "    p_x, p_y, p_z = p_down[1], p_down[2], p_down[3]\n",
    "\n",
    "    # --- Step 2: Compute geometric terms ---\n",
    "    r_sq = x**2 + y**2 + z**2\n",
    "    rho_sq = x**2 + y**2\n",
    "    \n",
    "    # --- Step 3: Compute p_theta^2 directly in Cartesian components ---\n",
    "    # This avoids square roots and potential complex number issues in sympy.\n",
    "    # p_theta^2 = r^2 * p_z^2 + cot^2(theta) * (x*p_x + y*p_y)^2 - 2*r*p_z*cot(theta)*(x*p_x+y*p_y)\n",
    "    # where cot(theta) = z / rho\n",
    "    \n",
    "    # This term is (x*p_x + y*p_y)\n",
    "    xpx_plus_ypy = x*p_x + y*p_y\n",
    "    \n",
    "    # This is p_theta^2, constructed to avoid dividing by rho before squaring.\n",
    "    # It is equivalent to (z*xpx_plus_ypy/rho - rho*p_z)^2\n",
    "    p_theta_sq = (z**2 * xpx_plus_ypy**2 / rho_sq) - (2 * z * p_z * xpx_plus_ypy) + (rho_sq * p_z**2)\n",
    "\n",
    "    # --- Step 4: Assemble the final formula for Q ---\n",
    "    # Q = p_theta^2 + cos^2(theta) * (-a^2*E^2 + L_z^2/sin^2(theta))\n",
    "    # where cos^2(theta) = z^2/r^2 and sin^2(theta) = rho^2/r^2\n",
    "    \n",
    "    # This is the second term in the Q formula\n",
    "    second_term = (z**2 / r_sq) * (-a**2 * E**2 + Lz**2 * (r_sq / rho_sq))\n",
    "    \n",
    "    Q_formula = p_theta_sq + second_term\n",
    "    \n",
    "    # --- Step 5: Handle the axial singularity ---\n",
    "    # For motion on the z-axis (rho_sq -> 0), Lz=0 and p_theta=0, so Q=0.\n",
    "    Q_final = sp.Piecewise(\n",
    "        (0, rho_sq < 1e-12),\n",
    "        (Q_formula, True)\n",
    "    )\n",
    "    \n",
    "    return Q_final\n",
    "\n",
    "print(\"Final symbolic recipes for conserved quantities defined (Carter Constant re-derived).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe10d0a",
   "metadata": {},
   "source": [
    "<a id='spacetime_definition'></a>\n",
    "# Step 4: Spacetime Definition in Kerr-Schild Coordinates\n",
    "\n",
    "This section defines the specific spacetime geometry in which the geodesics will be integrated. Instead of defining separate metrics for Schwarzschild (non-rotating) and Kerr (rotating) black holes, we use a single, powerful coordinate system: **Cartesian Kerr-Schild coordinates**. This system has a major advantage over more common coordinate systems like Boyer-Lindquist: it is regular everywhere, including at the event horizon. This means the metric components and their derivatives do not diverge, allowing the numerical integrator to trace a photon's path seamlessly across the horizon without encountering coordinate singularities.\n",
    "\n",
    "The Kerr-Schild metric $g_{\\mu\\nu}$ is constructed by adding a correction term to the flat Minkowski metric $\\eta_{\\mu\\nu}$:\n",
    "$$ g_{\\mu\\nu} = \\eta_{\\mu\\nu} + 2H l_\\mu l_\\nu $$\n",
    "where $\\eta_{\\mu\\nu}$ is the Minkowski metric `diag(-1, 1, 1, 1)`, $l_\\mu$ is a special null vector, and $H$ is a scalar function that depends on the black hole's mass $M$ and spin $a$.\n",
    "\n",
    "The function `define_kerr_metric_Cartesian_Kerr_Schild()` implements this formula symbolically. It defines the coordinates `(t, x, y, z)`, the mass `M`, and the spin `a` as `sympy` symbols. It then constructs the components of the null vector $l_\\mu$ and the scalar function $H$. Finally, it assembles the full metric tensor $g_{\\mu\\nu}$.\n",
    "\n",
    "A key feature of this formulation is that if the spin parameter `a` is set to zero, the metric automatically and exactly reduces to the Schwarzschild metric in Cartesian coordinates. This allows a single set of symbolic expressions and a single set of C functions to handle both spacetimes, with the specific behavior controlled by the runtime value of the `a_spin` parameter.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.zerorank1(dimension)`**: Previously introduced. Used to initialize the null vector $l_\\mu$.\n",
    "*   **`nrpy.indexedexp.zerorank2(dimension)`**: Previously introduced. Used to initialize the metric tensor $g_{\\mu\\nu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_kerr_metric_Cartesian_Kerr_Schild():\n",
    "    \"\"\"\n",
    "    Defines the Kerr metric tensor in Cartesian Kerr-Schild coordinates.\n",
    "\n",
    "    This function is the new, unified source for both Kerr (a != 0) and\n",
    "    Schwarzschild (a = 0) spacetimes. The coordinates are (t, x, y, z).\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (g4DD, xx), where g4DD is the symbolic 4x4 metric tensor\n",
    "        and xx is the list of symbolic coordinate variables.\n",
    "    \"\"\"\n",
    "    # Define the symbolic coordinates using the 'y[i]' convention for the integrator\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    xx = [t, x, y, z]\n",
    "\n",
    "    # Access the symbolic versions of the mass and spin parameters\n",
    "    M = M_scale.symbol\n",
    "    a = a_spin.symbol\n",
    "\n",
    "    # Define intermediate quantities\n",
    "    r2 = x**2 + y**2 + z**2\n",
    "    r = sp.sqrt(r2)\n",
    "    \n",
    "    # Define the Kerr-Schild null vector l_μ\n",
    "    l_down = ixp.zerorank1(dimension=4)\n",
    "    l_down[0] = 1\n",
    "    l_down[1] = (r*x + a*y) / (r2 + a**2)\n",
    "    l_down[2] = (r*y - a*x) / (r2 + a**2)\n",
    "    l_down[3] = z/r\n",
    "\n",
    "    # Define the scalar function H\n",
    "    H = (M * r**3) / (r**4 + a**2 * z**2)\n",
    "\n",
    "    # The Kerr-Schild metric is g_μν = η_μν + 2H * l_μ * l_ν\n",
    "    # where η_μν is the Minkowski metric diag(-1, 1, 1, 1)\n",
    "    g4DD = ixp.zerorank2(dimension=4)\n",
    "    for mu in range(4):\n",
    "        for nu in range(4):\n",
    "            eta_mu_nu = 0\n",
    "            if mu == nu:\n",
    "                eta_mu_nu = 1\n",
    "            if mu == 0 and nu == 0:\n",
    "                eta_mu_nu = -1\n",
    "            \n",
    "            g4DD[mu][nu] = eta_mu_nu + 2 * H * l_down[mu] * l_down[nu]\n",
    "            \n",
    "    return g4DD, xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234e04d",
   "metadata": {},
   "source": [
    "# Markdown for Schwarzschild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In the NEW CELL after define_kerr_metric_Cartesian_Kerr_Schild\n",
    "\n",
    "def define_schwarzschild_metric_cartesian():\n",
    "    \"\"\"\n",
    "    Defines the Schwarzschild metric tensor directly in Cartesian coordinates.\n",
    "    \n",
    "    This version uses the standard textbook formula and ensures all components\n",
    "    are sympy objects to prevent C-generation errors.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (g4DD, xx), where g4DD is the symbolic 4x4 metric tensor\n",
    "        and xx is the list of symbolic coordinate variables.\n",
    "    \"\"\"\n",
    "    # Define Cartesian coordinates\n",
    "    t, x, y, z = sp.symbols(\"y[0] y[1] y[2] y[3]\", real=True)\n",
    "    xx = [t, x, y, z]\n",
    "\n",
    "    # Access the symbolic mass parameter\n",
    "    M = M_scale.symbol\n",
    "\n",
    "    # Define r in terms of Cartesian coordinates\n",
    "    r = sp.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "    # Define the Cartesian Schwarzschild metric components directly\n",
    "    g4DD = ixp.zerorank2(dimension=4)\n",
    "    \n",
    "    # g_tt\n",
    "    g4DD[0][0] = -(1 - 2*M/r)\n",
    "    \n",
    "    # Spatial components g_ij = δ_ij + (2M/r) * (x_i * x_j / r^2)\n",
    "    x_i = [x, y, z]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # --- CORRECTED: Use sp.sympify() for the kronecker delta ---\n",
    "            delta_ij = sp.sympify(0)\n",
    "            if i == j:\n",
    "                delta_ij = sp.sympify(1)\n",
    "            \n",
    "            # The indices for g4DD are off by 1 from the spatial indices\n",
    "            g4DD[i+1][j+1] = delta_ij + (2*M/r) * (x_i[i] * x_i[j] / (r**2))\n",
    "\n",
    "    # --- CORRECTED: Ensure time-space components are sympy objects ---\n",
    "    g4DD[0][1] = g4DD[1][0] = sp.sympify(0)\n",
    "    g4DD[0][2] = g4DD[2][0] = sp.sympify(0)\n",
    "    g4DD[0][3] = g4DD[3][0] = sp.sympify(0)\n",
    "            \n",
    "    return g4DD, xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf83f1b",
   "metadata": {},
   "source": [
    "# Symbolic equations for u^t,u^phi on xy plane (Designed to keep sig figs for extreme a_spin and r_initial values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec78a4",
   "metadata": {},
   "source": [
    "<a id='symbolic_execution'></a>\n",
    "# Step 5: Symbolic Workflow Execution\n",
    "\n",
    "This cell acts as the central hub for the symbolic portion of our project. In the preceding cells, we *defined* a series of Python functions that perform individual mathematical tasks. Here, we *execute* those functions in the correct sequence to generate all the final symbolic expressions that will serve as \"recipes\" for our C code generators.\n",
    "\n",
    "This \"symbolic-first\" approach is a core `nrpy` principle and offers significant advantages:\n",
    "1.  **Efficiency**: The complex symbolic calculations, such as inverting the metric tensor and deriving the Christoffel symbols, are performed **only once** when this notebook is run. The results are stored in global Python variables, preventing redundant and time-consuming recalculations. This is especially important for the Kerr metric, whose Christoffel symbols can take several minutes to compute.\n",
    "2.  **Modularity**: This workflow creates a clean separation between the *specific solution* for a metric (e.g., the explicit formulas for the Kerr-Schild Christoffels) and the *generic form* of the equations of motion (which are valid for any metric).\n",
    "\n",
    "This cell produces two key sets of symbolic expressions that are stored in global variables for later use:\n",
    "*   **`Gamma4UDD_kerr`**: The explicit symbolic formulas for the Christoffel symbols of the unified Kerr-Schild metric.\n",
    "*   **`all_rhs_expressions`**: A Python list containing the 9 symbolic expressions for the right-hand-sides of our generic ODE system. To achieve this generality, we create a symbolic **placeholder** for the Christoffel symbols using `ixp.declarerank3(\"conn->Gamma4UDD\", ...)`. This placeholder is passed to `geodesic_mom_rhs()` to construct the geodesic equation in its abstract form. This elegant technique embeds the final C variable name (`conn->Gamma4UDD...`) directly into the symbolic expression, which dramatically simplifies the C code generation step for the `calculate_ode_rhs()` engine.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank3(name, dimension)`**: Previously introduced. Used here to create a symbolic placeholder for the Christoffel symbols that will be passed to the generic RHS engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfe0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_5_mass_geodesic.ipynb, Cell ID 5fbfe0b5 (UPDATED)\n",
    "\n",
    "# --- 1. Define the Kerr-Schild metric and get its derivatives ---\n",
    "print(\" -> Computing Kerr-Schild metric and Christoffel symbols...\")\n",
    "g4DD_kerr, xx_kerr = define_kerr_metric_Cartesian_Kerr_Schild()\n",
    "g4DD_dD_kerr = derivative_g4DD(g4DD_kerr, xx_kerr)\n",
    "Gamma4UDD_kerr = four_connections(g4DD_kerr, g4DD_dD_kerr)\n",
    "print(\"    ... Done.\")\n",
    "\n",
    "# --- 2. Define the Standard Schwarzschild metric in Cartesian and get its derivatives ---\n",
    "print(\" -> Computing Standard Schwarzschild (Cartesian) metric and Christoffel symbols...\")\n",
    "g4DD_schw_cart, xx_schw_cart = define_schwarzschild_metric_cartesian()\n",
    "g4DD_dD_schw_cart = derivative_g4DD(g4DD_schw_cart, xx_schw_cart)\n",
    "Gamma4UDD_schw_cart = four_connections(g4DD_schw_cart, g4DD_dD_schw_cart)\n",
    "print(\"    ... Done.\")\n",
    "\n",
    "# --- 3. Generate GENERIC symbolic RHS expressions for MASSIVE geodesics ---\n",
    "rhs_pos_massive = geodesic_pos_rhs_massive() \n",
    "rhs_vel_massive = geodesic_vel_rhs_massive()\n",
    "all_rhs_expressions_massive = rhs_pos_massive + rhs_vel_massive\n",
    "\n",
    "# --- 4. Generate symbolic recipes using the STABLE and original methods ---\n",
    "print(\" -> Generating symbolic recipes for initial conditions and conserved quantities...\")\n",
    "ut_expr_from_vel = ut_massive() # Keep original for the general C initializer\n",
    "# *** THE FIX IS HERE: Call the new, numerically stable function ***\n",
    "ut_expr, uphi_expr = symbolic_ut_uphi_from_r_stable()\n",
    "print(\"    ... Initial condition recipes generated.\")\n",
    "\n",
    "# --- 5. Generate symbolic recipes for conserved quantities ---\n",
    "E_expr = symbolic_energy()\n",
    "Lx_expr, Ly_expr, Lz_expr = symbolic_L_components_cart()\n",
    "Q_expr_kerr = symbolic_carter_constant_Q()\n",
    "Q_expr_schw = Lx_expr**2 + Ly_expr**2 + Lz_expr**2\n",
    "list_of_expressions_kerr = [E_expr, Lx_expr, Ly_expr, Lz_expr, Q_expr_kerr]\n",
    "list_of_expressions_schw = [E_expr, Lx_expr, Ly_expr, Lz_expr, Q_expr_schw]\n",
    "print(\"    ... Conservation recipes generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032d263",
   "metadata": {},
   "source": [
    "<a id='generate_c_engines'></a>\n",
    "# Step 6: C Code Generation - Physics \"Engines\" and \"Workers\"\n",
    "\n",
    "This section marks our transition from pure symbolic mathematics to C code generation. The Python functions defined here are \"meta-functions\": their job is not to perform calculations themselves, but to **generate the C code** that will perform the calculations in the final compiled program.\n",
    "\n",
    "We distinguish between two types of generated functions:\n",
    "*   **Workers**: These are specialized functions that implement the physics for a *specific metric*. For example, `con_kerr_schild()` is a worker that only knows how to compute Christoffel symbols for the Kerr-Schild metric.\n",
    "*   **Engines**: These are generic functions that implement physics equations valid for *any metric*. For example, `calculate_ode_rhs()` is an engine that can compute the geodesic equations for any metric, as long as the Christoffel symbols are provided to it.\n",
    "\n",
    "This design pattern allows for maximum code reuse and extensibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c6765",
   "metadata": {},
   "source": [
    "# Schwarzschild Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ef433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g4DD_schwarzschild_cartesian():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Schwarzschild\n",
    "    metric components in standard Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: g4DD_schwarzschild_cartesian()...\")\n",
    "    \n",
    "    # Use the globally defined g4DD_schw_cart from the symbolic execution step\n",
    "    list_of_g4DD_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_syms.append(g4DD_schw_cart[i][j])\n",
    "\n",
    "    list_of_g4DD_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_C_vars.append(f\"metric->g{i}{j}\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the 10 unique components of the Schwarzschild metric in Cartesian coords.\"\"\"\n",
    "    name = \"g4DD_schwarzschild_cartesian\"\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], metric_struct *restrict metric\"\n",
    "   \n",
    "    body = ccg.c_codegen(list_of_g4DD_syms, list_of_g4DD_C_vars, enable_cse=True)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... g4DD_schwarzschild_cartesian() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd610ccf",
   "metadata": {},
   "source": [
    "<a id='g4DD_kerr_schild_engine'></a>\n",
    "### 6.a: `g4DD_kerr_schild()` Worker\n",
    "\n",
    "This Python function generates the C **worker** function `g4DD_kerr_schild()`, whose only job is to compute the 10 unique components of the Kerr-Schild metric tensor, $g_{\\mu\\nu}$, at a given point in spacetime.\n",
    "\n",
    "The generation process is as follows:\n",
    "1.  **Access Symbolic Recipe:** It accesses the global `g4DD_kerr` variable, which holds the symbolic `sympy` expression for the Kerr-Schild metric tensor, generated in Step 5.\n",
    "2.  **Define C Assignment:** It creates two Python lists: one containing the 10 unique symbolic metric expressions (`list_of_g4DD_syms`) and another containing the corresponding C variable names for the members of the `metric_struct` (e.g., `metric->g00`, `metric->g01`, etc.) in `list_of_g4DD_C_vars`.\n",
    "3.  **Generate C Code:** It passes these two lists to `nrpy.c_codegen.c_codegen`. This powerful `nrpy` function converts the symbolic math into highly optimized C code, including performing Common Subexpression Elimination (CSE).\n",
    "4.  **Register C Function:** Finally, it bundles the generated C code with its metadata (description, parameters, etc.) and registers the complete function with `nrpy.c_function.register_CFunction`. Crucially, it sets `include_CodeParameters_h=True` to automatically handle access to both the `M_scale` and `a_spin` parameters via the \"Triple-Lock\" system.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_codegen.c_codegen(sympy_expressions, C_variable_names, **kwargs)`**:\n",
    "    *   **Source File**: `nrpy/c_codegen.py`\n",
    "    *   **Description**: The core symbolic-to-C translation engine. It takes a list of `sympy` expressions and a corresponding list of C variable names and generates optimized C code to perform the assignments.\n",
    "    *   **Key Inputs**:\n",
    "        *   `sympy_expressions`: A Python list of symbolic expressions to be converted to C code.\n",
    "        *   `C_variable_names`: A Python list of strings for the C variables that will store the results.\n",
    "    *   **Key Keyword Arguments (`kwargs`)**:\n",
    "        *   `enable_cse=True`: Enables Common Subexpression Elimination, which finds repeated mathematical operations, assigns them to temporary variables, and reuses those variables to reduce redundant calculations. This is essential for performance.\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(name, params, body, **kwargs)`**:\n",
    "    *   **Source File**: `nrpy/c_function.py`\n",
    "    *   **Description**: This is the workhorse for defining a C function. It takes all necessary metadata and stores it in a global dictionary, `cfc.CFunction_dict`. The final build system uses this dictionary to write all the `.c` source files.\n",
    "    *   **Key Inputs**:\n",
    "        *   `name`: The name of the C function.\n",
    "        *   `params`: A string defining the function's parameters (e.g., `\"const double y[4], ...\"`).\n",
    "        *   `body`: A string containing the C code for the function's body.\n",
    "    *   **Key Keyword Arguments (`kwargs`)**:\n",
    "        *   `include_CodeParameters_h=True`: Enables the \"Triple-Lock\" system for this function, automatically including `set_CodeParameters.h` at the top of the function body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d52f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g4DD_kerr_schild():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Kerr-Schild\n",
    "    metric components in Cartesian coordinates. This is the new unified worker.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: g4DD_kerr_schild()...\")\n",
    "    \n",
    "    # We use the globally defined g4DD_kerr from the symbolic execution step\n",
    "    list_of_g4DD_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_syms.append(g4DD_kerr[i][j])\n",
    "\n",
    "    list_of_g4DD_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(i, 4):\n",
    "            list_of_g4DD_C_vars.append(f\"metric->g{i}{j}\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the 10 unique components of the Kerr metric in Cartesian Kerr-Schild coords.\"\"\"\n",
    "    name = \"g4DD_kerr_schild\"\n",
    "    # The state vector y now contains (t, x, y, z)\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], metric_struct *restrict metric\"\n",
    "   \n",
    "    body = ccg.c_codegen(list_of_g4DD_syms, list_of_g4DD_C_vars, enable_cse=True)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... g4DD_kerr_schild() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f1222",
   "metadata": {},
   "source": [
    "<a id='con_kerr_schild_engine'></a>\n",
    "### 6.b: `con_kerr_schild()` Worker\n",
    "\n",
    "This function is structured identically to the `g4DD_kerr_schild` worker. It generates the C **worker** function `con_kerr_schild()`, whose only job is to compute the 40 unique Christoffel symbols for the unified Kerr-Schild metric.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Access Symbolic Recipe:** It accesses the pre-computed symbolic Christoffel formulas from the global `Gamma4UDD_kerr` variable, which was generated in Step 5.\n",
    "2.  **Define C Assignment:** It creates a list of the 40 unique symbolic expressions and a corresponding list of the C variable names for the members of the `connection_struct` (e.g., `conn->Gamma4UDD012`).\n",
    "3.  **Generate C Code:** It uses `nrpy.c_codegen.c_codegen` to convert these highly complex symbolic expressions into optimized C code. The Common Subexpression Elimination (CSE) performed by `c_codegen` is absolutely essential here, as it reduces what would be thousands of floating-point operations into a much more manageable and efficient set of calculations.\n",
    "4.  **Register C Function:** Like the other workers, it registers the function using `nrpy.c_function.register_CFunction` and sets `include_CodeParameters_h=True` to handle its dependency on both `M_scale` and `a_spin`.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.indexedexp.declarerank3(name, dimension)`**: Previously introduced. Used here to programmatically generate the C variable names for the Christoffel symbols that will be stored in the `connection_struct`.\n",
    "*   **`nrpy.c_codegen.c_codegen(...)`**: Previously introduced.\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb471c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_kerr_schild():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Kerr-Schild Christoffel symbols.\n",
    "    This is the new unified worker.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: con_kerr_schild()...\")\n",
    "    \n",
    "    # We use the globally defined Gamma4UDD_kerr from the symbolic execution step\n",
    "    list_of_Gamma_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_syms.append(Gamma4UDD_kerr[i][j][k])\n",
    "\n",
    "    conn_Gamma4UDD = ixp.declarerank3(\"conn->Gamma4UDD\", dimension=4)\n",
    "    list_of_Gamma_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_C_vars.append(str(conn_Gamma4UDD[i][j][k]))\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the 40 unique Christoffel symbols for the Kerr metric in Kerr-Schild coords.\"\"\"\n",
    "    name = \"con_kerr_schild\"\n",
    "    # The state vector y now contains (t, x, y, z)\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], connection_struct *restrict conn\"\n",
    "\n",
    "    body = ccg.c_codegen(list_of_Gamma_syms, list_of_Gamma_C_vars, enable_cse=True)\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... con_kerr_schild() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166625f",
   "metadata": {},
   "source": [
    "# Con Schwarzschild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6072ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_schwarzschild_cartesian():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function to compute the Schwarzschild Christoffel symbols\n",
    "    in standard Cartesian coordinates.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C worker function: con_schwarzschild_cartesian()...\")\n",
    "    \n",
    "    # Use the globally defined Gamma4UDD_schw_cart\n",
    "    list_of_Gamma_syms = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_syms.append(Gamma4UDD_schw_cart[i][j][k])\n",
    "\n",
    "    conn_Gamma4UDD = ixp.declarerank3(\"conn->Gamma4UDD\", dimension=4)\n",
    "    list_of_Gamma_C_vars = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            for k in range(j, 4):\n",
    "                list_of_Gamma_C_vars.append(str(conn_Gamma4UDD[i][j][k]))\n",
    "\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes the unique Christoffel symbols for the Schwarzschild metric in Cartesian coords.\"\"\"\n",
    "    name = \"con_schwarzschild_cartesian\"\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const double y[4], connection_struct *restrict conn\"\n",
    "\n",
    "    body = ccg.c_codegen(list_of_Gamma_syms, list_of_Gamma_C_vars, enable_cse=True)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, name=name, params=params, body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... con_schwarzschild_cartesian() registration complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e27c2c",
   "metadata": {},
   "source": [
    "<a id='calculate_ode_rhs_engine'></a>\n",
    "### 6.d: `calculate_ode_rhs()` Engine\n",
    "\n",
    "This function generates the core \"engine\" of our ODE solver: the C function `calculate_ode_rhs()`. Its single responsibility is to calculate the right-hand sides for our entire system of 9 ODEs. It is completely generic and has no knowledge of any specific metric; it only knows how to compute the geodesic equations given a set of Christoffel symbols and the spatial metric components.\n",
    "\n",
    "The generation process is straightforward:\n",
    "1.  **Access Generic Recipe:** It accesses the global `all_rhs_expressions` list. This list contains the generic symbolic form of the ODEs for position, momentum, and proper length that we derived in Step 5.\n",
    "2.  **Generate C Code:** It passes this list directly to `nrpy.c_codegen.c_codegen`. The symbols used to build `all_rhs_expressions` were already created with their final C syntax (e.g., `y[5]` for the momentum, `conn->Gamma4UDD...` for the Christoffel placeholder, and `metric->g...` for the metric placeholder). Therefore, no further symbolic manipulation is needed. `nrpy` simply translates the expressions into optimized C code.\n",
    "3.  **Register C Function:** The generated C code body is bundled with its metadata and registered. This function does not require the `include_CodeParameters_h` flag because it is physically generic and receives all necessary information through its arguments.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_codegen.c_codegen(...)`**: Previously introduced.\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ode_rhs_massive():\n",
    "    \"\"\"\n",
    "    Generates the C engine to calculate the RHS of the 8 massive particle ODEs.\n",
    "    \"\"\"\n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Calculates the right-hand sides (RHS) of the 8 massive particle geodesic ODEs.\n",
    "    \n",
    "    This is a generic engine that implements the geodesic equation using pre-computed\n",
    "    Christoffel symbols from the connection_struct.\n",
    "    \n",
    "    @param[in]  y         The 8-component state vector [t, x, y, z, u^t, u^x, u^y, u^z].\n",
    "    @param[in]  conn      A pointer to the connection_struct holding the Christoffel symbols.\n",
    "    @param[out] rhs_out   A pointer to the 8-component output array for the RHS results.\"\"\"\n",
    "    name = \"calculate_ode_rhs_massive\"\n",
    "    params = \"const double y[8], const connection_struct *restrict conn, double rhs_out[8]\"\n",
    "    \n",
    "    rhs_output_vars = [f\"rhs_out[{i}]\" for i in range(8)]\n",
    "    body = ccg.c_codegen(all_rhs_expressions_massive, rhs_output_vars)\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624cd49",
   "metadata": {},
   "source": [
    "# Markdown for check_conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4260e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conservation_massive():\n",
    "    \"\"\"\n",
    "    Generates the C function `check_conservation_massive`.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: check_conservation_massive()...\")\n",
    "\n",
    "    output_vars_kerr = [\"*E\", \"*Lx\", \"*Ly\", \"*Lz\", \"*Q\"]\n",
    "    output_vars_schw = [\"*E\", \"*Lx\", \"*Ly\", \"*Lz\", \"*Q\"]\n",
    "\n",
    "    body_C_code_kerr = ccg.c_codegen(list_of_expressions_kerr, output_vars_kerr, enable_cse=True, include_braces=False)\n",
    "    body_C_code_schw = ccg.c_codegen(list_of_expressions_schw, output_vars_schw, enable_cse=True, include_braces=False)\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Computes conserved quantities (E, L_i, Q/L^2) for a given massive particle state vector.\"\"\"\n",
    "    name = \"check_conservation_massive\"\n",
    "    params = \"\"\"const commondata_struct *restrict commondata,\n",
    "        const params_struct *restrict params,\n",
    "        const metric_params *restrict metric_params_in,\n",
    "        const double y[8], \n",
    "        double *E, double *Lx, double *Ly, double *Lz, double *Q\"\"\"\n",
    "        \n",
    "    body = r\"\"\"\n",
    "    // Unpack parameters from commondata struct that are needed symbolically\n",
    "    const REAL a_spin = commondata->a_spin;\n",
    "\n",
    "    metric_struct* metric = (metric_struct*)malloc(sizeof(metric_struct));\n",
    "    g4DD_metric(commondata, params, metric_params_in, y, metric);\n",
    "\n",
    "    if (metric_params_in->type == Kerr) {\n",
    "        \"\"\" + body_C_code_kerr + r\"\"\"\n",
    "    } else { // Both Schwarzschild types are now Cartesian\n",
    "        \"\"\" + body_C_code_schw + r\"\"\"\n",
    "    }\n",
    "    \n",
    "    free(metric);\n",
    "    \"\"\"\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, cfunc_type=\"void\",\n",
    "        name=name, params=params, body=body\n",
    "    )\n",
    "    print(f\"    ... {name}() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fb7b8",
   "metadata": {},
   "source": [
    "<a id='generate_c_orchestrators'></a>\n",
    "# Step 7: C Code Generation - Orchestrators and Dispatchers\n",
    "\n",
    "With the low-level \"engine\" and \"worker\" functions defined in the previous step, we now generate the higher-level C functions that manage the simulation. These functions are responsible for dispatching to the correct worker based on runtime parameters and for orchestrating the overall program flow.\n",
    "\n",
    "*   **Dispatchers** are functions that contain a `switch` statement to select the correct \"worker\" function based on the chosen metric (e.g., `Schwarzschild` vs. `Kerr`).\n",
    "*   **Orchestrators** are functions that execute a sequence of calls to other engines, workers, and dispatchers to perform a complex task, like setting up initial conditions or running the main integration loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db0d5",
   "metadata": {},
   "source": [
    "<a id='g4DD_metric_dispatcher'></a>\n",
    "### 7.a: `g4DD_metric()` Dispatcher\n",
    "\n",
    "This Python function generates the C function `g4DD_metric()`, which serves as a high-level **dispatcher.** Its role is to select and call the correct worker function to compute the components of the metric tensor, $g_{\\mu\\nu}$.\n",
    "\n",
    "The generated C code uses a `switch` statement that reads the `metric->type` member of the `metric_params` struct. In this project, both the Schwarzschild and Kerr spacetimes are handled by the unified `g4DD_kerr_schild()` worker function. The dispatcher calls this single worker, and the specific metric returned by the worker depends on the runtime value of the `a_spin` parameter (if `a_spin` is 0, the Schwarzschild metric is computed).\n",
    "\n",
    "This modular approach cleanly separates the control flow (deciding *which* metric to use) from the physics implementation (the worker functions that know *how* to compute a specific metric). This makes the project easy to extend with new spacetimes in the future by adding new cases to the `switch` statement and new worker functions.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced. Used to register the manually written C code for the dispatcher function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65702cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In cell [65702cb7]\n",
    "\n",
    "def g4DD_metric():\n",
    "    \"\"\"\n",
    "    Generates and registers the C function g4DD_metric(), which serves as a\n",
    "    dispatcher to call the appropriate metric-specific worker function.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C dispatcher function: g4DD_metric()...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Dispatcher to compute the 4-metric g_munu for the chosen metric.\"\"\"\n",
    "    name = \"g4DD_metric\"\n",
    "    # The signature is now coordinate-aware, but the y vector is always Cartesian here.\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const metric_params *restrict metric, const double y[8], metric_struct *restrict metric_out\"\n",
    "    \n",
    "    body = r\"\"\"\n",
    "    // The state vector y_pos contains only the position coordinates.\n",
    "    const double y_pos[4] = {y[0], y[1], y[2], y[3]};\n",
    "\n",
    "    // This switch statement chooses which \"worker\" function to call\n",
    "    // based on the metric type provided.\n",
    "    switch(metric->type) {\n",
    "        case Schwarzschild:\n",
    "        case Kerr:\n",
    "            // For Kerr or Schwarzschild in KS coords, call the unified Kerr-Schild C function.\n",
    "            g4DD_kerr_schild(commondata, params, y_pos, metric_out);\n",
    "            break;\n",
    "        // <-- MODIFIED: Call the new Cartesian worker\n",
    "        case Schwarzschild_Standard:\n",
    "            g4DD_schwarzschild_cartesian(commondata, params, y_pos, metric_out);\n",
    "            break;\n",
    "        case Numerical:\n",
    "            printf(\"Error: Numerical metric not supported yet.\\n\");\n",
    "            exit(1);\n",
    "            break;\n",
    "        default:\n",
    "            printf(\"Error: MetricType %d not supported in g4DD_metric() yet.\\n\", metric->type);\n",
    "            exit(1);\n",
    "            break;\n",
    "    }\n",
    "\"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)\n",
    "    print(\"    ... g4DD_metric() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db30c0a",
   "metadata": {},
   "source": [
    "<a id='connections_dispatcher'></a>\n",
    "### 7.b: `connections()` Dispatcher\n",
    "\n",
    "This Python function generates the C function `connections()`, which acts as a second **dispatcher.** Its sole responsibility is to select and call the correct metric-specific worker function (like `con_kerr_schild()`) to compute the Christoffel symbols.\n",
    "\n",
    "Like the `g4DD_metric()` dispatcher, the generated C code uses a `switch` statement based on the `metric->type`. It dispatches the call to the appropriate specialized worker, which in this case is the unified `con_kerr_schild()` function for both Kerr and Schwarzschild spacetimes. This design is highly extensible: adding a new metric simply requires writing a new worker function for its Christoffel symbols and adding a new `case` to this `switch` statement.\n",
    "\n",
    "This function demonstrates how `nrpy` allows for the seamless integration of developer-written control flow with the automatically generated worker functions.\n",
    "\n",
    "### `nrpy` Functions Used in this Cell:\n",
    "\n",
    "*   **`nrpy.c_function.register_CFunction(...)`**: Previously introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V11_0_Python_to_C_via_NRPy.ipynb\n",
    "# In cell [b92b7851]\n",
    "\n",
    "def connections():\n",
    "    \"\"\"\n",
    "    Generates and registers the C dispatcher for Christoffel symbols.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C dispatcher: connections()...\")\n",
    "\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"stdio.h\", \"stdlib.h\"]\n",
    "    desc = r\"\"\"@brief Dispatcher to compute Christoffel symbols for the chosen metric.\"\"\"\n",
    "    \n",
    "    name = \"connections\"\n",
    "    cfunc_type = \"void\" \n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const metric_params *restrict metric, const double y[8], connection_struct *restrict conn\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // The state vector y_pos contains only the position coordinates.\n",
    "    const double y_pos[4] = {y[0], y[1], y[2], y[3]};\n",
    "\n",
    "    // This switch statement chooses which \"worker\" function to call\n",
    "    // based on the metric type provided.\n",
    "    switch(metric->type) {\n",
    "        case Schwarzschild:\n",
    "        case Kerr:\n",
    "            con_kerr_schild(commondata, params, y_pos, conn);\n",
    "            break;\n",
    "        // <-- MODIFIED: Call the new Cartesian worker\n",
    "        case Schwarzschild_Standard:\n",
    "            con_schwarzschild_cartesian(commondata, params, y_pos, conn);\n",
    "            break;\n",
    "        case Numerical:\n",
    "            printf(\"Error: Numerical metric not supported yet.\\n\");\n",
    "            exit(1);\n",
    "            break;\n",
    "        default:\n",
    "            printf(\"Error: MetricType %d not supported yet.\\n\", metric->type);\n",
    "            exit(1);\n",
    "            break;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes, desc=desc, cfunc_type=cfunc_type,\n",
    "        name=name, params=params, body=body\n",
    "    )\n",
    "    print(\"    ... connections() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02f58d",
   "metadata": {},
   "source": [
    "# Calculate ut and uphi values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_5_mass_geodesic.ipynb, Cell ID bb00ac33 (UPDATED)\n",
    "\n",
    "def calculate_ut_uphi_from_r():\n",
    "    \"\"\"\n",
    "    Generates a C helper function to compute u^t and u^phi from a radius\n",
    "    using a numerically stable recipe.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: calculate_ut_uphi_from_r() [STABLE VERSION]...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\"]\n",
    "    desc = r\"\"\"@brief Computes u^t and u^phi for a circular orbit at a given radius using a numerically stable method.\"\"\"\n",
    "    name = \"calculate_ut_uphi_from_r\"\n",
    "    params = \"const double r_initial, const commondata_struct *restrict commondata, const params_struct *restrict params, double *ut, double *uphi\"\n",
    "    \n",
    "    # The global ut_expr and uphi_expr are now the stable versions\n",
    "    body = ccg.c_codegen(\n",
    "        [ut_expr, uphi_expr],\n",
    "        [\"*ut\", \"*uphi\"],\n",
    "        enable_cse=True\n",
    "    )\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body,\n",
    "        include_CodeParameters_h=True\n",
    "    )\n",
    "    print(\"    ... calculate_ut_uphi_from_r() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ea28a",
   "metadata": {},
   "source": [
    "\n",
    "# set_initial_conditions_massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd43b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V1_5_mass_geodesic (Copy).ipynb\n",
    "# In cell [bfd43b53] (DEFINITIVE STABLE VERSION)\n",
    "\n",
    "def set_initial_conditions_massive():\n",
    "    \"\"\"\n",
    "    Generates the C engine to set the full initial 8-component state vector.\n",
    "    \n",
    "    VERSION 2: This version implements the NUMERICALLY STABLE method for\n",
    "    calculating u^t, as detailed in the Gemini report. It computes the\n",
    "    Boyer-Lindquist metric components and Omega internally to avoid\n",
    "    catastrophic cancellation.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C engine: set_initial_conditions_massive() [STABLE VERSION]...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"<math.h>\"]\n",
    "    desc = r\"\"\"@brief Sets the initial 8-component state vector for a massive particle using a numerically stable method.\"\"\"\n",
    "    name = \"set_initial_conditions_massive\"\n",
    "    params = \"const particle_initial_state_t *restrict initial_state, const commondata_struct *restrict commondata, double y_out[8]\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // Unpack parameters for clarity\n",
    "    const double M = commondata->M_scale;\n",
    "    const double a = commondata->a_spin;\n",
    "    \n",
    "    // Unpack initial position\n",
    "    const double x = initial_state->pos[1];\n",
    "    const double y = initial_state->pos[2];\n",
    "    const double z = initial_state->pos[3];\n",
    "    const double r = sqrt(x*x + y*y + z*z);\n",
    "\n",
    "    // --- Numerically Stable Method to find u^t and u^phi ---\n",
    "    \n",
    "    // Step 1: Calculate Omega = d(phi)/dt\n",
    "    const double Omega = sqrt(M) / (pow(r, 1.5) + a * sqrt(M));\n",
    "\n",
    "    // Step 2: Calculate Boyer-Lindquist metric components in the equatorial plane\n",
    "    const double g_tt = -(1.0 - 2.0*M/r);\n",
    "    const double g_tphi = -2.0*a*M/r;\n",
    "    const double g_phiphi = r*r + a*a + (2.0*M*a*a)/r;\n",
    "\n",
    "    // Step 3: Solve for u^t using the normalization condition\n",
    "    const double ut_inv_denom = g_tt + 2.0*g_tphi*Omega + g_phiphi*Omega*Omega;\n",
    "    if (ut_inv_denom >= 0) {\n",
    "        // This indicates an unstable or invalid orbit. Set state to NaN.\n",
    "        for(int i=0; i<8; i++) y_out[i] = NAN;\n",
    "        return;\n",
    "    }\n",
    "    const double ut = sqrt(-1.0 / ut_inv_denom);\n",
    "    const double uphi = Omega * ut;\n",
    "\n",
    "    // --- Assemble the final 8-component state vector ---\n",
    "    y_out[0] = initial_state->pos[0]; // t\n",
    "    y_out[1] = x;\n",
    "    y_out[2] = y;\n",
    "    y_out[3] = z;\n",
    "    \n",
    "    y_out[4] = ut;\n",
    "    // Transform u^phi to Cartesian u^x, u^y\n",
    "    y_out[5] = -y * uphi;\n",
    "    y_out[6] =  x * uphi;\n",
    "    y_out[7] = 0.0; // u^z is zero for equatorial orbits\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da743f31",
   "metadata": {},
   "source": [
    "# Disk Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disk_initial_conditions():\n",
    "    \"\"\"\n",
    "    Generates a C function that programmatically creates the initial conditions\n",
    "    for a Keplerian disk.\n",
    "    \"\"\"\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\"]\n",
    "    desc = r\"\"\"@brief Generates the complete y[8] initial state for all particles in a Keplerian disk.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"generate_disk_initial_conditions\"\n",
    "\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, double *restrict y_initial_states\"\n",
    "    \n",
    "\n",
    "    body = r\"\"\"\n",
    "    int particle_count = 0;\n",
    "    const double dr = (commondata->disk_num_r > 1) ? (commondata->disk_r_max - commondata->disk_r_min) / (commondata->disk_num_r - 1) : 0;\n",
    "\n",
    "    for (int i = 0; i < commondata->disk_num_r; i++) {\n",
    "        const double r = commondata->disk_r_min + i * dr;\n",
    "        \n",
    "        const int num_phi_at_r = (commondata->disk_num_phi > 1) ? (int)(commondata->disk_num_phi * (r / commondata->disk_r_max)) : 1;\n",
    "        if (num_phi_at_r == 0) continue;\n",
    "        const double dphi = 2.0 * M_PI / num_phi_at_r;\n",
    "        \n",
    "        double ut_at_r, uphi_at_r;\n",
    "        // The call is now valid because 'params' is available in this function's scope.\n",
    "        calculate_ut_uphi_from_r(r, commondata, params, &ut_at_r, &uphi_at_r);\n",
    "\n",
    "        for (int j = 0; j < num_phi_at_r; j++) {\n",
    "            const double phi = j * dphi;\n",
    "            const double cos_phi = cos(phi);\n",
    "            const double sin_phi = sin(phi);\n",
    "            \n",
    "            double *y = &y_initial_states[particle_count * 8];\n",
    "            \n",
    "            y[0] = 0.0;\n",
    "            y[1] = r * cos_phi;\n",
    "            y[2] = r * sin_phi;\n",
    "            y[3] = 0.0;\n",
    "            \n",
    "            y[4] = ut_at_r;\n",
    "            y[5] = -(r * sin_phi) * uphi_at_r;\n",
    "            y[6] =  (r * cos_phi) * uphi_at_r;\n",
    "            y[7] = 0.0;\n",
    "            \n",
    "            particle_count++;\n",
    "        }\n",
    "    }\n",
    "    return particle_count;\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f02175",
   "metadata": {},
   "source": [
    "# Spiral Galaxy initail conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_galaxy_initial_conditions():\n",
    "    \"\"\"\n",
    "    Generates a C function that programmatically creates the initial conditions\n",
    "    for a spiral galaxy disk.\n",
    "    \n",
    "    UPDATED to use runtime parameters from the commondata struct to control\n",
    "    the number and tightness of the spiral arms.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C function: generate_spiral_galaxy_initial_conditions()...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<math.h>\", \"<stdlib.h>\"] # Added stdlib.h for rand()\n",
    "    desc = r\"\"\"@brief Generates the complete y[8] initial state for all particles in a spiral galaxy disk.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"generate_spiral_galaxy_initial_conditions\"\n",
    "\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, double *restrict y_initial_states\"\n",
    "    \n",
    "    body = r\"\"\"\n",
    "    int particle_count = 0;\n",
    "    const int num_particles_total = commondata->disk_num_r * commondata->disk_num_phi;\n",
    "    \n",
    "    // --- Spiral galaxy parameters are now read from the commondata struct ---\n",
    "    const int num_arms = commondata->spiral_galaxy_num_arms;\n",
    "    const double arm_tightness = commondata->spiral_galaxy_arm_tightness;\n",
    "\n",
    "    // Seed the random number generator for reproducibility if needed.\n",
    "    // For true randomness on each run, you could seed with time(NULL).\n",
    "    srand(42); \n",
    "\n",
    "    for (int i = 0; i < num_particles_total; i++) {\n",
    "        // --- Particle Placement Logic (Unchanged, but now uses parameters) ---\n",
    "        \n",
    "        const double r = commondata->disk_r_min + (commondata->disk_r_max - commondata->disk_r_min) * sqrt((double)rand() / RAND_MAX);\n",
    "\n",
    "        // The base angle for this radius from the logarithmic spiral formula.\n",
    "        const double theta_base = (1.0 / arm_tightness) * log(r / commondata->disk_r_min);\n",
    "\n",
    "        const int arm_index = rand() % num_arms;\n",
    "        const double arm_offset = (2.0 * M_PI / num_arms) * arm_index;\n",
    "\n",
    "        const double phi_spread = (M_PI / num_arms) * 0.2 * ((double)rand() / RAND_MAX - 0.5);\n",
    "        \n",
    "        const double phi = theta_base + arm_offset + phi_spread;\n",
    "        \n",
    "        // --- The rest of this logic is IDENTICAL to the original ---\n",
    "        \n",
    "        const double cos_phi = cos(phi);\n",
    "        const double sin_phi = sin(phi);\n",
    "        \n",
    "        double ut_at_r, uphi_at_r;\n",
    "        calculate_ut_uphi_from_r(r, commondata, params, &ut_at_r, &uphi_at_r);\n",
    "\n",
    "        double *y = &y_initial_states[particle_count * 8];\n",
    "        \n",
    "        y[0] = 0.0;\n",
    "        y[1] = r * cos_phi;\n",
    "        y[2] = r * sin_phi;\n",
    "        y[3] = 0.0;\n",
    "        y[4] = ut_at_r;\n",
    "        y[5] = -(r * sin_phi) * uphi_at_r;\n",
    "        y[6] =  (r * cos_phi) * uphi_at_r;\n",
    "        y[7] = 0.0;\n",
    "        \n",
    "        particle_count++;\n",
    "    }\n",
    "    return particle_count;\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )\n",
    "    print(\"    ... generate_spiral_galaxy_initial_conditions() registration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad147797",
   "metadata": {},
   "source": [
    "# Generate barred flocculent spiral ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef21456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_barred_flocculent_spiral_ic():\n",
    "    \"\"\"\n",
    "    Generates a C function that programmatically creates the initial conditions\n",
    "    for a realistic barred spiral galaxy with clumpy, flocculent arms.\n",
    "    \n",
    "    This function uses rejection sampling to place particles in one of three regions:\n",
    "    a central bulge, a rectangular bar, or flocculent spiral arms.\n",
    "    \n",
    "    Despite the complex geometry, all particles are placed on stable, circular\n",
    "    Keplerian orbits, satisfying the project constraints.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C function: generate_barred_flocculent_spiral_ic()...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<math.h>\", \"<stdlib.h>\", \"<time.h>\"]\n",
    "    desc = r\"\"\"@brief Generates the initial state for all particles in a barred flocculent spiral galaxy.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"generate_barred_flocculent_spiral_ic\"\n",
    "\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, double *restrict y_initial_states\"\n",
    "    \n",
    "\n",
    "\n",
    "    body = r\"\"\"\n",
    "        // Seed the random number generator. A unique seed is needed for each thread in parallel.\n",
    "        // We will use the particle index + a base seed.\n",
    "        unsigned int seed = 42;\n",
    "\n",
    "        int particle_count = 0;\n",
    "        const int num_particles_total = commondata->disk_num_r * commondata->disk_num_phi;\n",
    "\n",
    "        // Unpack geometry parameters from commondata for clarity\n",
    "        const double r_min = commondata->disk_r_min;\n",
    "        const double r_max = commondata->disk_r_max;\n",
    "        const double bar_len = commondata->bar_length;\n",
    "        const double bar_width = commondata->bar_length * commondata->bar_aspect_ratio;\n",
    "        const double bulge_rad = commondata->bulge_radius;\n",
    "        const int num_arms = commondata->spiral_galaxy_num_arms;\n",
    "        const double arm_tightness = commondata->spiral_galaxy_arm_tightness;\n",
    "\n",
    "        for (int i = 0; i < num_particles_total; i++) {\n",
    "            double r, phi, x, y;\n",
    "            \n",
    "            // Use the thread-safe random number generator\n",
    "            seed += i;\n",
    "\n",
    "            // --- REJECTION SAMPLING LOOP ---\n",
    "            while (1) {\n",
    "                // Generate a trial particle position uniformly in the disk annulus\n",
    "                double random_val = (double)rand_r(&seed) / RAND_MAX;\n",
    "                r = sqrt(random_val * (r_max*r_max - r_min*r_min) + r_min*r_min);\n",
    "                phi = 2.0 * M_PI * ((double)rand_r(&seed) / RAND_MAX);\n",
    "                x = r * cos(phi);\n",
    "                y = r * sin(phi);\n",
    "\n",
    "                double acceptance_prob = 0.0;\n",
    "\n",
    "                // Region 1: Central Bulge\n",
    "                if (r < bulge_rad) {\n",
    "                    acceptance_prob = commondata->bulge_density_factor * commondata->arm_particle_density;\n",
    "                }\n",
    "                // Region 2: Central Bar\n",
    "                else if (fabs(x) < bar_len / 2.0 && fabs(y) < bar_width / 2.0) {\n",
    "                    acceptance_prob = commondata->bar_density_factor * commondata->arm_particle_density;\n",
    "                }\n",
    "                // Region 3: Flocculent Spiral Arms\n",
    "                else {\n",
    "                    // Optimization: Do a cheap check first. If the base probability fails,\n",
    "                    // no need to do expensive log/exp/cos calls.\n",
    "                    if (((double)rand_r(&seed) / RAND_MAX) < commondata->arm_particle_density) {\n",
    "                        // --- CORRECTED ARM PROFILE LOGIC ---\n",
    "                        double theta_base = (1.0 / arm_tightness) * log(r / r_min);\n",
    "                        double delta_phi_raw = phi - theta_base;\n",
    "                        double angle_between_arms = 2.0 * M_PI / num_arms;\n",
    "                        double delta_phi_folded = fmod(delta_phi_raw, angle_between_arms);\n",
    "\n",
    "                        if (delta_phi_folded > 0.5 * angle_between_arms) {\n",
    "                            delta_phi_folded -= angle_between_arms;\n",
    "                        } else if (delta_phi_folded < -0.5 * angle_between_arms) {\n",
    "                            delta_phi_folded += angle_between_arms;\n",
    "                        }\n",
    "                        \n",
    "                        double arm_profile = exp(-0.5 * SQR(delta_phi_folded / (arm_tightness * 0.5)));\n",
    "                        \n",
    "                        // Flocculent/clumpy modulation\n",
    "                        double clump_modulation = cos(commondata->arm_clumpiness_factor * (theta_base - phi));\n",
    "                        clump_modulation *= cos(num_arms * phi * commondata->arm_clump_size);\n",
    "                        \n",
    "                        acceptance_prob = arm_profile + 0.5 * clump_modulation;\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if (((double)rand_r(&seed) / RAND_MAX) < acceptance_prob) {\n",
    "                    break; // Accept this particle's position\n",
    "                }\n",
    "            } // End of rejection sampling while loop\n",
    "\n",
    "            // --- Velocity Assignment ---\n",
    "            double ut_at_r, uphi_at_r;\n",
    "            calculate_ut_uphi_from_r(r, commondata, params, &ut_at_r, &uphi_at_r);\n",
    "\n",
    "            // --- Store the Full 8-Component State Vector ---\n",
    "            double *y_state = &y_initial_states[particle_count * 8];\n",
    "            \n",
    "            y_state[0] = 0.0;\n",
    "            y_state[1] = x;\n",
    "            y_state[2] = y;\n",
    "            y_state[3] = 0.0;\n",
    "            y_state[4] = ut_at_r;\n",
    "            y_state[5] = -y * uphi_at_r;\n",
    "            y_state[6] =  x * uphi_at_r;\n",
    "            y_state[7] = 0.0;\n",
    "            \n",
    "            particle_count++;\n",
    "        }\n",
    "        \n",
    "        return particle_count;\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdf4ce",
   "metadata": {},
   "source": [
    "<a id='gsl_wrapper'></a>\n",
    "### 7.d: The GSL Wrapper Function (Redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec563f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_gsl_wrapper_massive():\n",
    "    \"\"\"\n",
    "    Generates the C function that acts as a bridge between the GSL ODE\n",
    "    solver and our project-specific physics functions.\n",
    "    \"\"\"\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"gsl/gsl_errno.h\"]\n",
    "    desc = r\"\"\"@brief GSL wrapper for the massive particle geodesic ODEs.\n",
    "    \n",
    "    This function matches the signature required by the GSL ODE solver. It unpacks\n",
    "    the gsl_params carrier struct and calls the dispatchers for the metric and\n",
    "    Christoffel symbols, before finally calling the RHS engine.\n",
    "    \n",
    "    @param[in]  t      The current value of the independent variable (proper time τ). Unused.\n",
    "    @param[in]  y      The current 8-component state vector.\n",
    "    @param[in]  params A generic void pointer to our gsl_params carrier struct.\n",
    "    @param[out] f      A pointer to the 8-component output array where GSL expects the RHS results.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"ode_gsl_wrapper_massive\"\n",
    "    params = \"double t, const double y[8], double f[8], void *params\"\n",
    "    \n",
    "    body = r\"\"\"\n",
    "        (void)t; // Proper time 't' is not explicitly used in the RHS expressions.\n",
    "        \n",
    "        // Unpack the carrier struct to access simulation parameters and metric choice.\n",
    "        gsl_params *gsl_parameters = (gsl_params *)params;\n",
    "        \n",
    "        // Declare structs to hold metric and connection values.\n",
    "        metric_struct g4DD;\n",
    "        connection_struct conn;\n",
    "        \n",
    "        // Call dispatchers to compute the metric and Christoffel symbols at the current position y.\n",
    "        // Note: The y array is 8D, but these functions only need the first 4 position components.\n",
    "        g4DD_metric(gsl_parameters->commondata, gsl_parameters->params, gsl_parameters->metric, y, &g4DD);\n",
    "        connections(gsl_parameters->commondata, gsl_parameters->params, gsl_parameters->metric, y, &conn);\n",
    "        \n",
    "        // Call the engine to compute the RHS of the ODEs.\n",
    "        calculate_ode_rhs_massive(y, &conn, f);\n",
    "        \n",
    "        return GSL_SUCCESS;\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name, \n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb240e",
   "metadata": {},
   "source": [
    "# Run Mass integrator production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mass_integrator_production():\n",
    "    \"\"\"\n",
    "    Generates the C orchestrator for the production run.\n",
    "    \n",
    "    This definitive version acts as a dispatcher. It reads the \n",
    "    initial_conditions_type parameter and calls the appropriate particle \n",
    "    generator function before starting the full integration and snapshotting loop.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C orchestrator: run_mass_integrator_production() [Dispatcher Version]...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<math.h>\", \"<sys/stat.h>\", \"<string.h>\", \"<stdio.h>\", \"<stdlib.h>\"]\n",
    "    desc = r\"\"\"@brief Orchestrates the full production run for the mass integrator.\"\"\"\n",
    "    name = \"run_mass_integrator_production\"\n",
    "    params = \"const commondata_struct *restrict commondata, const params_struct *restrict params, const metric_params *restrict metric\"\n",
    "    \n",
    "    body = r\"\"\"\n",
    "    // Step 1: Allocate memory for the flat initial state array\n",
    "    const int max_particles = commondata->disk_num_r * commondata->disk_num_phi;\n",
    "    double *y_initial_states = (double *)malloc(sizeof(double) * 8 * max_particles);\n",
    "    if (y_initial_states == NULL) { fprintf(stderr, \"Error: Failed to allocate memory for initial states.\\n\"); exit(1); }\n",
    "    int num_particles = 0;\n",
    "\n",
    "    // --- Step 2: Dispatch to the correct initial conditions generator ---\n",
    "    printf(\"Generating initial conditions of type: %s\\n\", commondata->initial_conditions_type);\n",
    "     if (strcmp(commondata->initial_conditions_type, \"SpiralGalaxy\") == 0) {\n",
    "        num_particles = generate_spiral_galaxy_initial_conditions(commondata, params, y_initial_states);\n",
    "    } else if (strcmp(commondata->initial_conditions_type, \"BarredFlocculentSpiral\") == 0) {\n",
    "        num_particles = generate_barred_flocculent_spiral_ic(commondata, params, y_initial_states);\n",
    "    } else {// Default to KeplerianDisk\n",
    "        if (strcmp(commondata->initial_conditions_type, \"KeplerianDisk\") != 0) {\n",
    "            printf(\"Warning: Unrecognized initial_conditions_type '%s'. Defaulting to KeplerianDisk.\\n\", commondata->initial_conditions_type);\n",
    "        }\n",
    "        num_particles = generate_disk_initial_conditions(commondata, params, y_initial_states);\n",
    "    }\n",
    "    printf(\"Generated %d particles.\\n\", num_particles);\n",
    "\n",
    "    // Step 3: Create and populate our primary data structure, an array of mass_particle_state_t\n",
    "    mass_particle_state_t *particle_states = (mass_particle_state_t *)malloc(sizeof(mass_particle_state_t) * num_particles);\n",
    "    if (particle_states == NULL) { fprintf(stderr, \"Error: Failed to allocate memory for particle states.\\n\"); exit(1); }\n",
    "    for (int i=0; i<num_particles; i++) {\n",
    "        double *y_start = &y_initial_states[i*8];\n",
    "        particle_states[i].id = i;\n",
    "        particle_states[i].pos[0] = y_start[1];\n",
    "        particle_states[i].pos[1] = y_start[2];\n",
    "        particle_states[i].pos[2] = y_start[3];\n",
    "        particle_states[i].u[0] = y_start[4];\n",
    "        particle_states[i].u[1] = y_start[5];\n",
    "        particle_states[i].u[2] = y_start[6];\n",
    "        particle_states[i].u[3] = y_start[7];\n",
    "        \n",
    "        const double r = sqrt(y_start[1]*y_start[1] + y_start[2]*y_start[2]);\n",
    "        particle_states[i].lambda_rest = commondata->disk_lambda_rest_at_r_min * pow(r / commondata->disk_r_min, 0.75);\n",
    "        particle_states[i].j_intrinsic = (float)pow(r / commondata->disk_r_min, -3.0);\n",
    "    }\n",
    "    free(y_initial_states); \n",
    "\n",
    "    // Step 4: Create the output directory\n",
    "    mkdir(commondata->output_folder, 0755);\n",
    "\n",
    "    // Step 5: Main Time Evolution and Snapshotting Loop\n",
    "    int snapshot_count = 0;\n",
    "    for (double current_t = 0; current_t <= commondata->t_final; current_t += commondata->snapshot_every_t) {\n",
    "        char filename[200];\n",
    "        snprintf(filename, 200, \"%s/mass_blueprint_t_%04d.bin\", commondata->output_folder, snapshot_count);\n",
    "        printf(\"Saving snapshot: %s (t=%.2f)\\n\", filename, current_t);\n",
    "        \n",
    "        FILE *fp_out = fopen(filename, \"wb\");\n",
    "        if (fp_out == NULL) { exit(1); }\n",
    "        \n",
    "        int active_particles = 0;\n",
    "        for(int i=0; i<num_particles; i++) {\n",
    "            if (!isnan(particle_states[i].pos[0])) active_particles++;\n",
    "        }\n",
    "        fwrite(&active_particles, sizeof(int), 1, fp_out);\n",
    "        for (int i=0; i<num_particles; i++) {\n",
    "            if (!isnan(particle_states[i].pos[0])) {\n",
    "                fwrite(&particle_states[i], sizeof(mass_particle_state_t), 1, fp_out);\n",
    "            }\n",
    "        }\n",
    "        fclose(fp_out);\n",
    "        snapshot_count++;\n",
    "\n",
    "        if (current_t >= commondata->t_final) break;\n",
    "\n",
    "        // Evolve all particles for one snapshot interval\n",
    "        const double t_next_snapshot = current_t + commondata->snapshot_every_t;\n",
    "        #pragma omp parallel for\n",
    "        for (int i = 0; i < num_particles; i++) {\n",
    "            if (isnan(particle_states[i].pos[0])) continue;\n",
    "\n",
    "            double y_particle[8];\n",
    "            y_particle[0] = current_t;\n",
    "            y_particle[1] = particle_states[i].pos[0];\n",
    "            y_particle[2] = particle_states[i].pos[1];\n",
    "            y_particle[3] = particle_states[i].pos[2];\n",
    "            y_particle[4] = particle_states[i].u[0];\n",
    "            y_particle[5] = particle_states[i].u[1];\n",
    "            y_particle[6] = particle_states[i].u[2];\n",
    "            y_particle[7] = particle_states[i].u[3];\n",
    "\n",
    "            int status = integrate_single_particle(commondata, params, metric, y_particle[0], t_next_snapshot, y_particle);\n",
    "            \n",
    "            if (status != 0) {\n",
    "                particle_states[i].pos[0] = NAN; // Mark particle as terminated\n",
    "            } else {\n",
    "                particle_states[i].pos[0] = y_particle[1];\n",
    "                particle_states[i].pos[1] = y_particle[2];\n",
    "                particle_states[i].pos[2] = y_particle[3];\n",
    "                particle_states[i].u[0] = y_particle[4];\n",
    "                particle_states[i].u[1] = y_particle[5];\n",
    "                particle_states[i].u[2] = y_particle[6];\n",
    "                particle_states[i].u[3] = y_particle[7];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    free(particle_states);\n",
    "    \"\"\"\n",
    "    cfc.register_CFunction(includes=includes, desc=desc, name=name, params=params, body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6d709",
   "metadata": {},
   "source": [
    "# Main integration code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ba2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_single_particle():\n",
    "    \"\"\"\n",
    "    Generates the main C integration loop for a single massive particle.\n",
    "    This high-performance \"production\" version uses the GSL driver to ensure\n",
    "    the state is returned at the exact requested snapshot times.\n",
    "    \"\"\"\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"gsl/gsl_errno.h\", \"gsl/gsl_odeiv2.h\", \"<math.h>\"]\n",
    "    desc = r\"\"\"@brief Integrates a single massive particle path between two times.\n",
    "    \n",
    "    This function uses the GSL driver, which internally uses an adaptive\n",
    "    step-size algorithm (RKF45) to evolve the particle's state vector y_in_out\n",
    "    from t_start to t_end, returning the state at the precise t_end.\n",
    "    \n",
    "    @param[in]      commondata  Pointer to commondata struct.\n",
    "    @param[in]      params      Pointer to params_struct.\n",
    "    @param[in]      metric      Pointer to metric_params struct.\n",
    "    @param[in]      t_start     The starting proper time (τ) for the integration.\n",
    "    @param[in]      t_end       The ending proper time (τ) for the integration.\n",
    "    @param[in,out]  y_in_out    The 8-component state vector. Input is the state at t_start, output is the state at t_end.\n",
    "    \n",
    "    @return 0 on success, 1 on GSL failure.\n",
    "    \"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"integrate_single_particle\"\n",
    "    params = \"\"\"const commondata_struct *restrict commondata,\n",
    "    const params_struct *restrict params,\n",
    "    const metric_params *restrict metric,\n",
    "    const double t_start, const double t_end,\n",
    "    double y_in_out[8]\"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // Define the GSL ODE system\n",
    "    gsl_params gsl_parameters = {commondata, params, metric};\n",
    "    gsl_odeiv2_system sys = {ode_gsl_wrapper_massive, NULL, 8, &gsl_parameters};\n",
    "    \n",
    "    // Set up the GSL driver\n",
    "    gsl_odeiv2_driver *d = gsl_odeiv2_driver_alloc_y_new(\n",
    "        &sys, gsl_odeiv2_step_rkf45, 1e-6, 1e-11, 1e-11);\n",
    "    \n",
    "    double t = t_start;\n",
    "    \n",
    "    // The driver will take internal steps to reach t_end precisely.\n",
    "    int status = gsl_odeiv2_driver_apply(d, &t, t_end, y_in_out);\n",
    "\n",
    "    if (status != GSL_SUCCESS) {\n",
    "        // Don't print an error here, as the orchestrator will check the status.\n",
    "        // Just free memory and return the failure code.\n",
    "        gsl_odeiv2_driver_free(d);\n",
    "        return 1; // Return failure code\n",
    "    }\n",
    "\n",
    "    // Robustness check after the step\n",
    "    const double r_sq = y_in_out[1]*y_in_out[1] + y_in_out[2]*y_in_out[2] + y_in_out[3]*y_in_out[3];\n",
    "    const double r_horizon = commondata->M_scale * (1.0 + sqrt(1.0 - commondata->a_spin*commondata->a_spin));\n",
    "\n",
    "    if (r_sq < r_horizon*r_horizon || r_sq > r_escape*r_escape || fabs(y_in_out[4]) > ut_max) {\n",
    "        gsl_odeiv2_driver_free(d);\n",
    "        return 1; // Return failure code\n",
    "    }\n",
    "\n",
    "    gsl_odeiv2_driver_free(d);\n",
    "    return 0; // Return success code\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body, \n",
    "        include_CodeParameters_h=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3977c",
   "metadata": {},
   "source": [
    "# Bebugging integration (for single masslike particle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ff83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_single_particle_DEBUG():\n",
    "    \"\"\"\n",
    "    Generates a DEBUG version of the integrator that writes the full trajectory\n",
    "    of a single massive particle to a text file for validation.\n",
    "    \"\"\"\n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"gsl/gsl_errno.h\", \"gsl/gsl_odeiv2.h\"]\n",
    "    desc = r\"\"\"@brief DEBUG integrator for a single massive particle.\n",
    "    \n",
    "    This function integrates the path of a single particle and writes the full\n",
    "    8-component state vector at each step to 'massive_particle_path.txt'.\n",
    "    It also prints progress to the console and checks for termination conditions.\n",
    "    \n",
    "    @param[in]  commondata  Pointer to the commondata_struct.\n",
    "    @param[in]  params      Pointer to the params_struct.\n",
    "    @param[in]  metric      Pointer to the metric_params struct.\n",
    "    @param[in]  start_y     The 8-component initial state vector.\n",
    "    @param[out] final_y_state The 8-component final state vector upon termination.\"\"\"\n",
    "    cfunc_type = \"void\"\n",
    "    name = \"integrate_single_particle_DEBUG\"\n",
    "    params = \"\"\"const commondata_struct *restrict commondata,\n",
    "    const params_struct *restrict params,\n",
    "    const metric_params *restrict metric,\n",
    "    const double start_y[8],\n",
    "    double final_y_state[8]\"\"\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // GSL Setup\n",
    "    const gsl_odeiv2_step_type * T = gsl_odeiv2_step_rkf45;\n",
    "    gsl_odeiv2_step * step = gsl_odeiv2_step_alloc(T, 8);\n",
    "    gsl_odeiv2_control * control = gsl_odeiv2_control_yp_new(1e-14, 1e-14);\n",
    "    gsl_odeiv2_evolve * evol = gsl_odeiv2_evolve_alloc(8);\n",
    "    gsl_params gsl_parameters = {commondata, params, metric};\n",
    "    gsl_odeiv2_system sys = {ode_gsl_wrapper_massive, NULL, 8, &gsl_parameters};\n",
    "\n",
    "    double y_c[8];\n",
    "    double t = 0.0, dt = 0.01; // t is proper time τ\n",
    "    for (int j = 0; j < 8; j++) { y_c[j] = start_y[j]; }\n",
    "\n",
    "    // Setup output file\n",
    "    FILE *fp = fopen(\"massive_particle_path.txt\", \"w\");\n",
    "    if (fp == NULL) { \n",
    "        fprintf(stderr, \"Error: Could not open massive_particle_path.txt for writing.\\n\");\n",
    "        exit(1); \n",
    "    }\n",
    "    fprintf(fp, \"# ProperTime_tau\\tCoordTime_t\\tx\\ty\\tz\\tu^t\\tu^x\\tu^y\\tu^z\\n\");\n",
    "\n",
    "    printf(\"Starting debug trace for single massive particle...\\n\");\n",
    "    printf(\"Step | Proper Time (τ) | Coord Time (t) |      x     |      y     |      z     |      u^t   \\n\");\n",
    "    printf(\"-------------------------------------------------------------------------------------------\\n\");\n",
    "\n",
    "    // Main Integration Loop\n",
    "    for (int i = 0; i < 2000000; i++) {\n",
    "        int status = gsl_odeiv2_evolve_apply(evol, control, step, &sys, &t, 1e10, &dt, y_c);\n",
    "        \n",
    "        // Write full state to file\n",
    "        fprintf(fp, \"%.6e\\t%.6e\\t%.6e\\t%.6e\\t%.6e\\t%.6e\\t%.6e\\t%.6e\\t%.6e\\n\", \n",
    "                t, y_c[0], y_c[1], y_c[2], y_c[3], y_c[4], y_c[5], y_c[6], y_c[7]);\n",
    "\n",
    "        if (i % 500 == 0) {\n",
    "            printf(\"%4d | %15.4e | %14.4f | %10.4f | %10.4f | %10.4f | %10.4f\\n\",\n",
    "                   i, t, y_c[0], y_c[1], y_c[2], y_c[3], y_c[4]);\n",
    "        }\n",
    "\n",
    "        const double r_sq = y_c[1]*y_c[1] + y_c[2]*y_c[2] + y_c[3]*y_c[3];\n",
    "        // Event horizon radius for a Kerr black hole\n",
    "        const double r_horizon = commondata->M_scale * (1.0 + sqrt(1.0 - commondata->a_spin*commondata->a_spin));\n",
    "\n",
    "        // Termination Conditions\n",
    "        if (status != GSL_SUCCESS) { printf(\"Termination: GSL ERROR (status = %d)\\n\", status); break; }\n",
    "        if (r_sq < r_horizon*r_horizon) { printf(\"Termination: Fell below event horizon (r=%.2f)\\n\", sqrt(r_sq)); break; }\n",
    "        if (r_sq > r_escape*r_escape) { printf(\"Termination: Escaped to r > %.1f\\n\", r_escape); break; }\n",
    "        if (fabs(y_c[4]) > ut_max) { printf(\"Termination: Runaway u^t > %.1e\\n\", ut_max); break; }\n",
    "        if (fabs(y_c[0]) > t_max_integration) { printf(\"Termination: Exceeded max integration time t > %.1f\\n\", t_max_integration); break; }\n",
    "    }\n",
    "\n",
    "    // Copy final state to output and clean up\n",
    "    for(int j=0; j<8; j++) { final_y_state[j] = y_c[j]; }\n",
    "    fclose(fp);\n",
    "    gsl_odeiv2_evolve_free(evol);\n",
    "    gsl_odeiv2_control_free(control);\n",
    "    gsl_odeiv2_step_free(step);\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body, \n",
    "        include_CodeParameters_h=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4eaf0",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Generates the main() C function.\n",
    "    \n",
    "    This final version restores the detailed parameter printout for production runs.\n",
    "    The production run logic is now handled by the run_mass_integrator_production dispatcher.\n",
    "    \"\"\"\n",
    "    print(\" -> Generating C entry point: main() [Final Version]...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<string.h>\", \"<stdio.h>\", \"<stdlib.h>\"]\n",
    "    \n",
    "    desc = r\"\"\"@brief Main entry point for the massive particle geodesic integrator.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"main\"\n",
    "    params = \"int argc, const char *argv[]\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // Step 1: Initialize structs and parameters\n",
    "    commondata_struct commondata;\n",
    "    params_struct params; // This struct is currently unused but required by function signatures.\n",
    "    metric_params metric;\n",
    "    \n",
    "    commondata_struct_set_to_default(&commondata);\n",
    "    cmdline_input_and_parfile_parser(&commondata, argc, argv);\n",
    "    \n",
    "    metric.type = (commondata.a_spin == 0.0) ? Schwarzschild : Kerr;\n",
    "\n",
    "    // Step 2: Check the run mode and execute the appropriate logic\n",
    "    if (commondata.run_in_debug_mode) {\n",
    "        /***********************************/\n",
    "        /*** SINGLE-PARTICLE DEBUG MODE ***/\n",
    "        /***********************************/\n",
    "        particle_initial_state_t initial_state;\n",
    "        initial_state.id = 0;\n",
    "\n",
    "        const char *filename = \"particle_debug_initial_conditions.txt\";\n",
    "        FILE *fp_in = fopen(filename, \"r\");\n",
    "        \n",
    "        // --- RESTORED FALLBACK LOGIC ---\n",
    "        // If the file doesn't exist, create it with default values.\n",
    "        if (fp_in == NULL) {\n",
    "            printf(\"File '%s' not found. Creating it with default values.\\n\", filename);\n",
    "            fp_in = fopen(filename, \"w\");\n",
    "            if (fp_in == NULL) { \n",
    "                fprintf(stderr, \"Error: Could not create '%s'.\\n\", filename); \n",
    "                return 1; \n",
    "            }\n",
    "            fprintf(fp_in, \"# Format: t_initial pos_x pos_y pos_z u_x u_y u_z\\n\");\n",
    "            fprintf(fp_in, \"# u_i are the spatial components of the 4-velocity (dx/dτ).\\n\");\n",
    "            fprintf(fp_in, \"0.0 10.0  0.0  0.0   0.0  0.363232  0.0\\n\");\n",
    "            fclose(fp_in);\n",
    "            \n",
    "            // Re-open the newly created file for reading\n",
    "            fp_in = fopen(filename, \"r\");\n",
    "            if (fp_in == NULL) { \n",
    "                fprintf(stderr, \"Error: Could not re-open '%s' for reading.\\n\", filename); \n",
    "                return 1; \n",
    "            }\n",
    "        }\n",
    "        // --- END OF RESTORED LOGIC ---\n",
    "\n",
    "        char line[256];\n",
    "        while (fgets(line, sizeof(line), fp_in)) {\n",
    "            if (line[0] != '#') {\n",
    "                sscanf(line, \"%lf %lf %lf %lf %lf %lf %lf\", \n",
    "                       &initial_state.pos[0], &initial_state.pos[1], &initial_state.pos[2], &initial_state.pos[3],\n",
    "                       &initial_state.u_spatial[0], &initial_state.u_spatial[1], &initial_state.u_spatial[2]);\n",
    "                break; \n",
    "            }\n",
    "        }\n",
    "        fclose(fp_in);\n",
    "\n",
    "        printf(\"--- Single Particle Debug Run ---\\n\");\n",
    "        printf(\"  pos = (t=%.4f, x=%.4f, y=%.4f, z=%.4f)\\n\", initial_state.pos[0], initial_state.pos[1], initial_state.pos[2], initial_state.pos[3]);\n",
    "        printf(\"  u_spatial = (%.4f, %.4f, %.4f)\\n\", initial_state.u_spatial[0], initial_state.u_spatial[1], initial_state.u_spatial[2]);\n",
    "\n",
    "        double y_start[8], y_final[8];\n",
    "        \n",
    "\n",
    "        set_initial_conditions_massive(&initial_state, &commondata, y_start);\n",
    "\n",
    "\n",
    "        printf(\"\\nInitial State Vector (y_start):\\n\");\n",
    "        printf(\"  t=%.2f, x=%.2f, y=%.2f, z=%.2f\\n\", y_start[0], y_start[1], y_start[2], y_start[3]);\n",
    "        printf(\"  u^t=%.4f, u^x=%.4f, u^y=%.4f, u^z=%.4f\\n\\n\", y_start[4], y_start[5], y_start[6], y_start[7]);\n",
    "\n",
    "        if(commondata.perform_conservation_check) {\n",
    "            double E_i, Lx_i, Ly_i, Lz_i, Q_i, E_f, Lx_f, Ly_f, Lz_f, Q_f;\n",
    "            check_conservation_massive(&commondata, &params, &metric, y_start, &E_i, &Lx_i, &Ly_i, &Lz_i, &Q_i);\n",
    "            integrate_single_particle_DEBUG(&commondata, &params, &metric, y_start, y_final);\n",
    "            check_conservation_massive(&commondata, &params, &metric, y_final, &E_f, &Lx_f, &Ly_f, &Lz_f, &Q_f);\n",
    "        } else {\n",
    "            integrate_single_particle_DEBUG(&commondata, &params, &metric, y_start, y_final);\n",
    "        }\n",
    "        \n",
    "        printf(\"\\nDebug run finished. Trajectory saved to 'massive_particle_path.txt'.\\n\");\n",
    "\n",
    "    } else {\n",
    "        /***********************************/\n",
    "        /*** FULL DISK PRODUCTION MODE ***/\n",
    "        /***********************************/\n",
    "        printf(\"----------------------------------------\\n\");\n",
    "        printf(\"Massive Particle Integrator\\n\");\n",
    "        printf(\"----------------------------------------\\n\");\n",
    "        printf(\"Metric Settings:\\n\");\n",
    "        printf(\"  Metric Type             = %s (a=%.2f, M=%.2f)\\n\", (metric.type == Kerr) ? \"Kerr\" : \"Schwarzschild\", commondata.a_spin, commondata.M_scale);\n",
    "        printf(\"\\nIntegration Settings:\\n\");\n",
    "        printf(\"  Max Integration Time    = %.1f M\\n\", commondata.t_final);\n",
    "        printf(\"  Snapshot Every          = %.1f M\\n\", commondata.snapshot_every_t);\n",
    "        printf(\"  Escape Radius           = %.1f M\\n\", commondata.r_escape);\n",
    "        printf(\"\\nInitial Conditions:\\n\");\n",
    "        printf(\"  Generator Type          = %s\\n\", commondata.initial_conditions_type);\n",
    "        printf(\"  Num Particles (r x phi) = %d x %d\\n\", commondata.disk_num_r, commondata.disk_num_phi);\n",
    "        printf(\"  Disk Radial Min/Max     = %.2f / %.2f M\\n\", commondata.disk_r_min, commondata.disk_r_max);\n",
    "        printf(\"----------------------------------------\\n\\n\");\n",
    "\n",
    "        run_mass_integrator_production(&commondata, &params, &metric);\n",
    "        printf(\"\\nProduction run finished successfully.\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "    \"\"\"\n",
    "    \n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5522b",
   "metadata": {},
   "source": [
    "# Debugging main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_5_mass_geodesic.ipynb, add as a NEW TEMPORARY CELL\n",
    "\n",
    "def main_ic_validation():\n",
    "    \"\"\"\n",
    "    Generates a TEMPORARY main() C function for the sole purpose of validating\n",
    "    the output of the new, numerically stable 'calculate_ut_uphi_from_r' function.\n",
    "    \n",
    "    This main() will loop through a pre-defined set of (a, r) pairs, call the\n",
    "    C function, and print the resulting u^t and u^phi to the console.\n",
    "    \"\"\"\n",
    "    print(\" -> Registering TEMPORARY main() function for IC validation...\")\n",
    "    \n",
    "    includes = [\"BHaH_defines.h\", \"BHaH_function_prototypes.h\", \"<stdio.h>\"]\n",
    "    desc = r\"\"\"@brief Temporary main() for validating initial conditions.\n",
    "    \n",
    "    This program tests the numerically stable 'calculate_ut_uphi_from_r' C function\n",
    "    by calling it for a range of black hole spins 'a' and orbital radii 'r'.\n",
    "    It prints the resulting 4-velocity components u^t and u^phi to the console.\"\"\"\n",
    "    cfunc_type = \"int\"\n",
    "    name = \"main\"\n",
    "    params = \"int argc, const char *argv[]\"\n",
    "\n",
    "    body = r\"\"\"\n",
    "    // Suppress unused parameter warnings for this temporary main\n",
    "    (void)argc;\n",
    "    (void)argv;\n",
    "\n",
    "    // Initialize the commondata struct to get default M_scale\n",
    "    commondata_struct commondata;\n",
    "    params_struct params; // Dummy struct, not used but required by function signature\n",
    "    commondata_struct_set_to_default(&commondata);\n",
    "\n",
    "    printf(\"--- Initial Condition Validation for calculate_ut_uphi_from_r() ---\\n\");\n",
    "    printf(\"      (Using numerically stable method)\\n\\n\");\n",
    "    printf(\"%-10s | %-10s | %-20s | %-20s\\n\", \"a_spin\", \"r_initial\", \"u^t (dt/d_tau)\", \"u^phi (d_phi/d_tau)\");\n",
    "    printf(\"--------------------------------------------------------------------------\\n\");\n",
    "\n",
    "    // Define the array of test cases: {a_spin, r_initial}\n",
    "    const double test_cases[][2] = {\n",
    "        // Prograde, high spin (a=0.99), near ISCO (~1.23M)\n",
    "        {0.99, 1.5},\n",
    "        {0.99, 2.0},\n",
    "        {0.99, 6.0},\n",
    "        \n",
    "        // Schwarzschild (a=0.0), near ISCO (6M)\n",
    "        {0.0, 6.0},\n",
    "        {0.0, 8.0},\n",
    "        {0.0, 20.0},\n",
    "\n",
    "        // Retrograde, high spin (a=-0.99), near ISCO (~8.98M)\n",
    "        {-0.99, 9.0},\n",
    "        {-0.99, 12.0},\n",
    "        {-0.99, 50.0}\n",
    "    };\n",
    "    const int num_test_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n",
    "\n",
    "    for (int i = 0; i < num_test_cases; ++i) {\n",
    "        commondata.a_spin = test_cases[i][0];\n",
    "        const double r_initial = test_cases[i][1];\n",
    "        \n",
    "        double ut, uphi;\n",
    "        \n",
    "        // Call the C function we want to test\n",
    "        calculate_ut_uphi_from_r(r_initial, &commondata, &params, &ut, &uphi);\n",
    "        \n",
    "        printf(\"%-10.2f | %-10.2f | %-20.10f | %-20.10f\\n\", \n",
    "               commondata.a_spin, r_initial, ut, uphi);\n",
    "    }\n",
    "\n",
    "    printf(\"--------------------------------------------------------------------------\\n\");\n",
    "    printf(\"Validation complete. Check for NaN/inf values and physical consistency.\\n\");\n",
    "\n",
    "    return 0;\n",
    "    \"\"\"\n",
    "    \n",
    "    # This will overwrite any existing 'main' function in the dictionary,\n",
    "    # which is what we want for this temporary test.\n",
    "    cfc.register_CFunction(\n",
    "        includes=includes,\n",
    "        desc=desc,\n",
    "        cfunc_type=cfunc_type,\n",
    "        name=name,\n",
    "        params=params,\n",
    "        body=body\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8cb55",
   "metadata": {},
   "source": [
    "\n",
    "# Step 8: Project Assembly and Compilation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef03c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_custom_structures_and_params():\n",
    "    \"\"\"\n",
    "    Generates C code for all custom structs and enums, then registers them with BHaH.\n",
    "    \"\"\"\n",
    "    print(\"Registering custom C data structures for mass integrator...\")\n",
    "    \n",
    "    metric_components = [f\"g{nu}{mu}\" for nu in range(4) for mu in range(nu, 4)]\n",
    "    metric_struct_str = \"typedef struct { double \" + \"; double \".join(metric_components) + \"; } metric_struct;\"\n",
    "    \n",
    "    connection_components = [f\"Gamma4UDD{i}{j}{k}\" for i in range(4) for j in range(4) for k in range(j, 4)]\n",
    "    connections_struct_str = \"typedef struct { double \" + \"; double \".join(connection_components) + \"; } connection_struct;\"\n",
    "\n",
    "    other_structs = r\"\"\"\n",
    "typedef enum { Schwarzschild, Kerr, Numerical, Schwarzschild_Standard } Metric_t;\n",
    "typedef struct { Metric_t type; } metric_params;\n",
    "\n",
    "typedef struct { \n",
    "    const commondata_struct *commondata; \n",
    "    const params_struct *params; \n",
    "    const metric_params *metric; \n",
    "} gsl_params;\n",
    "\n",
    "// Struct for reading initial conditions from a debug file (unchanged)\n",
    "typedef struct {\n",
    "    int id;\n",
    "    double pos[4];\n",
    "    double u_spatial[3];\n",
    "} particle_initial_state_t;\n",
    "\n",
    "// DEFINITIVE struct for a single particle in the output snapshot files.\n",
    "// This is the format that will be written to disk.\n",
    "typedef struct {\n",
    "    int id;\n",
    "    double pos[3];          // x, y, z position\n",
    "    double u[4];    //u^t u^x, u^y, u^z \n",
    "    double lambda_rest;     // Rest-frame emission wavelength (nm)\n",
    "    float j_intrinsic;      // Rest-frame intrinsic emissivity (intensity)\n",
    "} __attribute__((packed)) mass_particle_state_t;\n",
    "\"\"\"\n",
    "    # This line needs to be updated to include the new struct string\n",
    "    Bdefines_h.register_BHaH_defines(\"data_structures\", f\"{metric_struct_str}\\n{connections_struct_str}\\n{other_structs}\")\n",
    "    print(\" -> Registered all necessary data structures, including mass_particle_state_t.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583e0f3",
   "metadata": {},
   "source": [
    "<a id='final_build'></a>\n",
    "### 8.b: Final Build Command (Redo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf390696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In V1_1_mass_geodesic.ipynb, Cell ID a0eb212d (Final Build Script)\n",
    "print(\"\\nAssembling and building C project for the massive particle integrator...\")\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "# --- Step 1: Register all C-generating functions in the correct order ---\n",
    "print(\" -> Registering C data structures and functions...\")\n",
    "register_custom_structures_and_params()\n",
    "\n",
    "# Register symbolic recipes and C-generating functions for physics\n",
    "# symbolic_ut_uphi_from_r() is called by the next function\n",
    "calculate_ut_uphi_from_r() # The new, required helper function\n",
    "\n",
    "# Register C workers for metrics and connections\n",
    "g4DD_kerr_schild(); con_kerr_schild()\n",
    "g4DD_schwarzschild_cartesian(); con_schwarzschild_cartesian()\n",
    "\n",
    "# Register C dispatchers\n",
    "g4DD_metric(); connections()\n",
    "\n",
    "# Register C engines for the core logic\n",
    "calculate_ode_rhs_massive()\n",
    "ode_gsl_wrapper_massive()\n",
    "set_initial_conditions_massive()\n",
    "check_conservation_massive()\n",
    "integrate_single_particle() \n",
    "integrate_single_particle_DEBUG()\n",
    "run_mass_integrator_production()\n",
    "\n",
    "# Register the production-run orchestrators\n",
    "generate_disk_initial_conditions()\n",
    "generate_spiral_galaxy_initial_conditions()\n",
    "generate_barred_flocculent_spiral_ic()\n",
    "main()\n",
    "#main_ic_validation()\n",
    "\n",
    "\n",
    "# --- Step 2: Call BHaH infrastructure functions to generate the build system ---\n",
    "print(\" -> Generating BHaH infrastructure files...\")\n",
    "# Generate set_CodeParameters.h and its variants\n",
    "CPs.write_CodeParameters_h_files(project_dir=project_dir)\n",
    "# Register C functions to set parameters to default values\n",
    "CPs.register_CFunctions_params_commondata_struct_set_to_default()\n",
    "# Generate the default parameter file (mass_integrator.par)\n",
    "cmdline_input_and_parfiles.generate_default_parfile(project_dir=project_dir, project_name=project_name)\n",
    "\n",
    "# Register the C function that parses the command line and parameter file\n",
    "cmdline_input_and_parfiles.register_CFunction_cmdline_input_and_parfile_parser(\n",
    "    project_name=project_name,\n",
    "    cmdline_inputs=[\n",
    "        'M_scale', 'a_spin', 't_max_integration', 'flatness_threshold', \n",
    "        'r_escape', 'ut_max', 'perform_conservation_check', 'run_in_debug_mode',\n",
    "        'initial_conditions_type',\n",
    "        'spiral_galaxy_num_arms',  \n",
    "        'spiral_galaxy_arm_tightness',\n",
    "        'bar_length', 'bar_aspect_ratio', 'bulge_radius',\n",
    "        'arm_particle_density', 'arm_clumpiness_factor', 'arm_clump_size',\n",
    "        'bar_density_factor', 'bulge_density_factor'\n",
    "]\n",
    ")\n",
    "\n",
    "# --- Step 3: Generate the final C code, headers, and Makefile ---\n",
    "print(\"\\nGenerating BHaH master header file (BHaH_defines.h)...\")\n",
    "Bdefines_h.output_BHaH_defines_h(project_dir=project_dir)\n",
    "\n",
    "# Note: SIMD intrinsics are not used in this project, but the helper is harmless.\n",
    "print(\"Copying required helper files...\")\n",
    "gh.copy_files(\n",
    "    package=\"nrpy.helpers\",\n",
    "    filenames_list=[\"simd_intrinsics.h\"],\n",
    "    project_dir=project_dir,\n",
    "    subdirectory=\"simd\",\n",
    ")\n",
    "\n",
    "print(\"Generating all C source files, function prototypes, and the Makefile...\")\n",
    "# Add required GSL and OpenMP flags to the compiler\n",
    "addl_CFLAGS = [\"-Wall -Wextra -g $(shell gsl-config --cflags) -fopenmp\"]\n",
    "addl_libraries = [\"$(shell gsl-config --libs) -fopenmp\"]\n",
    "\n",
    "Makefile.output_CFunctions_function_prototypes_and_construct_Makefile(\n",
    "    project_dir=project_dir,\n",
    "    project_name=project_name,\n",
    "    exec_or_library_name=\"mass_integrator\", # The name of our final executable\n",
    "    addl_CFLAGS=addl_CFLAGS,\n",
    "    addl_libraries=addl_libraries,\n",
    ")\n",
    "\n",
    "print(f\"\\nFinished! A C project has been generated in '{project_dir}/'\")\n",
    "print(f\"To build, navigate to this directory in your terminal and type 'make'.\")\n",
    "print(f\"To run, type './mass_integrator'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb5d18",
   "metadata": {},
   "source": [
    "# Making the kd trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da27f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "import numba # Make sure to import numba\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 1: Create the JIT-compiled \"kernel\" for the heavy lifting.\n",
    "# ==============================================================================\n",
    "# Numba works best with simple data types, so we pass NumPy arrays.\n",
    "# The @numba.jit decorator is the key. 'nopython=True' ensures it compiles to fast machine code.\n",
    "@numba.jit(nopython=True)\n",
    "def build_implicit_kdtree_kernel(particle_pos_array, num_particles):\n",
    "    \"\"\"\n",
    "    Numba-compiled kernel to perform the heavy lifting of building the implicit k-d tree.\n",
    "    This function contains only the numerical logic that Numba can optimize.\n",
    "    \n",
    "    Args:\n",
    "        particle_pos_array (np.ndarray): A NumPy array of shape (N, 3) with just particle positions.\n",
    "        num_particles (int): The number of particles.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (reordering_indices, node_metadata)\n",
    "               - reordering_indices: An array that maps the old index to the new, reordered index.\n",
    "               - node_metadata: The array of split axes for each node.\n",
    "    \"\"\"\n",
    "    # These will store the final results\n",
    "    reordering_indices = np.zeros(num_particles, dtype=np.int32)\n",
    "    node_metadata = np.full(num_particles, -1, dtype=np.int32)\n",
    "\n",
    "    # Numba doesn't have deque, but we can simulate a queue with a NumPy array and pointers.\n",
    "    # This is a common pattern for JIT-compiling queue-based algorithms.\n",
    "    # We need to pre-allocate a large enough queue. num_particles is a safe upper bound.\n",
    "    q_target_idx = np.zeros(num_particles, dtype=np.int32)\n",
    "    # This queue will store slices (start, end) into a temporary index array\n",
    "    q_slice_start = np.zeros(num_particles, dtype=np.int32)\n",
    "    q_slice_end = np.zeros(num_particles, dtype=np.int32)\n",
    "    \n",
    "    q_head = 0\n",
    "    q_tail = 0\n",
    "\n",
    "    # The array of original indices that we will sort and partition\n",
    "    indices_array = np.arange(num_particles, dtype=np.int32)\n",
    "    \n",
    "    if num_particles > 0:\n",
    "        # Enqueue the first item: target index 0, and the slice representing all particles\n",
    "        q_target_idx[q_tail] = 0\n",
    "        q_slice_start[q_tail] = 0\n",
    "        q_slice_end[q_tail] = num_particles\n",
    "        q_tail += 1\n",
    "\n",
    "    while q_head < q_tail:\n",
    "        # Dequeue an item\n",
    "        target_idx = q_target_idx[q_head]\n",
    "        start = q_slice_start[q_head]\n",
    "        end = q_slice_end[q_head]\n",
    "        q_head += 1\n",
    "\n",
    "        # --- Adaptive Splitting Logic ---\n",
    "        # Note: Slicing is faster inside Numba than fancy indexing\n",
    "        current_indices = indices_array[start:end]\n",
    "        current_positions = particle_pos_array[current_indices]\n",
    "        \n",
    "        min_x, min_y, min_z = current_positions[0]\n",
    "        max_x, max_y, max_z = current_positions[0]\n",
    "        for i in range(1, len(current_positions)):\n",
    "            pos = current_positions[i]\n",
    "            min_x, max_x = min(min_x, pos[0]), max(max_x, pos[0])\n",
    "            min_y, max_y = min(min_y, pos[1]), max(max_y, pos[1])\n",
    "            min_z, max_z = min(min_z, pos[2]), max(max_z, pos[2])\n",
    "\n",
    "        spread_x = max_x - min_x\n",
    "        spread_y = max_y - min_y\n",
    "        spread_z = max_z - min_z\n",
    "        \n",
    "        # Find split axis (0=x, 1=y, 2=z)\n",
    "        split_axis = 0\n",
    "        if spread_y > spread_x: split_axis = 1\n",
    "        if spread_z > spread_y and spread_z > spread_x: split_axis = 2\n",
    "\n",
    "        # --- Partitioning ---\n",
    "        # Sort the current slice of the main index array based on the split axis\n",
    "        # This is the most performance-critical part\n",
    "        sorted_indices_on_axis = current_indices[np.argsort(particle_pos_array[current_indices, split_axis])]\n",
    "        indices_array[start:end] = sorted_indices_on_axis\n",
    "        \n",
    "        median_offset = len(sorted_indices_on_axis) // 2\n",
    "        median_original_index = sorted_indices_on_axis[median_offset]\n",
    "\n",
    "        # --- Place the Pivot Particle's Metadata ---\n",
    "        reordering_indices[target_idx] = median_original_index\n",
    "        node_metadata[target_idx] = split_axis\n",
    "        \n",
    "        # --- Prepare for Next Level (Enqueue children) ---\n",
    "        left_child_idx = 2 * target_idx + 1\n",
    "        right_child_idx = 2 * target_idx + 2\n",
    "        \n",
    "        # Left child points to the slice before the median\n",
    "        if median_offset > 0:\n",
    "            if left_child_idx < num_particles:\n",
    "                q_target_idx[q_tail] = left_child_idx\n",
    "                q_slice_start[q_tail] = start\n",
    "                q_slice_end[q_tail] = start + median_offset\n",
    "                q_tail += 1\n",
    "        \n",
    "        # Right child points to the slice after the median\n",
    "        if median_offset + 1 < len(sorted_indices_on_axis):\n",
    "            if right_child_idx < num_particles:\n",
    "                q_target_idx[q_tail] = right_child_idx\n",
    "                q_slice_start[q_tail] = start + median_offset + 1\n",
    "                q_slice_end[q_tail] = end\n",
    "                q_tail += 1\n",
    "                \n",
    "    return reordering_indices, node_metadata\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: Create the main driver function that calls the kernel.\n",
    "# ==============================================================================\n",
    "def build_and_save_kdtree_snapshot_fast(raw_snapshot_file, output_dir):\n",
    "    \"\"\"\n",
    "    Main driver function that handles file I/O and calls the fast Numba kernel.\n",
    "    \"\"\"\n",
    "    raw_dtype = np.dtype([\n",
    "        ('id', np.int32), ('pos', 'f8', (3,)), ('u', 'f8', (4,)),\n",
    "        ('lambda_rest', 'f8'), ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "\n",
    "    # --- Step 1: Load the Raw Data (Fast I/O) ---\n",
    "    with open(raw_snapshot_file, 'rb') as f:\n",
    "        num_particles = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        if num_particles == 0:\n",
    "            print(\"  Snapshot is empty, skipping.\")\n",
    "            return\n",
    "        particle_data = np.fromfile(f, dtype=raw_dtype, count=num_particles)\n",
    "\n",
    "    # --- Step 2: Call the Fast JIT-Compiled Kernel ---\n",
    "    # We only pass the position data to the kernel for efficiency.\n",
    "    particle_positions = np.ascontiguousarray(particle_data['pos'])\n",
    "    reordering_map, node_metadata = build_implicit_kdtree_kernel(particle_positions, num_particles)\n",
    "    \n",
    "    # Use the returned map to reorder the full, original particle data array\n",
    "    reordered_particles = particle_data[reordering_map]\n",
    "\n",
    "    # --- Step 3: Save the Two Parallel Arrays (Fast I/O) ---\n",
    "    output_filename = os.path.join(output_dir, os.path.basename(raw_snapshot_file).replace('.bin', '.kdtree.bin'))\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        f.write(np.uint64(num_particles))\n",
    "        f.write(np.uint64(3)) # dimensions\n",
    "        f.write(node_metadata.tobytes())\n",
    "        f.write(reordered_particles.tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304c5c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ce1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm import tqdm # For a nice progress bar! (pip install tqdm)\n",
    "\n",
    "# --- Place the TWO functions from Part 1 above this ---\n",
    "# (build_implicit_kdtree_kernel and build_and_save_kdtree_snapshot_fast)\n",
    "\n",
    "def run_kdtree_preprocessor_parallel():\n",
    "    \"\"\"\n",
    "    Finds all raw snapshot .bin files and processes them in parallel\n",
    "    using a pool of worker processes.\n",
    "    \"\"\"\n",
    "    base_project_dir = \"project\"\n",
    "    mass_project_name = \"mass_integrator\"\n",
    "    input_folder = os.path.join(base_project_dir, mass_project_name, \"output\")\n",
    "    processed_folder = os.path.join(base_project_dir, \"processed_snapshots\")\n",
    "\n",
    "    print(f\"--- Starting K-d Tree Pre-processing (Parallel) ---\")\n",
    "    print(f\"Input directory:  '{input_folder}'\")\n",
    "    print(f\"Shared Output directory: '{processed_folder}'\")\n",
    "\n",
    "    os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(input_folder, \"mass_blueprint_t_*.bin\")))\n",
    "    \n",
    "    if not snapshot_files:\n",
    "        print(\"\\nWARNING: No raw snapshot files found. Did you run the mass_integrator C code first?\")\n",
    "        return\n",
    "\n",
    "    num_processes = cpu_count()\n",
    "    print(f\"\\nFound {len(snapshot_files)} files. Processing in parallel using {num_processes} CPU cores...\")\n",
    "\n",
    "    task_function = partial(build_and_save_kdtree_snapshot_fast, output_dir=processed_folder)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        list(tqdm(pool.imap_unordered(task_function, snapshot_files), total=len(snapshot_files)))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n--- All snapshots processed successfully in {end_time - start_time:.2f} seconds. ---\")\n",
    "    print(f\"The query-ready .kdtree.bin files are now in the shared directory: '{processed_folder}'.\")\n",
    "\n",
    "\n",
    "# --- How to Run ---\n",
    "# It's best practice to put the execution call inside this block\n",
    "if __name__ == '__main__':\n",
    "    # Make sure to call the parallel version!\n",
    "    run_kdtree_preprocessor_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def view_kdtree_snapshot(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    processed_folder: str = \"processed_snapshots\",\n",
    "    snapshot_index: int = 0, # 0 for the first, -1 for the last\n",
    "    max_nodes_to_print: int = 15\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a single, processed .kdtree.bin file and prints a detailed,\n",
    "    human-readable summary to verify its contents and structure.\n",
    "    \n",
    "    UPDATED to read and display the full 4-velocity u^mu.\n",
    "    \"\"\"\n",
    "    print(\"--- K-d Tree Blueprint Inspector ---\")\n",
    "    \n",
    "    # --- Step 1: Find and select the snapshot file ---\n",
    "    snapshot_dir = os.path.join(project_dir, processed_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Processed snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(snapshot_dir, \"*.kdtree.bin\")))\n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No .kdtree.bin files found in '{snapshot_dir}'\")\n",
    "        print(\"Please run the k-d tree pre-processor cell first.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        file_to_load = snapshot_files[snapshot_index]\n",
    "    except IndexError:\n",
    "        print(f\"ERROR: Snapshot index {snapshot_index} is out of bounds. Only {len(snapshot_files)} files exist.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Loading and inspecting file: '{os.path.basename(file_to_load)}'\")\n",
    "\n",
    "    # --- Step 2: Define the dtypes to read the file ---\n",
    "    # MODIFICATION: This dtype must exactly match the new particle data struct in C\n",
    "    particle_dtype = np.dtype([\n",
    "        ('id', np.int32), \n",
    "        ('pos', 'f8', (3,)), \n",
    "        ('u', 'f8', (4,)), # Changed from 'u_spatial' to 'u'\n",
    "        ('lambda_rest', 'f8'),\n",
    "        ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "    metadata_dtype = np.int32\n",
    "\n",
    "    # --- Step 3: Read the binary file according to the custom format ---\n",
    "    try:\n",
    "        with open(file_to_load, 'rb') as f:\n",
    "            num_particles = np.fromfile(f, dtype=np.uint64, count=1)[0]\n",
    "            dimensions = np.fromfile(f, dtype=np.uint64, count=1)[0]\n",
    "            node_metadata = np.fromfile(f, dtype=metadata_dtype, count=num_particles)\n",
    "            particle_data = np.fromfile(f, dtype=particle_dtype, count=num_particles)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Failed to read or parse the binary file. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Step 4: Print a summary and verify the contents ---\n",
    "    print(\"\\n--- File Header ---\")\n",
    "    print(f\"  Number of Particles: {num_particles}\")\n",
    "    print(f\"  Dimensions:          {dimensions}\")\n",
    "    \n",
    "    if len(node_metadata) != num_particles or len(particle_data) != num_particles:\n",
    "        print(\"\\nERROR: Mismatch between header particle count and data array lengths!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Data Verification ---\")\n",
    "    \n",
    "    axis_map = {0: 'X', 1: 'Y', 2: 'Z', -1: 'LEAF'}\n",
    "    \n",
    "    # MODIFICATION: Updated header to show the full 4-velocity\n",
    "    header = (f\"{'Index':<6} | {'Split Axis':<10} | {'Particle ID':<12} | \"\n",
    "              f\"{'Position (x, y, z)':<25} | {'4-Velocity (ut, ux, uy, uz)':<40}\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for i in range(min(num_particles, max_nodes_to_print)):\n",
    "        split_axis_val = node_metadata[i]\n",
    "        particle = particle_data[i]\n",
    "        \n",
    "        split_axis_str = axis_map.get(split_axis_val, 'INVALID')\n",
    "        pos_str = (f\"({particle['pos'][0]:>6.2f}, {particle['pos'][1]:>6.2f}, \"\n",
    "                   f\"{particle['pos'][2]:>6.2f})\")\n",
    "        \n",
    "        # MODIFICATION: Format the full 4-velocity for display\n",
    "        vel_str = (f\"({particle['u'][0]:>6.2f}, {particle['u'][1]:>6.2f}, \"\n",
    "                   f\"{particle['u'][2]:>6.2f}, {particle['u'][3]:>6.2f})\")\n",
    "        \n",
    "        print(f\"{i:<6} | {split_axis_str:<10} | {particle['id']:<12} | \"\n",
    "              f\"{pos_str:<25} | {vel_str:<40}\")\n",
    "\n",
    "    if num_particles > max_nodes_to_print:\n",
    "        print(\"...\")\n",
    "        print(f\"(... and {num_particles - max_nodes_to_print} more nodes)\")\n",
    "\n",
    "    print(\"\\n--- Verification Complete ---\")\n",
    "    print(\"Check that the 4-Velocity column contains four components and looks reasonable.\")\n",
    "view_kdtree_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89602521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_particle_trajectory(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads the trajectory data from the C code's output file and generates\n",
    "    a 3D plot of the particle's orbit around the black hole.\n",
    "\n",
    "    Args:\n",
    "        project_dir: The root directory of the C project where the output file is located.\n",
    "        input_filename: The name of the trajectory data file.\n",
    "        M_scale: The mass of the black hole, used to plot the event horizon.\n",
    "        a_spin: The spin of the black hole, used to plot the event horizon.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Particle Trajectory Plot ---\")\n",
    "    \n",
    "    # --- 1. Construct the full path and load the data ---\n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        print(\"Please ensure you have compiled and run the C code successfully.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the data, skipping the header row\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        # Columns: 0:τ, 1:t, 2:x, 3:y, 4:z, 5:u^t, 6:u^x, 7:u^y, 8:u^z\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "        print(f\"Successfully loaded {len(x_coords)} data points from trajectory file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{full_path}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Set up the 3D plot ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # --- 3. Plot the particle's trajectory ---\n",
    "    ax.plot(x_coords, y_coords, z_coords, label='Particle Orbit', color='cyan', lw=2)\n",
    "    \n",
    "    # Mark the start and end points\n",
    "    ax.scatter(x_coords[0], y_coords[0], z_coords[0], color='lime', s=100, label='Start', marker='o')\n",
    "    ax.scatter(x_coords[-1], y_coords[-1], z_coords[-1], color='red', s=100, label='End', marker='X')\n",
    "\n",
    "    # --- 4. Plot the black hole's event horizon ---\n",
    "    # The radius of the event horizon for a Kerr black hole\n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    \n",
    "    # Create a sphere for the event horizon\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    \n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    # Add a grey wireframe for better visibility\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    # --- 5. Customize the plot ---\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Set equal aspect ratio\n",
    "    max_range = np.array([x_coords.max()-x_coords.min(), y_coords.max()-y_coords.min(), z_coords.max()-z_coords.min()]).max() / 2.0\n",
    "    mid_x = (x_coords.max()+x_coords.min()) * 0.5\n",
    "    mid_y = (y_coords.max()+y_coords.min()) * 0.5\n",
    "    mid_z = (z_coords.max()+z_coords.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "    ax.set_title(f\"Massive Particle Trajectory (M={M_scale}, a={a_spin})\", fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.view_init(elev=30., azim=45) # Set a nice viewing angle\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_trajectory_components(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and creates four plots:\n",
    "    x vs t, y vs t, z vs t, and proper time (τ) vs t.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Trajectory Component Plots ---\")\n",
    "    \n",
    "    # --- 1. Load the data ---\n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        # Columns: 0:τ, 1:t, 2:x, 3:y, 4:z, ...\n",
    "        proper_time = data[:, 0]\n",
    "        coord_time = data[:, 1]\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "        print(f\"Successfully loaded {len(coord_time)} data points.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse data file '{full_path}'. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Create the plots ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Particle Trajectory Components vs. Coordinate Time', fontsize=20)\n",
    "\n",
    "    # Plot 1: x(t)\n",
    "    axes[0, 0].plot(coord_time, x_coords, color='cyan')\n",
    "    axes[0, 0].set_title('X Coordinate vs. Time', fontsize=14)\n",
    "    axes[0, 0].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('x [M]', fontsize=12)\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Plot 2: y(t)\n",
    "    axes[0, 1].plot(coord_time, y_coords, color='magenta')\n",
    "    axes[0, 1].set_title('Y Coordinate vs. Time', fontsize=14)\n",
    "    axes[0, 1].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('y [M]', fontsize=12)\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot 3: z(t)\n",
    "    axes[1, 0].plot(coord_time, z_coords, color='lime')\n",
    "    axes[1, 0].set_title('Z Coordinate vs. Time', fontsize=14)\n",
    "    axes[1, 0].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('z [M]', fontsize=12)\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot 4: τ(t)\n",
    "    axes[1, 1].plot(coord_time, proper_time, color='gold')\n",
    "    axes[1, 1].set_title('Proper Time vs. Coordinate Time', fontsize=14)\n",
    "    axes[1, 1].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Proper Time (τ) [M]', fontsize=12)\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_radius_vs_time(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and plots the particle's radial distance (r)\n",
    "    as a function of coordinate time (t) to validate circularity.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Radius vs. Time Validation Plot ---\")\n",
    "    \n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        coord_time = data[:, 1]\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the radius at each time step\n",
    "    radius = np.sqrt(x_coords**2 + y_coords**2 + z_coords**2)\n",
    "    \n",
    "    # Calculate statistics on the radius\n",
    "    mean_radius = np.mean(radius)\n",
    "    min_radius = np.min(radius)\n",
    "    max_radius = np.max(radius)\n",
    "    percent_variation = 100 * (max_radius - min_radius) / mean_radius\n",
    "\n",
    "    print(f\"Radius Statistics:\")\n",
    "    print(f\"  Mean Radius: {mean_radius:.6f} M\")\n",
    "    print(f\"  Min Radius:  {min_radius:.6f} M\")\n",
    "    print(f\"  Max Radius:  {max_radius:.6f} M\")\n",
    "    print(f\"  Total Variation: {percent_variation:.4f}%\")\n",
    "\n",
    "    # Create the plot\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(coord_time, radius, label='Particle Radius r(t)', color='cyan')\n",
    "    \n",
    "    # Add lines for mean, min, and max to visualize the variation\n",
    "    plt.axhline(mean_radius, color='lime', linestyle='--', label=f'Mean r = {mean_radius:.4f}')\n",
    "    \n",
    "    plt.title('Validation: Particle Radius vs. Coordinate Time', fontsize=16)\n",
    "    plt.xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    plt.ylabel('Radius (r) [M]', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Use a \"tight\" y-axis to emphasize any small variations\n",
    "    plt.ylim(min_radius * 0.999, max_radius * 1.001)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def plot_precession_validation(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9,\n",
    "    r_initial: float = 10.0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and validates the orbital precession rate against\n",
    "    the theoretical Lense-Thirring formula.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Precession Validation Plot ---\")\n",
    "    \n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        coord_time = data[:, 1]\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the azimuthal angle phi at each time step\n",
    "    phi = np.arctan2(y_coords, x_coords)\n",
    "    \n",
    "    # The angle will wrap around from +pi to -pi. We need to unwrap it.\n",
    "    phi_unwrapped = np.unwrap(phi)\n",
    "\n",
    "    # --- Theoretical Calculation ---\n",
    "    r = r_initial\n",
    "    Omega_K = (M_scale**0.5) / (r**1.5 + a_spin * M_scale**0.5)\n",
    "    Omega_LT = (2 * M_scale * a_spin) / (r**3)\n",
    "    Omega_phi_theory = Omega_K + Omega_LT\n",
    "    \n",
    "    # --- Measurement from Simulation Data ---\n",
    "    # Perform a linear regression to find the slope of phi(t)\n",
    "    regression = linregress(coord_time, phi_unwrapped)\n",
    "    Omega_phi_measured = regression.slope\n",
    "    \n",
    "    percent_error = 100 * abs(Omega_phi_measured - Omega_phi_theory) / Omega_phi_theory\n",
    "\n",
    "    print(\"Precession Rate (dφ/dt) Validation:\")\n",
    "    print(f\"  Theoretical Ω_φ: {Omega_phi_theory:.6f} rad/M\")\n",
    "    print(f\"  Measured Ω_φ (from data): {Omega_phi_measured:.6f} rad/M\")\n",
    "    print(f\"  Relative Error: {percent_error:.4f}%\")\n",
    "\n",
    "    # --- Create the Plot ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(coord_time, phi_unwrapped, label='Measured φ(t) from Simulation', color='cyan', lw=2)\n",
    "    plt.plot(coord_time, Omega_phi_theory * coord_time, label=f'Theoretical φ(t) (slope={Omega_phi_theory:.4f})', color='lime', linestyle='--', lw=2)\n",
    "    \n",
    "    plt.title('Validation: Orbital Precession (Frame-Dragging)', fontsize=16)\n",
    "    plt.xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    plt.ylabel('Azimuthal Angle (φ) [radians]', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216be7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def visualize_disk_snapshot(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    output_folder: str = \"output\",\n",
    "    snapshot_index: int = -1, # -1 means the last available snapshot\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a specific mass blueprint snapshot file, performs sanity checks,\n",
    "    and generates a 3D plot of the particle disk.\n",
    "    \"\"\"\n",
    "    print(\"--- Visualizing Mass Blueprint Snapshot ---\")\n",
    "    \n",
    "    # --- 1. Find and Load the Snapshot File ---\n",
    "    snapshot_dir = os.path.join(project_dir, output_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    # Find all blueprint files and sort them\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(snapshot_dir, \"mass_blueprint_t_*.bin\")))\n",
    "    \n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No snapshot .bin files found in '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    if snapshot_index == -1:\n",
    "        # Select the last file\n",
    "        snapshot_to_load = snapshot_files[-1]\n",
    "    elif snapshot_index < len(snapshot_files):\n",
    "        snapshot_to_load = snapshot_files[snapshot_index]\n",
    "    else:\n",
    "        print(f\"ERROR: Snapshot index {snapshot_index} is out of bounds. Only {len(snapshot_files)} snapshots exist.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading snapshot file: '{snapshot_to_load}'\")\n",
    "\n",
    "    # Define the data type for a single record in the binary file\n",
    "    # Format: int (id), double (x), double (y), double (z), double (ux), double (uy), double (uz)\n",
    "    snapshot_dtype = np.dtype([\n",
    "        ('id', np.int32),\n",
    "        ('pos', 'f8', (3,)), # 3 doubles for position\n",
    "        ('u_spatial', 'f8', (3,)) # 3 doubles for spatial 4-velocity\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        data = np.fromfile(snapshot_to_load, dtype=snapshot_dtype)\n",
    "        num_particles = len(data)\n",
    "        print(f\"Successfully loaded data for {num_particles} particles.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{snapshot_to_load}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Perform Sanity Checks ---\n",
    "    positions = data['pos']\n",
    "    velocities = data['u_spatial']\n",
    "    \n",
    "    radii = np.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n",
    "    speeds = np.sqrt(velocities[:, 0]**2 + velocities[:, 1]**2 + velocities[:, 2]**2)\n",
    "\n",
    "    print(\"\\n--- Data Sanity Checks ---\")\n",
    "    print(f\"  Particle count: {num_particles}\")\n",
    "    print(f\"  Mean radius: {np.mean(radii):.3f} M (should be between disk_r_min and disk_r_max)\")\n",
    "    print(f\"  Mean spatial 4-velocity magnitude: {np.mean(speeds):.3f}\")\n",
    "    print(f\"  Max z-coordinate: {np.max(np.abs(positions[:, 2])):.2e} (should be close to zero)\")\n",
    "    \n",
    "    if np.any(np.isnan(positions)) or np.any(np.isnan(velocities)):\n",
    "        nan_count = np.count_nonzero(np.isnan(data['pos'][:,0]))\n",
    "        print(f\"  WARNING: Found {nan_count} terminated (NaN) particles in this snapshot.\")\n",
    "    \n",
    "    # --- 3. Create the 3D Plot ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot a subset of particles to avoid cluttering the plot\n",
    "    num_to_plot = min(num_particles, 2000)\n",
    "    plot_indices = np.random.choice(num_particles, num_to_plot, replace=False)\n",
    "    \n",
    "    # Use color to represent the radial position of the particles\n",
    "    colors = radii[plot_indices]\n",
    "    sc = ax.scatter(positions[plot_indices, 0], positions[plot_indices, 1], positions[plot_indices, 2], \n",
    "                    c=colors, cmap='plasma', s=5, label='Disk Particles')\n",
    "    \n",
    "    # --- Plot the black hole's event horizon ---\n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    # --- Customize the plot ---\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    max_radius_plot = np.max(radii) * 1.1\n",
    "    ax.set_xlim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_ylim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_zlim(-max_radius_plot/2, max_radius_plot/2) # Exaggerate z-axis if needed\n",
    "\n",
    "    ax.set_title(f\"Accretion Disk Snapshot from {os.path.basename(snapshot_to_load)}\", fontsize=16)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.6, aspect=10, label='Particle Radius (M)')\n",
    "    ax.view_init(elev=45., azim=45)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def visualize_disk_snapshot(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    output_folder: str = \"output\",\n",
    "    snapshot_index: int = -1, # -1 means the last available snapshot\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a specific mass blueprint snapshot file, performs sanity checks,\n",
    "    and generates a 3D plot of the particle disk.\n",
    "    \n",
    "    UPDATED to read the new binary format with the full 4-velocity.\n",
    "    \"\"\"\n",
    "    print(\"--- Visualizing Mass Blueprint Snapshot ---\")\n",
    "    \n",
    "    # --- 1. Find and Load the Snapshot File ---\n",
    "    snapshot_dir = os.path.join(project_dir, output_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(snapshot_dir, \"mass_blueprint_t_*.bin\")))\n",
    "    \n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No snapshot .bin files found in '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    if snapshot_index == -1:\n",
    "        snapshot_to_load = snapshot_files[-1]\n",
    "    elif snapshot_index < len(snapshot_files):\n",
    "        snapshot_to_load = snapshot_files[snapshot_index]\n",
    "    else:\n",
    "        print(f\"ERROR: Snapshot index {snapshot_index} is out of bounds. Only {len(snapshot_files)} snapshots exist.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading snapshot file: '{snapshot_to_load}'\")\n",
    "\n",
    "    # MODIFICATION: Define the dtype to match the new C struct with u[4].\n",
    "    snapshot_dtype = np.dtype([\n",
    "        ('id', np.int32),\n",
    "        ('pos', 'f8', (3,)), \n",
    "        ('u', 'f8', (4,)),   # Changed from ('u_spatial', 'f8', (3,))\n",
    "        ('lambda_rest', 'f8'),\n",
    "        ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # The header is a 4-byte int, not part of the dtype\n",
    "        with open(snapshot_to_load, 'rb') as f:\n",
    "            num_particles = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "            data = np.fromfile(f, dtype=snapshot_dtype, count=num_particles)\n",
    "        print(f\"Successfully loaded data for {num_particles} particles.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{snapshot_to_load}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Perform Sanity Checks ---\n",
    "    positions = data['pos']\n",
    "    # MODIFICATION: Use the new 'u' field for velocities.\n",
    "    velocities = data['u'] \n",
    "    \n",
    "    radii = np.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n",
    "    # Calculate speed from the SPATIAL components of the 4-velocity\n",
    "    speeds = np.sqrt(velocities[:, 1]**2 + velocities[:, 2]**2 + velocities[:, 3]**2)\n",
    "\n",
    "    print(\"\\n--- Data Sanity Checks ---\")\n",
    "    print(f\"  Particle count: {num_particles}\")\n",
    "    print(f\"  Mean radius: {np.mean(radii):.3f} M (should be between disk_r_min and disk_r_max)\")\n",
    "    print(f\"  Mean spatial 4-velocity magnitude: {np.mean(speeds):.3f}\")\n",
    "    print(f\"  Max z-coordinate: {np.max(np.abs(positions[:, 2])):.2e} (should be close to zero)\")\n",
    "    \n",
    "    if np.any(np.isnan(positions)):\n",
    "        nan_count = np.count_nonzero(np.isnan(data['pos'][:,0]))\n",
    "        print(f\"  WARNING: Found {nan_count} terminated (NaN) particles in this snapshot.\")\n",
    "    \n",
    "    # --- 3. Create the 3D Plot (No changes needed here) ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    num_to_plot = min(num_particles, 2000)\n",
    "    plot_indices = np.random.choice(num_particles, num_to_plot, replace=False)\n",
    "    \n",
    "    colors = radii[plot_indices]\n",
    "    sc = ax.scatter(positions[plot_indices, 0], positions[plot_indices, 1], positions[plot_indices, 2], \n",
    "                    c=colors, cmap='plasma', s=5, label='Disk Particles')\n",
    "    \n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    max_radius_plot = np.max(radii) * 1.1\n",
    "    ax.set_xlim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_ylim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_zlim(-max_radius_plot/2, max_radius_plot/2)\n",
    "\n",
    "    ax.set_title(f\"Accretion Disk Snapshot from {os.path.basename(snapshot_to_load)}\", fontsize=16)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.6, aspect=10, label='Particle Radius (M)')\n",
    "    ax.view_init(elev=45., azim=45)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_apsidal_precession_validation(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and validates the apsidal precession rate against\n",
    "    the theoretical GR formula for nearly circular orbits.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Apsidal Precession Validation Plot ---\")\n",
    "    \n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    radius = np.sqrt(x_coords**2 + y_coords**2)\n",
    "    phi = np.unwrap(np.arctan2(y_coords, x_coords))\n",
    "    \n",
    "    # Find the angles where the particle is at periapsis (minimum radius)\n",
    "    # We find indices where the radius is a local minimum\n",
    "    periapsis_indices = (np.r_[True, radius[1:] < radius[:-1]] & np.r_[radius[:-1] < radius[1:], True]).nonzero()[0]\n",
    "    \n",
    "    if len(periapsis_indices) < 2:\n",
    "        print(\"Could not find at least two periapsis points. Cannot calculate precession.\")\n",
    "        return\n",
    "        \n",
    "    # Calculate the measured precession angle per orbit\n",
    "    delta_phi_measured = phi[periapsis_indices[1]] - phi[periapsis_indices[0]]\n",
    "    precession_per_orbit_measured = delta_phi_measured - 2 * np.pi\n",
    "\n",
    "    # --- Theoretical Calculation at the average radius of the orbit ---\n",
    "    r_avg = np.mean(radius)\n",
    "    M = M_scale\n",
    "    a = a_spin\n",
    "    \n",
    "    Omega_phi_theory = (M**0.5) / (r_avg**1.5 + a * M**0.5)\n",
    "    Omega_r_theory_sq = Omega_phi_theory**2 * (1 - (6*M)/r_avg + (8*a*M**0.5)/r_avg**1.5 - (3*a**2)/r_avg**2)\n",
    "    \n",
    "    if Omega_r_theory_sq < 0:\n",
    "        print(\"Theoretical orbit is unstable (Ω_r^2 < 0). Cannot calculate precession.\")\n",
    "        return\n",
    "        \n",
    "    Omega_r_theory = np.sqrt(Omega_r_theory_sq)\n",
    "    \n",
    "    # Precession per unit time\n",
    "    Omega_precession_theory = Omega_phi_theory - Omega_r_theory\n",
    "    \n",
    "    # Period of one radial oscillation\n",
    "    T_r = 2 * np.pi / Omega_r_theory\n",
    "    \n",
    "    # Total precession angle over one radial period\n",
    "    precession_per_orbit_theory = Omega_precession_theory * T_r\n",
    "    \n",
    "    percent_error = 100 * abs(precession_per_orbit_measured - precession_per_orbit_theory) / precession_per_orbit_theory\n",
    "\n",
    "    print(f\"Apsidal Precession Validation (at average radius r={r_avg:.3f} M):\")\n",
    "    print(f\"  Measured precession per orbit:   {precession_per_orbit_measured:.6f} radians\")\n",
    "    print(f\"  Theoretical precession per orbit: {precession_per_orbit_theory:.6f} radians\")\n",
    "    print(f\"  Relative Error: {percent_error:.4f}%\")\n",
    "\n",
    "    # --- Create the Plot (Polar Plot) ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    ax.plot(phi, radius, color='cyan', label='Particle Orbit')\n",
    "    ax.scatter(phi[periapsis_indices], radius[periapsis_indices], color='lime', s=100, label='Periapsis Points', zorder=5)\n",
    "    \n",
    "    ax.set_title('Validation: Apsidal Precession of a Nearly Circular Orbit', fontsize=16)\n",
    "    ax.set_xlabel('Azimuthal Angle (φ)', fontsize=12)\n",
    "    ax.set_ylabel('Radius (r) [M]', fontsize=12, labelpad=-50)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_particle_trajectory(\n",
    "#    project_dir=\"project/mass_integrator\",\n",
    "#    input_filename=\"massive_particle_path.txt\",\n",
    "#    M_scale=1.0,\n",
    "#    a_spin=0.9\n",
    "#)\n",
    "# After running the C code, call this function.\n",
    "plot_trajectory_components()\n",
    "# Call this function after running your C code.\n",
    "plot_radius_vs_time()\n",
    "# --- How to run ---\n",
    "plot_precession_validation()\n",
    "# --- How to run ---\n",
    "plot_apsidal_precession_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cbc74",
   "metadata": {},
   "source": [
    "# Video of disk movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import nrpy.params as par\n",
    "\n",
    "# This is the corrected helper function.\n",
    "def _plot_single_frame(\n",
    "    positions: np.ndarray,\n",
    "    radii: np.ndarray,\n",
    "    M_scale: float,\n",
    "    a_spin: float,\n",
    "    current_time: float,\n",
    "    output_filename: str,\n",
    "    fig_width_inches: float,\n",
    "    fig_dpi: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots a single frame of the disk animation and saves it to a file,\n",
    "    guaranteeing the output image has even dimensions for video encoding.\n",
    "    \"\"\"\n",
    "    # Calculate the desired pixel dimensions\n",
    "    desired_width_px = fig_width_inches * fig_dpi\n",
    "    \n",
    "    # Get the aspect ratio from the data to calculate height\n",
    "    x_range = np.max(positions[:, 0]) - np.min(positions[:, 0])\n",
    "    y_range = np.max(positions[:, 1]) - np.min(positions[:, 1])\n",
    "    aspect_ratio = y_range / x_range if x_range > 0 else 1.0\n",
    "    desired_height_px = desired_width_px * aspect_ratio\n",
    "\n",
    "    # Round down to the nearest even number\n",
    "    final_width_px = int(desired_width_px // 2 * 2)\n",
    "    final_height_px = int(desired_height_px // 2 * 2)\n",
    "    \n",
    "    # Recalculate figure size in inches to match the final pixel dimensions\n",
    "    final_fig_width_inches = final_width_px / fig_dpi\n",
    "    final_fig_height_inches = final_height_px / fig_dpi\n",
    "    \n",
    "    fig = plt.figure(figsize=(final_fig_width_inches, final_fig_height_inches))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    num_particles = len(positions)\n",
    "    num_to_plot = min(num_particles, 5000)\n",
    "    plot_indices = np.random.choice(num_particles, num_to_plot, replace=False)\n",
    "    \n",
    "    colors = radii[plot_indices]\n",
    "    sc = ax.scatter(positions[plot_indices, 0], positions[plot_indices, 1], positions[plot_indices, 2], \n",
    "                    c=colors, cmap='plasma', s=5)\n",
    "    \n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    max_radius_plot = np.max(radii) * 1.1\n",
    "    ax.set_xlim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_ylim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_zlim(-max_radius_plot/2, max_radius_plot/2)\n",
    "\n",
    "    ax.set_title(f\"Accretion Disk Snapshot at t = {current_time:.2f} M\", fontsize=16)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.6, aspect=10, label='Particle Radius (M)')\n",
    "    ax.view_init(elev=45., azim=45)\n",
    "    \n",
    "    # --- THE CRITICAL FIX IS HERE ---\n",
    "    # Adjust layout first, then save without the problematic argument.\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename, dpi=fig_dpi) # REMOVED: bbox_inches='tight'\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "# This is the main function that calls the helper above. It does not need changes,\n",
    "# but is included for completeness.\n",
    "def generate_animation_frames(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    output_folder: str = \"output_GrandDesign_ISCO\",\n",
    "    frames_output_dir: str = \"animation_frames\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.95,\n",
    "    fig_width_inches: float = 12.0,\n",
    "    fig_dpi: int = 150,\n",
    "    overwrite_existing_frames: bool = False\n",
    ") -> None:\n",
    "    # ... (This function's body is unchanged from the previous answer) ...\n",
    "    print(\"--- Starting Animation Frame Generation ---\")\n",
    "    plt.style.use('dark_background')\n",
    "    \n",
    "    snapshot_dir = os.path.join(project_dir, output_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files = glob.glob(os.path.join(snapshot_dir, \"mass_blueprint_t_*.bin\"))\n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No snapshot .bin files found in '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files.sort(key=lambda f: int(os.path.basename(f).split('_t_')[1].split('.bin')[0]))\n",
    "    print(f\"Found {len(snapshot_files)} snapshots to process.\")\n",
    "\n",
    "    full_frames_dir = os.path.join(project_dir, frames_output_dir)\n",
    "    os.makedirs(full_frames_dir, exist_ok=True)\n",
    "\n",
    "    if not overwrite_existing_frames:\n",
    "        print(\"Overwrite is OFF. Will skip any frames that already exist.\")\n",
    "\n",
    "    snapshot_dtype = np.dtype([\n",
    "        ('id', np.int32), ('pos', 'f8', (3,)), ('u', 'f8', (4,)),\n",
    "        ('lambda_rest', 'f8'), ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        snapshot_every_t = par.parval_from_str(\"snapshot_every_t\")\n",
    "    except ValueError:\n",
    "        snapshot_every_t = 1.0\n",
    "\n",
    "    for i, snapshot_file in enumerate(snapshot_files):\n",
    "        frame_filename = os.path.join(full_frames_dir, f\"frame_{i:04d}.png\")\n",
    "        \n",
    "        if not overwrite_existing_frames and os.path.exists(frame_filename):\n",
    "            print(f\"  Skipping frame {i+1}/{len(snapshot_files)}: {os.path.basename(frame_filename)} already exists.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Processing frame {i+1}/{len(snapshot_files)}: {os.path.basename(snapshot_file)}\")\n",
    "        \n",
    "        with open(snapshot_file, 'rb') as f:\n",
    "            num_particles = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "            if num_particles == 0: continue\n",
    "            data = np.fromfile(f, dtype=snapshot_dtype, count=num_particles)\n",
    "        \n",
    "        positions = data['pos']\n",
    "        radii = np.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n",
    "        \n",
    "        snapshot_number = int(os.path.basename(snapshot_file).split('_t_')[1].split('.bin')[0])\n",
    "        current_time = snapshot_number * snapshot_every_t\n",
    "        \n",
    "        _plot_single_frame(\n",
    "            positions, radii, M_scale, a_spin, current_time, frame_filename,\n",
    "            fig_width_inches=fig_width_inches, fig_dpi=fig_dpi\n",
    "        )\n",
    "    \n",
    "    print(\"\\n--- Frame generation complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def encode_video_from_frames(\n",
    "    image_folder: str,\n",
    "    output_video_path: str,\n",
    "    frame_rate: int = 30,\n",
    "    crf: int = 18\n",
    ") -> None:\n",
    "    # ... (This function is unchanged) ...\n",
    "    print(f\"--- Starting Video Encoding ---\")\n",
    "    output_dir = os.path.dirname(output_video_path)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    command = [\n",
    "        'ffmpeg', '-y', '-framerate', str(frame_rate),\n",
    "        '-i', os.path.join(image_folder, 'frame_%04d.png'),\n",
    "        '-c:v', 'libx264', '-pix_fmt', 'yuv420p',\n",
    "        '-r', str(frame_rate), '-crf', str(crf),\n",
    "        output_video_path\n",
    "    ]\n",
    "    print(f\"Running FFmpeg command:\\n{' '.join(command)}\")\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        print(f\"\\n[✓] Video encoding successful. File saved to '{output_video_path}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n[!] ERROR: FFmpeg not found.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n[!] ERROR: FFmpeg failed with exit code {e.returncode}.\")\n",
    "        print(\"\\n--- FFmpeg stderr ---\")\n",
    "        print(e.stderr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HOW TO RUN THE DEFINITIVE FIX ---\n",
    "\n",
    "# 1. Re-generate the frames. This is necessary because the files on disk are still wrong.\n",
    "#    This will replace the 1371px-wide images with new, even-dimensioned ones.\n",
    "print(\"STEP 1: Re-generating all frames with corrected dimensions...\")\n",
    "generate_animation_frames(\n",
    "    project_dir=\"/home/daltonm/Documents/project/mass_integrator\",\n",
    "    output_folder=\"output_GrandDesign_ISCO\",\n",
    "    frames_output_dir=\"animation_frames_grand_design\",\n",
    "    M_scale=1.0,\n",
    "    a_spin=0.95,\n",
    "    overwrite_existing_frames=True\n",
    ")\n",
    "print(\"STEP 1 COMPLETE.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode the video from the newly created, valid frames.\n",
    "print(\"STEP 2: Encoding the new frames into a video...\")\n",
    "encode_video_from_frames(\n",
    "    image_folder=\"/home/daltonm/Documents/project/mass_integrator/animation_frames_grand_design\",\n",
    "    output_video_path=\"/home/daltonm/Documents/project/mass_integrator/grand_design_disk_evolution.mp4\",\n",
    "    frame_rate=15\n",
    ")\n",
    "print(\"STEP 2 COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c83c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Run This Test ---\n",
    "# 1. Run your C code in PRODUCTION mode (set run_in_debug_mode = false in the .par file).\n",
    "# 2. This will create an 'output' folder with several .bin files.\n",
    "# 3. Call this function. It will automatically find and plot the LAST snapshot.\n",
    "output_folder=\"output_GrandDesign_ISCO\"\n",
    "visualize_disk_snapshot(output_folder=output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a34f44",
   "metadata": {},
   "source": [
    "# Circular Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63539fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V1_5_mass_geodesic (Copy).ipynb\n",
    "# In cell [63539fd1] (DEFINITIVE STABLE VERSION)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_analytical_solution(tau_values, r_initial, M_scale, a_spin):\n",
    "    \"\"\"\n",
    "    Calculates the analytical trajectory (t,x,y,z) for a circular, equatorial\n",
    "    orbit in Kerr spacetime using a NUMERICALLY STABLE method.\n",
    "\n",
    "    This version implements the three-step process that avoids catastrophic\n",
    "    cancellation for orbits near the ISCO.\n",
    "\n",
    "    Args:\n",
    "        tau_values (np.ndarray): Array of proper time values.\n",
    "        r_initial (float): The constant radius of the orbit.\n",
    "        M_scale (float): The mass of the black hole.\n",
    "        a_spin (float): The spin parameter of the black hole.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U_t, Omega_tau, analytical_positions)\n",
    "    \"\"\"\n",
    "    # Use shorter variable names for clarity\n",
    "    r = r_initial\n",
    "    M = M_scale\n",
    "    a = a_spin\n",
    "    \n",
    "    # --- Step 1: Calculate the stable angular velocity Omega = d(phi)/dt ---\n",
    "    sqrt_M = np.sqrt(M)\n",
    "    Omega = sqrt_M / (r**1.5 + a * sqrt_M)\n",
    "    \n",
    "    # --- Step 2: Calculate the required metric components in Boyer-Lindquist ---\n",
    "    # These are specialized for the equatorial plane (theta=pi/2)\n",
    "    g_tt = -(1 - 2*M/r)\n",
    "    g_tphi = -2*a*M/r\n",
    "    g_phiphi = r**2 + a**2 + (2*M*a**2)/r\n",
    "    \n",
    "    # --- Step 3: Solve for u^t and u^phi using the stable formulas ---\n",
    "    ut_squared_inv_denom = g_tt + 2*g_tphi*Omega + g_phiphi*Omega**2\n",
    "    \n",
    "    # Check for instability. If the denominator is non-negative, the orbit is not possible.\n",
    "    if ut_squared_inv_denom >= 0:\n",
    "        # Return NaNs to signal that this orbit is unstable/invalid.\n",
    "        nan_array = np.full((len(tau_values), 4), np.nan)\n",
    "        return np.nan, np.nan, nan_array\n",
    "\n",
    "    ut_squared = -1.0 / ut_squared_inv_denom\n",
    "    U_t = np.sqrt(ut_squared)\n",
    "    \n",
    "    u_phi = Omega * U_t\n",
    "    Omega_tau = u_phi # d(phi)/d(tau) is u^phi\n",
    "\n",
    "    # --- Step 4: Calculate the trajectory over the given proper time values ---\n",
    "    phi_of_tau = Omega_tau * tau_values\n",
    "    t_of_tau = U_t * tau_values\n",
    "    x_of_tau = r_initial * np.cos(phi_of_tau)\n",
    "    y_of_tau = r_initial * np.sin(phi_of_tau)\n",
    "    z_of_tau = np.zeros_like(tau_values)\n",
    "    \n",
    "    analytical_positions = np.vstack([t_of_tau, x_of_tau, y_of_tau, z_of_tau]).T\n",
    "    \n",
    "    return U_t, Omega_tau, analytical_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In file: V1_5_mass_geodesic (Copy).ipynb\n",
    "# In cell [43df2a3a] (DEFINITIVE, ROBUST VERSION)\n",
    "\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "def verify_mass_integrator(\n",
    "    test_cases: List[Tuple[float, float]],\n",
    "    M_scale: float = 1.0,\n",
    "    tau_max: float = 2000.0,\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    executable_name: str = \"mass_integrator\",\n",
    "    output_folder: str = \"verification_plots\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Runs a verification suite for the massive particle integrator for specific test cases.\n",
    "    \n",
    "    VERSION 3: Includes robust NaN checking to correctly handle and report unstable orbits.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Mass Integrator Verification Suite ---\")\n",
    "    \n",
    "    full_output_folder = os.path.join(project_dir, output_folder)\n",
    "    os.makedirs(full_output_folder, exist_ok=True)\n",
    "    print(f\"Plots will be saved to: {full_output_folder}\")\n",
    "    \n",
    "    for a_spin, r_initial in test_cases:\n",
    "        print(f\"\\n--- Verifying r_initial={r_initial:.2f}, a_spin={a_spin:.2f} ---\")\n",
    "\n",
    "        # --- 1. Generate Initial Conditions ---\n",
    "        U_t_initial, Omega_tau_initial, _ = get_analytical_solution(np.array([0.0]), r_initial, M_scale, a_spin)\n",
    "        \n",
    "        # *** ROBUSTNESS CHECK 1: Check if analytical solution itself is unstable ***\n",
    "        if np.isnan(U_t_initial):\n",
    "            print(\"  [✗] ANALYTICAL FAILURE: The chosen r_initial is inside the ISCO or numerically unstable.\")\n",
    "            print(\"      Skipping this test case.\")\n",
    "            continue\n",
    "        # *** END OF CHECK ***\n",
    "\n",
    "        u_x_initial = 0.0\n",
    "        u_y_initial = r_initial * Omega_tau_initial\n",
    "        u_z_initial = 0.0\n",
    "        \n",
    "        ic_filename = os.path.join(project_dir, \"particle_debug_initial_conditions.txt\")\n",
    "        with open(ic_filename, \"w\") as f:\n",
    "            f.write(\"# Format: t_initial pos_x pos_y pos_z u_x u_y u_z\\n\")\n",
    "            f.write(f\"0.0 {r_initial:.10f} 0.0 0.0   {u_x_initial:.10f} {u_y_initial:.10f} {u_z_initial:.10f}\\n\")\n",
    "        \n",
    "        # --- 2. Run the C Integrator ---\n",
    "        par_filename = os.path.join(project_dir, \"mass_integrator.par\")\n",
    "        with open(par_filename, \"w\") as f:\n",
    "            f.write(f\"run_in_debug_mode = True\\n\")\n",
    "            f.write(f\"a_spin = {a_spin:.10f}\\n\")\n",
    "            f.write(f\"M_scale = {M_scale:.10f}\\n\")\n",
    "            f.write(f\"t_max_integration = {tau_max * 2 * U_t_initial}\\n\")\n",
    "            f.write(f\"metric_choice = 0\\n\")\n",
    "\n",
    "        output_path = os.path.join(project_dir, \"massive_particle_path.txt\")\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                f\"./{executable_name}\", shell=True, capture_output=True, text=True, check=True, cwd=project_dir\n",
    "            )\n",
    "            print(\"  [✓] C Integrator ran successfully.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  [✗] ERROR: C Integrator failed for r={r_initial}, a={a_spin}.\")\n",
    "            print(e.stderr)\n",
    "            continue\n",
    "\n",
    "        # --- 3. Load Numerical and Generate Analytical Results ---\n",
    "        try:\n",
    "            numerical_data = np.loadtxt(output_path, skiprows=1)\n",
    "            if numerical_data.size == 0:\n",
    "                print(\"  [✗] NUMERICAL FAILURE: Output file is empty.\")\n",
    "                continue\n",
    "            numerical_data = numerical_data[numerical_data[:, 0] <= tau_max]\n",
    "            print(f\"  [✓] Loaded {len(numerical_data)} numerical data points.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [✗] ERROR: Could not load or parse output file '{output_path}'. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # *** ROBUSTNESS CHECK 2: Check if numerical integrator produced NaNs ***\n",
    "        if np.any(np.isnan(numerical_data)):\n",
    "            print(\"  [✗] NUMERICAL FAILURE: The C integrator produced NaN values.\")\n",
    "            print(\"      This indicates the orbit was unstable as predicted. Skipping statistics.\")\n",
    "            continue\n",
    "        # *** END OF CHECK ***\n",
    "\n",
    "        tau_values = numerical_data[:, 0]\n",
    "        _, _, analytical_data = get_analytical_solution(tau_values, r_initial, M_scale, a_spin)\n",
    "        print(\"  [✓] Generated analytical ground truth.\")\n",
    "\n",
    "        # --- 4. Calculate Statistics ---\n",
    "        pos_error = np.sqrt(np.sum((numerical_data[:, 2:5] - analytical_data[:, 1:4])**2, axis=1))\n",
    "        radius_error = np.abs(np.sqrt(numerical_data[:, 2]**2 + numerical_data[:, 3]**2) - r_initial)\n",
    "        \n",
    "        phi_num = np.unwrap(np.arctan2(numerical_data[:, 3], numerical_data[:, 2]))\n",
    "        phi_ana = np.unwrap(np.arctan2(analytical_data[:, 2], analytical_data[:, 1]))\n",
    "        phase_error = np.abs(phi_num - phi_ana)\n",
    "\n",
    "        print(\"\\n  --- STATISTICAL REPORT ---\")\n",
    "        print(f\"  Final Position Error:      {pos_error[-1]:.3e} M\")\n",
    "        print(f\"  Max Radius Error (Drift):  {np.max(radius_error):.3e} M\")\n",
    "        print(f\"  Mean Radius Error:         {np.mean(radius_error):.3e} M\")\n",
    "        print(f\"  Max Phase Error (Timing):  {np.max(phase_error):.3e} radians\")\n",
    "        print(f\"  Mean Phase Error:          {np.mean(phase_error):.3e} radians\")\n",
    "        \n",
    "        # *** ROBUSTNESS CHECK 3: Corrected verdict logic ***\n",
    "        if np.max(radius_error) > 1e-5 or np.max(phase_error) > 1e-5:\n",
    "             print(\"\\n  --- VERDICT ---\\n  FAIL: Error metrics exceed tolerance.\")\n",
    "        else:\n",
    "             print(\"\\n  --- VERDICT ---\\n  PASS: All error metrics are within tolerance.\")\n",
    "        # *** END OF CHECK ***\n",
    "\n",
    "        # --- 5. Generate and Save Plots ---\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        fig.suptitle(f\"Verification for r_initial={r_initial:.2f}, a_spin={a_spin:.2f}\", fontsize=16)\n",
    "\n",
    "        axes[0].plot(analytical_data[:, 1], analytical_data[:, 2], 'r--', label='Analytical', lw=2)\n",
    "        axes[0].plot(numerical_data[:, 2], numerical_data[:, 3], 'c-', label='Numerical', lw=1, alpha=0.8)\n",
    "        axes[0].set_title(\"Orbit Trajectory (Top-Down View)\")\n",
    "        axes[0].set_xlabel(\"x (M)\"); axes[0].set_ylabel(\"y (M)\")\n",
    "        axes[0].set_aspect('equal', 'box')\n",
    "        axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "        axes[1].plot(tau_values, radius_error, label='Radius Error |r_num - r_initial|')\n",
    "        axes[1].plot(tau_values, phase_error, label='Phase Error |φ_num - φ_ana|')\n",
    "        axes[1].set_title(\"Error Growth over Proper Time\")\n",
    "        axes[1].set_xlabel(\"Proper Time (τ) [M]\"); axes[1].set_ylabel(\"Error\")\n",
    "        axes[1].set_yscale('log')\n",
    "        axes[1].legend(); axes[1].grid(True)\n",
    "        \n",
    "        plot_filename = f\"verification_r{r_initial:.2f}_a{a_spin:.2f}.png\"\n",
    "        full_plot_path = os.path.join(full_output_folder, plot_filename)\n",
    "        plt.savefig(full_plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  [✓] Plot saved to '{full_plot_path}'\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise KeyboardInterrupt(\"Execution stopped here intentionally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e28e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prograde orbits with high spin (a=0.99)\n",
    "# ISCO is at approximately r = 1.232 M. Orbits below ~1.4M can be numerically sensitive.\n",
    "prograde_test_cases_stable = [\n",
    "    # (a_spin, r_initial)\n",
    "    (0.99, 1.5),   # A stable orbit just outside the numerically sensitive region\n",
    "    (0.99, 8),   # Close, stable orbit\n",
    "    (0.99, 9),\n",
    "    (0.99, 10),   # Photon sphere region\n",
    "    (0.99, 11),\n",
    "    (0.99, 6.0),   # Equivalent to Schwarzschild ISCO\n",
    "    (0.99, 9.0),   # Equivalent to Retrograde ISCO\n",
    "    (0.99, 15.0),  # Intermediate orbit\n",
    "    (0.99, 25.0),\n",
    "    (0.99, 50.0)   # Weak-field orbit\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Example function call:\n",
    "verify_mass_integrator(\n",
    "     test_cases=prograde_test_cases_stable,\n",
    "     output_folder=\"verification_prograde_a0.99\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrograde orbits with high spin (a=-0.99)\n",
    "# ISCO is at approximately r = 8.98 M\n",
    "retrograde_test_cases = [\n",
    "    # (a_spin, r_initial)\n",
    "    (-0.99, 9.0),    # Just outside the ISCO\n",
    "    (-0.99, 9.5),    # Near-ISCO\n",
    "    (-0.99, 10.0),\n",
    "    (-0.99, 12.0),\n",
    "    (-0.99, 15.0),   # Intermediate orbit\n",
    "    (-0.99, 20.0),\n",
    "    (-0.99, 30.0),\n",
    "    (-0.99, 50.0),\n",
    "    (-0.99, 75.0),\n",
    "    (-0.99, 100.0)   # Weak-field orbit\n",
    "]\n",
    "\n",
    "# Example function call:\n",
    "verify_mass_integrator(\n",
    "     test_cases=retrograde_test_cases,\n",
    "     output_folder=\"verification_retrograde_a-0.99\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-spin (Schwarzschild) orbits (a=0.0)\n",
    "# ISCO is at r = 6.0 M\n",
    "schwarzschild_test_cases = [\n",
    "    # (a_spin, r_initial)\n",
    "    (0.0, 6.0),    # At the ISCO\n",
    "    (0.0, 6.5),    # Near-ISCO\n",
    "    (0.0, 7.0),\n",
    "    (0.0, 8.0),\n",
    "    (0.0, 10.0),   # Intermediate orbit\n",
    "    (0.0, 15.0),\n",
    "    (0.0, 20.0),\n",
    "    (0.0, 30.0),\n",
    "    (0.0, 50.0),\n",
    "    (0.0, 100.0)   # Weak-field orbit\n",
    "]\n",
    "\n",
    "# Example function call:\n",
    "verify_mass_integrator(\n",
    "     test_cases=schwarzschild_test_cases,\n",
    "     output_folder=\"verification_schwarzschild_a0.00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ace0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prograde orbits with high spin (a=0.99)\n",
    "# ISCO is at approximately r = 1.232 M. Orbits below ~1.4M can be numerically sensitive.\n",
    "prograde_test_cases_stable_test = [\n",
    "    # (a_spin, r_initial)\n",
    "    (0.99, 3) # Close, stable orbit\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Example function call:\n",
    "verify_mass_integrator(\n",
    "     test_cases=prograde_test_cases_stable_test,\n",
    "     output_folder=\"verification_prograde_test_a0.99\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4995a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_particle_trajectory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Photon Integrator)",
   "language": "python",
   "name": "photon-integrator-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
