{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cc56e4",
   "metadata": {},
   "source": [
    "# Mass Integrator: Visualization and Analysis Toolkit\n",
    "\n",
    "**Notebook Status:** <font color='green'><b>Validated</b></font>\n",
    "\n",
    "## Introduction and Goals\n",
    "\n",
    "This notebook is the primary tool for visualizing, analyzing, and validating the data produced by the `mass_integrator` C code project. While the `mass_integrator` notebook is responsible for *generating* the simulation code, this notebook is responsible for *processing its output*.\n",
    "\n",
    "The tools provided here serve three main purposes:\n",
    "\n",
    "1.  **Validation (Debug Mode):** When the C code is run in \"debug mode,\" it produces a detailed text file (`massive_particle_path.txt`) tracing the trajectory of a single particle. The functions in **Part 1** of this notebook read this file and generate plots to validate the numerical accuracy of the integrator against known physics, such as the conservation of orbital radius and the rates of relativistic precession.\n",
    "\n",
    "2.  **Visualization (Production Mode):** When the C code is run in \"production mode,\" it generates a series of binary snapshot files (`mass_blueprint_t_xxxx.bin`) representing the state of the entire accretion disk at different times. The functions in **Part 2** read these binary files to produce 3D visualizations of the disk and stitch them together into animations of the disk's evolution.\n",
    "\n",
    "3.  **Pre-processing for the Photon Integrator:** The `photon_geodesic_integrator` requires the disk snapshot data to be in a highly optimized format for fast spatial queries. The functions in **Part 3** perform this crucial pre-processing step, converting the raw `.bin` snapshots into query-ready `.kdtree.bin` files.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "This notebook is intended to be used *after* you have successfully compiled and run the `mass_integrator` C code.\n",
    "\n",
    "*   To use the **Part 1** validation tools, run the C code with the `run_in_debug_mode = True` parameter.\n",
    "*   To use the **Part 2 & 3** visualization and pre-processing tools, run the C code with `run_in_debug_mode = False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b7029",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "This notebook provides tools for visualizing and validating the output of the `mass_integrator` C code.\n",
    "\n",
    "*   [Imports and Setup](#imports)\n",
    "\n",
    "**Part 1: Debug Mode Analysis (Single Particle Trajectory)**\n",
    "*   [1.a: 3D Trajectory Plot](#part_1a)\n",
    "*   [1.b: Trajectory Component Plots](#part_1b)\n",
    "*   [1.c: Orbital Validation Plots](#part_1c)\n",
    "\n",
    "**Part 2: Production Mode Analysis (Disk Snapshots)**\n",
    "*   [2.a: Disk Snapshot Visualizer](#part_2a)\n",
    "*   [2.b: Create Animation of Disk Evolution](#part_2b)\n",
    "\n",
    "**Part 3: K-d Tree Pre-processor**\n",
    "*   [3.a: Build K-d Trees from Snapshots](#part_3a)\n",
    "*   [3.b: Inspect a K-d Tree File](#part_3b)\n",
    "\n",
    "**Part 4: Verification Suite for Keplerian Orbits**\n",
    "*   [4.a: Analytical Solution and Verification Driver](#part_4a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db07ca8",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "### **Imports and Setup**\n",
    "\n",
    "This cell imports all Python libraries required for the notebook to function. By centralizing all imports here, we can easily see the dependencies of the project and avoid redundant `import` statements in later cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "from collections import deque\n",
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Core data science and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Performance optimization\n",
    "import numba\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The nrpy.params module is needed to read the snapshot_every_t parameter\n",
    "# when generating animation frames.\n",
    "try:\n",
    "    import nrpy.params as par\n",
    "except ImportError:\n",
    "    print(\"Warning: nrpy.params not found. Animation generation might use a default time step.\")\n",
    "    # Define a dummy class if nrpy is not available, so the notebook doesn't crash.\n",
    "    class Par:\n",
    "        def parval_from_str(self, param_name):\n",
    "            if param_name == \"snapshot_every_t\":\n",
    "                return 10.0 # A reasonable default\n",
    "            raise ValueError(f\"Parameter {param_name} not found.\")\n",
    "    par = Par()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930358ad",
   "metadata": {},
   "source": [
    "<a id='part_1a'></a>\n",
    "### 1.a: 3D Trajectory Plot\n",
    "\n",
    "This section provides the function `plot_particle_trajectory`, which reads the `massive_particle_path.txt` file generated in debug mode. It creates a 3D visualization of the particle's orbit, showing its path relative to the black hole's event horizon. This is useful for getting an intuitive, qualitative sense of the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_particle_trajectory(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads the trajectory data from the C code's output file and generates\n",
    "    a 3D plot of the particle's orbit around the black hole.\n",
    "\n",
    "    Args:\n",
    "        project_dir: The root directory of the C project where the output file is located.\n",
    "        input_filename: The name of the trajectory data file.\n",
    "        M_scale: The mass of the black hole, used to plot the event horizon.\n",
    "        a_spin: The spin of the black hole, used to plot the event horizon.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Particle Trajectory Plot ---\")\n",
    "    \n",
    "    # --- 1. Construct the full path and load the data ---\n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        print(\"Please ensure you have compiled and run the C code successfully.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the data, skipping the header row\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        # Columns: 0:τ, 1:t, 2:x, 3:y, 4:z, 5:u^t, 6:u^x, 7:u^y, 8:u^z\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "        print(f\"Successfully loaded {len(x_coords)} data points from trajectory file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{full_path}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Set up the 3D plot ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # --- 3. Plot the particle's trajectory ---\n",
    "    ax.plot(x_coords, y_coords, z_coords, label='Particle Orbit', color='cyan', lw=2)\n",
    "    \n",
    "    # Mark the start and end points\n",
    "    ax.scatter(x_coords[0], y_coords[0], z_coords[0], color='lime', s=100, label='Start', marker='o')\n",
    "    ax.scatter(x_coords[-1], y_coords[-1], z_coords[-1], color='red', s=100, label='End', marker='X')\n",
    "\n",
    "    # --- 4. Plot the black hole's event horizon ---\n",
    "    # The radius of the event horizon for a Kerr black hole\n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    \n",
    "    # Create a sphere for the event horizon\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    \n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    # Add a grey wireframe for better visibility\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    # --- 5. Customize the plot ---\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Set equal aspect ratio\n",
    "    max_range = np.array([x_coords.max()-x_coords.min(), y_coords.max()-y_coords.min(), z_coords.max()-z_coords.min()]).max() / 2.0\n",
    "    mid_x = (x_coords.max()+x_coords.min()) * 0.5\n",
    "    mid_y = (y_coords.max()+y_coords.min()) * 0.5\n",
    "    mid_z = (z_coords.max()+z_coords.min()) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "    ax.set_title(f\"Massive Particle Trajectory (M={M_scale}, a={a_spin})\", fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.view_init(elev=30., azim=45) # Set a nice viewing angle\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e991c",
   "metadata": {},
   "source": [
    "<a id='part_1b'></a>\n",
    "### 1.b: Trajectory Component Plots\n",
    "\n",
    "The `plot_trajectory_components` function also reads the debug output file but generates a set of 2D plots. It shows the evolution of each Cartesian coordinate (`x`, `y`, `z`) as a function of coordinate time `t`, and also plots the relationship between the particle's proper time `τ` and coordinate time `t`. These plots are useful for analyzing oscillations and time dilation effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_components(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and creates four plots:\n",
    "    x vs t, y vs t, z vs t, and proper time (τ) vs t.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Trajectory Component Plots ---\")\n",
    "    \n",
    "    # --- 1. Load the data ---\n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    \n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        # Columns: 0:τ, 1:t, 2:x, 3:y, 4:z, ...\n",
    "        proper_time = data[:, 0]\n",
    "        coord_time = data[:, 1]\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "        print(f\"Successfully loaded {len(coord_time)} data points.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse data file '{full_path}'. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Create the plots ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Particle Trajectory Components vs. Coordinate Time', fontsize=20)\n",
    "\n",
    "    # Plot 1: x(t)\n",
    "    axes[0, 0].plot(coord_time, x_coords, color='cyan')\n",
    "    axes[0, 0].set_title('X Coordinate vs. Time', fontsize=14)\n",
    "    axes[0, 0].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('x [M]', fontsize=12)\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Plot 2: y(t)\n",
    "    axes[0, 1].plot(coord_time, y_coords, color='magenta')\n",
    "    axes[0, 1].set_title('Y Coordinate vs. Time', fontsize=14)\n",
    "    axes[0, 1].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('y [M]', fontsize=12)\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot 3: z(t)\n",
    "    axes[1, 0].plot(coord_time, z_coords, color='lime')\n",
    "    axes[1, 0].set_title('Z Coordinate vs. Time', fontsize=14)\n",
    "    axes[1, 0].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('z [M]', fontsize=12)\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot 4: τ(t)\n",
    "    axes[1, 1].plot(coord_time, proper_time, color='gold')\n",
    "    axes[1, 1].set_title('Proper Time vs. Coordinate Time', fontsize=14)\n",
    "    axes[1, 1].set_xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Proper Time (τ) [M]', fontsize=12)\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363abe23",
   "metadata": {},
   "source": [
    "<a id='part_1c'></a>\n",
    "### 1.c: Orbital Validation Plots\n",
    "\n",
    "The following functions perform quantitative validation of the integrator's accuracy by comparing the numerical results against known theoretical predictions from General Relativity for stable orbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radius_vs_time(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and plots the particle's radial distance (r)\n",
    "    as a function of coordinate time (t) to validate circularity.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Radius vs. Time Validation Plot ---\")\n",
    "    \n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        coord_time = data[:, 1]\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "        z_coords = data[:, 4]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the radius at each time step\n",
    "    radius = np.sqrt(x_coords**2 + y_coords**2 + z_coords**2)\n",
    "    \n",
    "    # Calculate statistics on the radius\n",
    "    mean_radius = np.mean(radius)\n",
    "    min_radius = np.min(radius)\n",
    "    max_radius = np.max(radius)\n",
    "    percent_variation = 100 * (max_radius - min_radius) / mean_radius\n",
    "\n",
    "    print(f\"Radius Statistics:\")\n",
    "    print(f\"  Mean Radius: {mean_radius:.6f} M\")\n",
    "    print(f\"  Min Radius:  {min_radius:.6f} M\")\n",
    "    print(f\"  Max Radius:  {max_radius:.6f} M\")\n",
    "    print(f\"  Total Variation: {percent_variation:.4f}%\")\n",
    "\n",
    "    # Create the plot\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(coord_time, radius, label='Particle Radius r(t)', color='cyan')\n",
    "    \n",
    "    # Add lines for mean, min, and max to visualize the variation\n",
    "    plt.axhline(mean_radius, color='lime', linestyle='--', label=f'Mean r = {mean_radius:.4f}')\n",
    "    \n",
    "    plt.title('Validation: Particle Radius vs. Coordinate Time', fontsize=16)\n",
    "    plt.xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    plt.ylabel('Radius (r) [M]', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Use a \"tight\" y-axis to emphasize any small variations\n",
    "    plt.ylim(min_radius * 0.999, max_radius * 1.001)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc89c7",
   "metadata": {},
   "source": [
    "### Plotting Precession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a086db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precession_validation(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9,\n",
    "    r_initial: float = 10.0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and validates the orbital precession rate against\n",
    "    the theoretical Lense-Thirring formula.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Precession Validation Plot ---\")\n",
    "    \n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        coord_time = data[:, 1]\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the azimuthal angle phi at each time step\n",
    "    phi = np.arctan2(y_coords, x_coords)\n",
    "    \n",
    "    # The angle will wrap around from +pi to -pi. We need to unwrap it.\n",
    "    phi_unwrapped = np.unwrap(phi)\n",
    "\n",
    "    # --- Theoretical Calculation ---\n",
    "    r = r_initial\n",
    "    Omega_K = (M_scale**0.5) / (r**1.5 + a_spin * M_scale**0.5)\n",
    "    Omega_LT = (2 * M_scale * a_spin) / (r**3)\n",
    "    Omega_phi_theory = Omega_K + Omega_LT\n",
    "    \n",
    "    # --- Measurement from Simulation Data ---\n",
    "    # Perform a linear regression to find the slope of phi(t)\n",
    "    regression = linregress(coord_time, phi_unwrapped)\n",
    "    Omega_phi_measured = regression.slope\n",
    "    \n",
    "    percent_error = 100 * abs(Omega_phi_measured - Omega_phi_theory) / Omega_phi_theory\n",
    "\n",
    "    print(\"Precession Rate (dφ/dt) Validation:\")\n",
    "    print(f\"  Theoretical Ω_φ: {Omega_phi_theory:.6f} rad/M\")\n",
    "    print(f\"  Measured Ω_φ (from data): {Omega_phi_measured:.6f} rad/M\")\n",
    "    print(f\"  Relative Error: {percent_error:.4f}%\")\n",
    "\n",
    "    # --- Create the Plot ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(coord_time, phi_unwrapped, label='Measured φ(t) from Simulation', color='cyan', lw=2)\n",
    "    plt.plot(coord_time, Omega_phi_theory * coord_time, label=f'Theoretical φ(t) (slope={Omega_phi_theory:.4f})', color='lime', linestyle='--', lw=2)\n",
    "    \n",
    "    plt.title('Validation: Orbital Precession (Frame-Dragging)', fontsize=16)\n",
    "    plt.xlabel('Coordinate Time (t) [M]', fontsize=12)\n",
    "    plt.ylabel('Azimuthal Angle (φ) [radians]', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcebd3",
   "metadata": {},
   "source": [
    "### Plot apsidal precession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_apsidal_precession_validation(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    input_filename: str = \"massive_particle_path.txt\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads trajectory data and validates the apsidal precession rate against\n",
    "    the theoretical GR formula for nearly circular orbits.\n",
    "    \"\"\"\n",
    "    print(\"--- Generating Apsidal Precession Validation Plot ---\")\n",
    "    \n",
    "    full_path = os.path.join(project_dir, input_filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"ERROR: Trajectory file not found at '{full_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.loadtxt(full_path, skiprows=1)\n",
    "        x_coords = data[:, 2]\n",
    "        y_coords = data[:, 3]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load data. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    radius = np.sqrt(x_coords**2 + y_coords**2)\n",
    "    phi = np.unwrap(np.arctan2(y_coords, x_coords))\n",
    "    \n",
    "    # Find the angles where the particle is at periapsis (minimum radius)\n",
    "    # We find indices where the radius is a local minimum\n",
    "    periapsis_indices = (np.r_[True, radius[1:] < radius[:-1]] & np.r_[radius[:-1] < radius[1:], True]).nonzero()[0]\n",
    "    \n",
    "    if len(periapsis_indices) < 2:\n",
    "        print(\"Could not find at least two periapsis points. Cannot calculate precession.\")\n",
    "        return\n",
    "        \n",
    "    # Calculate the measured precession angle per orbit\n",
    "    delta_phi_measured = phi[periapsis_indices[1]] - phi[periapsis_indices[0]]\n",
    "    precession_per_orbit_measured = delta_phi_measured - 2 * np.pi\n",
    "\n",
    "    # --- Theoretical Calculation at the average radius of the orbit ---\n",
    "    r_avg = np.mean(radius)\n",
    "    M = M_scale\n",
    "    a = a_spin\n",
    "    \n",
    "    Omega_phi_theory = (M**0.5) / (r_avg**1.5 + a * M**0.5)\n",
    "    Omega_r_theory_sq = Omega_phi_theory**2 * (1 - (6*M)/r_avg + (8*a*M**0.5)/r_avg**1.5 - (3*a**2)/r_avg**2)\n",
    "    \n",
    "    if Omega_r_theory_sq < 0:\n",
    "        print(\"Theoretical orbit is unstable (Ω_r^2 < 0). Cannot calculate precession.\")\n",
    "        return\n",
    "        \n",
    "    Omega_r_theory = np.sqrt(Omega_r_theory_sq)\n",
    "    \n",
    "    # Precession per unit time\n",
    "    Omega_precession_theory = Omega_phi_theory - Omega_r_theory\n",
    "    \n",
    "    # Period of one radial oscillation\n",
    "    T_r = 2 * np.pi / Omega_r_theory\n",
    "    \n",
    "    # Total precession angle over one radial period\n",
    "    precession_per_orbit_theory = Omega_precession_theory * T_r\n",
    "    \n",
    "    percent_error = 100 * abs(precession_per_orbit_measured - precession_per_orbit_theory) / precession_per_orbit_theory\n",
    "\n",
    "    print(f\"Apsidal Precession Validation (at average radius r={r_avg:.3f} M):\")\n",
    "    print(f\"  Measured precession per orbit:   {precession_per_orbit_measured:.6f} radians\")\n",
    "    print(f\"  Theoretical precession per orbit: {precession_per_orbit_theory:.6f} radians\")\n",
    "    print(f\"  Relative Error: {percent_error:.4f}%\")\n",
    "\n",
    "    # --- Create the Plot (Polar Plot) ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    ax.plot(phi, radius, color='cyan', label='Particle Orbit')\n",
    "    ax.scatter(phi[periapsis_indices], radius[periapsis_indices], color='lime', s=100, label='Periapsis Points', zorder=5)\n",
    "    \n",
    "    ax.set_title('Validation: Apsidal Precession of a Nearly Circular Orbit', fontsize=16)\n",
    "    ax.set_xlabel('Azimuthal Angle (φ)', fontsize=12)\n",
    "    ax.set_ylabel('Radius (r) [M]', fontsize=12, labelpad=-50)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e191b0f",
   "metadata": {},
   "source": [
    "<a id='part_2a'></a>\n",
    "## Part 2: Production Mode Analysis (Disk Snapshots)\n",
    "\n",
    "### 2.a: Disk Snapshot Visualizer\n",
    "\n",
    "When the `mass_integrator` is run in production mode, it outputs binary `.bin` files containing the state of all particles at specific time intervals. The `visualize_disk_snapshot` function reads one of these files, performs several sanity checks on the data, and generates a 3D scatter plot of the particle positions. This allows for a quick visual inspection of the disk's structure at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_disk_snapshot(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    output_folder: str = \"output\",\n",
    "    snapshot_index: int = -1, # -1 means the last available snapshot\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.9\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a specific mass blueprint snapshot file, performs sanity checks,\n",
    "    and generates a 3D plot of the particle disk.\n",
    "    \n",
    "    UPDATED to read the new binary format with the full 4-velocity.\n",
    "    \"\"\"\n",
    "    print(\"--- Visualizing Mass Blueprint Snapshot ---\")\n",
    "    \n",
    "    # --- 1. Find and Load the Snapshot File ---\n",
    "    snapshot_dir = os.path.join(project_dir, output_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(snapshot_dir, \"mass_blueprint_t_*.bin\")))\n",
    "    \n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No snapshot .bin files found in '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    if snapshot_index == -1:\n",
    "        snapshot_to_load = snapshot_files[-1]\n",
    "    elif snapshot_index < len(snapshot_files):\n",
    "        snapshot_to_load = snapshot_files[snapshot_index]\n",
    "    else:\n",
    "        print(f\"ERROR: Snapshot index {snapshot_index} is out of bounds. Only {len(snapshot_files)} snapshots exist.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading snapshot file: '{snapshot_to_load}'\")\n",
    "\n",
    "    # MODIFICATION: Define the dtype to match the new C struct with u[4].\n",
    "    snapshot_dtype = np.dtype([\n",
    "        ('id', np.int32),\n",
    "        ('pos', 'f8', (3,)), \n",
    "        ('u', 'f8', (4,)),   # Changed from ('u_spatial', 'f8', (3,))\n",
    "        ('lambda_rest', 'f8'),\n",
    "        ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # The header is a 4-byte int, not part of the dtype\n",
    "        with open(snapshot_to_load, 'rb') as f:\n",
    "            num_particles = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "            data = np.fromfile(f, dtype=snapshot_dtype, count=num_particles)\n",
    "        print(f\"Successfully loaded data for {num_particles} particles.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse the data file '{snapshot_to_load}'.\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Perform Sanity Checks ---\n",
    "    positions = data['pos']\n",
    "    # MODIFICATION: Use the new 'u' field for velocities.\n",
    "    velocities = data['u'] \n",
    "    \n",
    "    radii = np.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n",
    "    # Calculate speed from the SPATIAL components of the 4-velocity\n",
    "    speeds = np.sqrt(velocities[:, 1]**2 + velocities[:, 2]**2 + velocities[:, 3]**2)\n",
    "\n",
    "    print(\"\\n--- Data Sanity Checks ---\")\n",
    "    print(f\"  Particle count: {num_particles}\")\n",
    "    print(f\"  Mean radius: {np.mean(radii):.3f} M (should be between disk_r_min and disk_r_max)\")\n",
    "    print(f\"  Mean spatial 4-velocity magnitude: {np.mean(speeds):.3f}\")\n",
    "    print(f\"  Max z-coordinate: {np.max(np.abs(positions[:, 2])):.2e} (should be close to zero)\")\n",
    "    \n",
    "    if np.any(np.isnan(positions)):\n",
    "        nan_count = np.count_nonzero(np.isnan(data['pos'][:,0]))\n",
    "        print(f\"  WARNING: Found {nan_count} terminated (NaN) particles in this snapshot.\")\n",
    "    \n",
    "    # --- 3. Create the 3D Plot (No changes needed here) ---\n",
    "    plt.style.use('dark_background')\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    num_to_plot = min(num_particles, 2000)\n",
    "    plot_indices = np.random.choice(num_particles, num_to_plot, replace=False)\n",
    "    \n",
    "    colors = radii[plot_indices]\n",
    "    sc = ax.scatter(positions[plot_indices, 0], positions[plot_indices, 1], positions[plot_indices, 2], \n",
    "                    c=colors, cmap='plasma', s=5, label='Disk Particles')\n",
    "    \n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    max_radius_plot = np.max(radii) * 1.1\n",
    "    ax.set_xlim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_ylim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_zlim(-max_radius_plot/2, max_radius_plot/2)\n",
    "\n",
    "    ax.set_title(f\"Accretion Disk Snapshot from {os.path.basename(snapshot_to_load)}\", fontsize=16)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.6, aspect=10, label='Particle Radius (M)')\n",
    "    ax.view_init(elev=45., azim=45)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eccd56",
   "metadata": {},
   "source": [
    "<a id='part_2b'></a>\n",
    "### 2.b: Create Animation of Disk Evolution\n",
    "\n",
    "This section provides the functions necessary to convert the entire sequence of snapshot files into a video. The process involves two steps:\n",
    "1.  **`generate_animation_frames`**: This orchestrator function finds all snapshot files, then calls a helper (`_plot_single_frame`) to generate a `.png` image for each one.\n",
    "2.  **`encode_video_from_frames`**: After all frames are generated, this function uses the external command-line tool `ffmpeg` to stitch the individual images into a single `.mp4` video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a991bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_single_frame(\n",
    "    positions: np.ndarray,\n",
    "    radii: np.ndarray,\n",
    "    M_scale: float,\n",
    "    a_spin: float,\n",
    "    current_time: float,\n",
    "    output_filename: str,\n",
    "    fig_width_inches: float,\n",
    "    fig_dpi: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots a single frame of the disk animation and saves it to a file,\n",
    "    guaranteeing the output image has even dimensions for video encoding.\n",
    "    \"\"\"\n",
    "    # Calculate the desired pixel dimensions\n",
    "    desired_width_px = fig_width_inches * fig_dpi\n",
    "    \n",
    "    # Get the aspect ratio from the data to calculate height\n",
    "    x_range = np.max(positions[:, 0]) - np.min(positions[:, 0])\n",
    "    y_range = np.max(positions[:, 1]) - np.min(positions[:, 1])\n",
    "    aspect_ratio = y_range / x_range if x_range > 0 else 1.0\n",
    "    desired_height_px = desired_width_px * aspect_ratio\n",
    "\n",
    "    # Round down to the nearest even number\n",
    "    final_width_px = int(desired_width_px // 2 * 2)\n",
    "    final_height_px = int(desired_height_px // 2 * 2)\n",
    "    \n",
    "    # Recalculate figure size in inches to match the final pixel dimensions\n",
    "    final_fig_width_inches = final_width_px / fig_dpi\n",
    "    final_fig_height_inches = final_height_px / fig_dpi\n",
    "    \n",
    "    fig = plt.figure(figsize=(final_fig_width_inches, final_fig_height_inches))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    num_particles = len(positions)\n",
    "    num_to_plot = min(num_particles, 5000)\n",
    "    plot_indices = np.random.choice(num_particles, num_to_plot, replace=False)\n",
    "    \n",
    "    colors = radii[plot_indices]\n",
    "    sc = ax.scatter(positions[plot_indices, 0], positions[plot_indices, 1], positions[plot_indices, 2], \n",
    "                    c=colors, cmap='plasma', s=5)\n",
    "    \n",
    "    r_horizon = M_scale * (1 + np.sqrt(1 - a_spin**2))\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x_bh = r_horizon * np.outer(np.cos(u), np.sin(v))\n",
    "    y_bh = r_horizon * np.outer(np.sin(u), np.sin(v))\n",
    "    z_bh = r_horizon * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(x_bh, y_bh, z_bh, color='black', alpha=0.9, rstride=4, cstride=4)\n",
    "    ax.plot_wireframe(x_bh, y_bh, z_bh, color='dimgrey', alpha=0.2, rstride=10, cstride=10)\n",
    "\n",
    "    ax.set_xlabel('X (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel('Y (M)', fontsize=12, labelpad=10)\n",
    "    ax.set_zlabel('Z (M)', fontsize=12, labelpad=10)\n",
    "    \n",
    "    max_radius_plot = np.max(radii) * 1.1\n",
    "    ax.set_xlim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_ylim(-max_radius_plot, max_radius_plot)\n",
    "    ax.set_zlim(-max_radius_plot/2, max_radius_plot/2)\n",
    "\n",
    "    ax.set_title(f\"Accretion Disk Snapshot at t = {current_time:.2f} M\", fontsize=16)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.6, aspect=10, label='Particle Radius (M)')\n",
    "    ax.view_init(elev=45., azim=45)\n",
    "    \n",
    "    # --- THE CRITICAL FIX IS HERE ---\n",
    "    # Adjust layout first, then save without the problematic argument.\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename, dpi=fig_dpi) # REMOVED: bbox_inches='tight'\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "# This is the main function that calls the helper above. It does not need changes,\n",
    "# but is included for completeness.\n",
    "def generate_animation_frames(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    output_folder: str = \"output_GrandDesign_ISCO\",\n",
    "    frames_output_dir: str = \"animation_frames\",\n",
    "    M_scale: float = 1.0,\n",
    "    a_spin: float = 0.95,\n",
    "    fig_width_inches: float = 12.0,\n",
    "    fig_dpi: int = 150,\n",
    "    overwrite_existing_frames: bool = False\n",
    ") -> None:\n",
    "    # ... (This function's body is unchanged from the previous answer) ...\n",
    "    print(\"--- Starting Animation Frame Generation ---\")\n",
    "    plt.style.use('dark_background')\n",
    "    \n",
    "    snapshot_dir = os.path.join(project_dir, output_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files = glob.glob(os.path.join(snapshot_dir, \"mass_blueprint_t_*.bin\"))\n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No snapshot .bin files found in '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files.sort(key=lambda f: int(os.path.basename(f).split('_t_')[1].split('.bin')[0]))\n",
    "    print(f\"Found {len(snapshot_files)} snapshots to process.\")\n",
    "\n",
    "    full_frames_dir = os.path.join(project_dir, frames_output_dir)\n",
    "    os.makedirs(full_frames_dir, exist_ok=True)\n",
    "\n",
    "    if not overwrite_existing_frames:\n",
    "        print(\"Overwrite is OFF. Will skip any frames that already exist.\")\n",
    "\n",
    "    snapshot_dtype = np.dtype([\n",
    "        ('id', np.int32), ('pos', 'f8', (3,)), ('u', 'f8', (4,)),\n",
    "        ('lambda_rest', 'f8'), ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        snapshot_every_t = par.parval_from_str(\"snapshot_every_t\")\n",
    "    except ValueError:\n",
    "        snapshot_every_t = 1.0\n",
    "\n",
    "    for i, snapshot_file in enumerate(snapshot_files):\n",
    "        frame_filename = os.path.join(full_frames_dir, f\"frame_{i:04d}.png\")\n",
    "        \n",
    "        if not overwrite_existing_frames and os.path.exists(frame_filename):\n",
    "            print(f\"  Skipping frame {i+1}/{len(snapshot_files)}: {os.path.basename(frame_filename)} already exists.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Processing frame {i+1}/{len(snapshot_files)}: {os.path.basename(snapshot_file)}\")\n",
    "        \n",
    "        with open(snapshot_file, 'rb') as f:\n",
    "            num_particles = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "            if num_particles == 0: continue\n",
    "            data = np.fromfile(f, dtype=snapshot_dtype, count=num_particles)\n",
    "        \n",
    "        positions = data['pos']\n",
    "        radii = np.sqrt(positions[:, 0]**2 + positions[:, 1]**2)\n",
    "        \n",
    "        snapshot_number = int(os.path.basename(snapshot_file).split('_t_')[1].split('.bin')[0])\n",
    "        current_time = snapshot_number * snapshot_every_t\n",
    "        \n",
    "        _plot_single_frame(\n",
    "            positions, radii, M_scale, a_spin, current_time, frame_filename,\n",
    "            fig_width_inches=fig_width_inches, fig_dpi=fig_dpi\n",
    "        )\n",
    "    \n",
    "    print(\"\\n--- Frame generation complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62435c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HOW TO RUN THE DEFINITIVE FIX ---\n",
    "\n",
    "# 1. Re-generate the frames. This is necessary because the files on disk are still wrong.\n",
    "#    This will replace the 1371px-wide images with new, even-dimensioned ones.\n",
    "print(\"STEP 1: Re-generating all frames with corrected dimensions...\")\n",
    "generate_animation_frames(\n",
    "    project_dir=\"/home/daltonm/Documents/project/mass_integrator\",\n",
    "    output_folder=\"output_GrandDesign_ISCO\",\n",
    "    frames_output_dir=\"animation_frames_grand_design\",\n",
    "    M_scale=1.0,\n",
    "    a_spin=0.95,\n",
    "    overwrite_existing_frames=True\n",
    ")\n",
    "print(\"STEP 1 COMPLETE.\\n\")\n",
    "\n",
    "# 2. Encode the video from the newly created, valid frames.\n",
    "print(\"STEP 2: Encoding the new frames into a video...\")\n",
    "encode_video_from_frames(\n",
    "    image_folder=\"/home/daltonm/Documents/project/mass_integrator/animation_frames_grand_design\",\n",
    "    output_video_path=\"/home/daltonm/Documents/project/mass_integrator/grand_design_disk_evolution.mp4\",\n",
    "    frame_rate=15\n",
    ")\n",
    "print(\"STEP 2 COMPLETE.\")\n",
    "\n",
    "# --- How to Run This Test ---\n",
    "# 1. Run your C code in PRODUCTION mode (set run_in_debug_mode = false in the .par file).\n",
    "# 2. This will create an 'output' folder with several .bin files.\n",
    "# 3. Call this function. It will automatically find and plot the LAST snapshot.\n",
    "output_folder=\"output_GrandDesign_ISCO\"\n",
    "visualize_disk_snapshot(output_folder=output_folder)\n",
    "\n",
    "#plot_particle_trajectory(\n",
    "#    project_dir=\"project/mass_integrator\",\n",
    "#    input_filename=\"massive_particle_path.txt\",\n",
    "#    M_scale=1.0,\n",
    "#    a_spin=0.9\n",
    "#)\n",
    "# After running the C code, call this function.\n",
    "plot_trajectory_components()\n",
    "# Call this function after running your C code.\n",
    "plot_radius_vs_time()\n",
    "# --- How to run ---\n",
    "plot_precession_validation()\n",
    "# --- How to run ---\n",
    "plot_apsidal_precession_validation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e84af1",
   "metadata": {},
   "source": [
    "### Video of disk movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521243f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video_from_frames(\n",
    "    image_folder: str,\n",
    "    output_video_path: str,\n",
    "    frame_rate: int = 30,\n",
    "    crf: int = 18\n",
    ") -> None:\n",
    "    # ... (This function is unchanged) ...\n",
    "    print(f\"--- Starting Video Encoding ---\")\n",
    "    output_dir = os.path.dirname(output_video_path)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    command = [\n",
    "        'ffmpeg', '-y', '-framerate', str(frame_rate),\n",
    "        '-i', os.path.join(image_folder, 'frame_%04d.png'),\n",
    "        '-c:v', 'libx264', '-pix_fmt', 'yuv420p',\n",
    "        '-r', str(frame_rate), '-crf', str(crf),\n",
    "        output_video_path\n",
    "    ]\n",
    "    print(f\"Running FFmpeg command:\\n{' '.join(command)}\")\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        print(f\"\\n[✓] Video encoding successful. File saved to '{output_video_path}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n[!] ERROR: FFmpeg not found.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n[!] ERROR: FFmpeg failed with exit code {e.returncode}.\")\n",
    "        print(\"\\n--- FFmpeg stderr ---\")\n",
    "        print(e.stderr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20f764",
   "metadata": {},
   "source": [
    "<a id='part_3a'></a>\n",
    "## Part 3: K-d Tree Pre-processor\n",
    "\n",
    "### 3.a: Build K-d Trees from Snapshots\n",
    "\n",
    "The `photon_geodesic_integrator` needs to perform millions of fast spatial queries to check for intersections with the accretion disk. To enable this, the raw snapshot data must be pre-processed into a more efficient data structure. This section provides the tools to build a **k-d tree** from each snapshot.\n",
    "\n",
    "The process is handled by two main functions:\n",
    "*   **`build_implicit_kdtree_kernel`**: A high-performance kernel, accelerated with `numba`, that takes particle positions and recursively partitions them to build the tree structure.\n",
    "*   **`run_kdtree_preprocessor_parallel`**: The main driver that finds all raw `.bin` snapshots and uses a multiprocessing pool to run the kernel on all of them in parallel, creating the final `.kdtree.bin` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  STEP 1: Create the JIT-compiled \"kernel\" for the heavy lifting.\n",
    "# ==============================================================================\n",
    "# Numba works best with simple data types, so we pass NumPy arrays.\n",
    "# The @numba.jit decorator is the key. 'nopython=True' ensures it compiles to fast machine code.\n",
    "@numba.jit(nopython=True)\n",
    "def build_implicit_kdtree_kernel(particle_pos_array, num_particles):\n",
    "    \"\"\"\n",
    "    Numba-compiled kernel to perform the heavy lifting of building the implicit k-d tree.\n",
    "    This function contains only the numerical logic that Numba can optimize.\n",
    "    \n",
    "    Args:\n",
    "        particle_pos_array (np.ndarray): A NumPy array of shape (N, 3) with just particle positions.\n",
    "        num_particles (int): The number of particles.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (reordering_indices, node_metadata)\n",
    "               - reordering_indices: An array that maps the old index to the new, reordered index.\n",
    "               - node_metadata: The array of split axes for each node.\n",
    "    \"\"\"\n",
    "    # These will store the final results\n",
    "    reordering_indices = np.zeros(num_particles, dtype=np.int32)\n",
    "    node_metadata = np.full(num_particles, -1, dtype=np.int32)\n",
    "\n",
    "    # Numba doesn't have deque, but we can simulate a queue with a NumPy array and pointers.\n",
    "    # This is a common pattern for JIT-compiling queue-based algorithms.\n",
    "    # We need to pre-allocate a large enough queue. num_particles is a safe upper bound.\n",
    "    q_target_idx = np.zeros(num_particles, dtype=np.int32)\n",
    "    # This queue will store slices (start, end) into a temporary index array\n",
    "    q_slice_start = np.zeros(num_particles, dtype=np.int32)\n",
    "    q_slice_end = np.zeros(num_particles, dtype=np.int32)\n",
    "    \n",
    "    q_head = 0\n",
    "    q_tail = 0\n",
    "\n",
    "    # The array of original indices that we will sort and partition\n",
    "    indices_array = np.arange(num_particles, dtype=np.int32)\n",
    "    \n",
    "    if num_particles > 0:\n",
    "        # Enqueue the first item: target index 0, and the slice representing all particles\n",
    "        q_target_idx[q_tail] = 0\n",
    "        q_slice_start[q_tail] = 0\n",
    "        q_slice_end[q_tail] = num_particles\n",
    "        q_tail += 1\n",
    "\n",
    "    while q_head < q_tail:\n",
    "        # Dequeue an item\n",
    "        target_idx = q_target_idx[q_head]\n",
    "        start = q_slice_start[q_head]\n",
    "        end = q_slice_end[q_head]\n",
    "        q_head += 1\n",
    "\n",
    "        # --- Adaptive Splitting Logic ---\n",
    "        # Note: Slicing is faster inside Numba than fancy indexing\n",
    "        current_indices = indices_array[start:end]\n",
    "        current_positions = particle_pos_array[current_indices]\n",
    "        \n",
    "        min_x, min_y, min_z = current_positions[0]\n",
    "        max_x, max_y, max_z = current_positions[0]\n",
    "        for i in range(1, len(current_positions)):\n",
    "            pos = current_positions[i]\n",
    "            min_x, max_x = min(min_x, pos[0]), max(max_x, pos[0])\n",
    "            min_y, max_y = min(min_y, pos[1]), max(max_y, pos[1])\n",
    "            min_z, max_z = min(min_z, pos[2]), max(max_z, pos[2])\n",
    "\n",
    "        spread_x = max_x - min_x\n",
    "        spread_y = max_y - min_y\n",
    "        spread_z = max_z - min_z\n",
    "        \n",
    "        # Find split axis (0=x, 1=y, 2=z)\n",
    "        split_axis = 0\n",
    "        if spread_y > spread_x: split_axis = 1\n",
    "        if spread_z > spread_y and spread_z > spread_x: split_axis = 2\n",
    "\n",
    "        # --- Partitioning ---\n",
    "        # Sort the current slice of the main index array based on the split axis\n",
    "        # This is the most performance-critical part\n",
    "        sorted_indices_on_axis = current_indices[np.argsort(particle_pos_array[current_indices, split_axis])]\n",
    "        indices_array[start:end] = sorted_indices_on_axis\n",
    "        \n",
    "        median_offset = len(sorted_indices_on_axis) // 2\n",
    "        median_original_index = sorted_indices_on_axis[median_offset]\n",
    "\n",
    "        # --- Place the Pivot Particle's Metadata ---\n",
    "        reordering_indices[target_idx] = median_original_index\n",
    "        node_metadata[target_idx] = split_axis\n",
    "        \n",
    "        # --- Prepare for Next Level (Enqueue children) ---\n",
    "        left_child_idx = 2 * target_idx + 1\n",
    "        right_child_idx = 2 * target_idx + 2\n",
    "        \n",
    "        # Left child points to the slice before the median\n",
    "        if median_offset > 0:\n",
    "            if left_child_idx < num_particles:\n",
    "                q_target_idx[q_tail] = left_child_idx\n",
    "                q_slice_start[q_tail] = start\n",
    "                q_slice_end[q_tail] = start + median_offset\n",
    "                q_tail += 1\n",
    "        \n",
    "        # Right child points to the slice after the median\n",
    "        if median_offset + 1 < len(sorted_indices_on_axis):\n",
    "            if right_child_idx < num_particles:\n",
    "                q_target_idx[q_tail] = right_child_idx\n",
    "                q_slice_start[q_tail] = start + median_offset + 1\n",
    "                q_slice_end[q_tail] = end\n",
    "                q_tail += 1\n",
    "                \n",
    "    return reordering_indices, node_metadata\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: Create the main driver function that calls the kernel.\n",
    "# ==============================================================================\n",
    "def build_and_save_kdtree_snapshot_fast(raw_snapshot_file, output_dir):\n",
    "    \"\"\"\n",
    "    Main driver function that handles file I/O and calls the fast Numba kernel.\n",
    "    \"\"\"\n",
    "    raw_dtype = np.dtype([\n",
    "        ('id', np.int32), ('pos', 'f8', (3,)), ('u', 'f8', (4,)),\n",
    "        ('lambda_rest', 'f8'), ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "\n",
    "    # --- Step 1: Load the Raw Data (Fast I/O) ---\n",
    "    with open(raw_snapshot_file, 'rb') as f:\n",
    "        num_particles = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "        if num_particles == 0:\n",
    "            print(\"  Snapshot is empty, skipping.\")\n",
    "            return\n",
    "        particle_data = np.fromfile(f, dtype=raw_dtype, count=num_particles)\n",
    "\n",
    "    # --- Step 2: Call the Fast JIT-Compiled Kernel ---\n",
    "    # We only pass the position data to the kernel for efficiency.\n",
    "    particle_positions = np.ascontiguousarray(particle_data['pos'])\n",
    "    reordering_map, node_metadata = build_implicit_kdtree_kernel(particle_positions, num_particles)\n",
    "    \n",
    "    # Use the returned map to reorder the full, original particle data array\n",
    "    reordered_particles = particle_data[reordering_map]\n",
    "\n",
    "    # --- Step 3: Save the Two Parallel Arrays (Fast I/O) ---\n",
    "    output_filename = os.path.join(output_dir, os.path.basename(raw_snapshot_file).replace('.bin', '.kdtree.bin'))\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        f.write(np.uint64(num_particles))\n",
    "        f.write(np.uint64(3)) # dimensions\n",
    "        f.write(node_metadata.tobytes())\n",
    "        f.write(reordered_particles.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kdtree_preprocessor_parallel():\n",
    "    \"\"\"\n",
    "    Finds all raw snapshot .bin files and processes them in parallel\n",
    "    using a pool of worker processes.\n",
    "    \"\"\"\n",
    "    base_project_dir = \"project\"\n",
    "    mass_project_name = \"mass_integrator\"\n",
    "    input_folder = os.path.join(base_project_dir, mass_project_name, \"output\")\n",
    "    processed_folder = os.path.join(base_project_dir, \"processed_snapshots\")\n",
    "\n",
    "    print(f\"--- Starting K-d Tree Pre-processing (Parallel) ---\")\n",
    "    print(f\"Input directory:  '{input_folder}'\")\n",
    "    print(f\"Shared Output directory: '{processed_folder}'\")\n",
    "\n",
    "    os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(input_folder, \"mass_blueprint_t_*.bin\")))\n",
    "    \n",
    "    if not snapshot_files:\n",
    "        print(\"\\nWARNING: No raw snapshot files found. Did you run the mass_integrator C code first?\")\n",
    "        return\n",
    "\n",
    "    num_processes = cpu_count()\n",
    "    print(f\"\\nFound {len(snapshot_files)} files. Processing in parallel using {num_processes} CPU cores...\")\n",
    "\n",
    "    task_function = partial(build_and_save_kdtree_snapshot_fast, output_dir=processed_folder)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        list(tqdm(pool.imap_unordered(task_function, snapshot_files), total=len(snapshot_files)))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n--- All snapshots processed successfully in {end_time - start_time:.2f} seconds. ---\")\n",
    "    print(f\"The query-ready .kdtree.bin files are now in the shared directory: '{processed_folder}'.\")\n",
    "\n",
    "\n",
    "# --- How to Run ---\n",
    "# It's best practice to put the execution call inside this block\n",
    "if __name__ == '__main__':\n",
    "    # Make sure to call the parallel version!\n",
    "    run_kdtree_preprocessor_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79791de7",
   "metadata": {},
   "source": [
    "<a id='part_3b'></a>\n",
    "### 3.b: Inspect a K-d Tree File\n",
    "\n",
    "After building the k-d trees, it is useful to have a tool to verify that the output files are structured correctly. The `view_kdtree_snapshot` function reads a single `.kdtree.bin` file and prints a human-readable summary of its header and the first few particle records, confirming that the pre-processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdef2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_kdtree_snapshot(\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    processed_folder: str = \"processed_snapshots\",\n",
    "    snapshot_index: int = 0, # 0 for the first, -1 for the last\n",
    "    max_nodes_to_print: int = 15\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a single, processed .kdtree.bin file and prints a detailed,\n",
    "    human-readable summary to verify its contents and structure.\n",
    "    \n",
    "    UPDATED to read and display the full 4-velocity u^mu.\n",
    "    \"\"\"\n",
    "    print(\"--- K-d Tree Blueprint Inspector ---\")\n",
    "    \n",
    "    # --- Step 1: Find and select the snapshot file ---\n",
    "    snapshot_dir = os.path.join(project_dir, processed_folder)\n",
    "    if not os.path.isdir(snapshot_dir):\n",
    "        print(f\"ERROR: Processed snapshot directory not found at '{snapshot_dir}'\")\n",
    "        return\n",
    "\n",
    "    snapshot_files = sorted(glob.glob(os.path.join(snapshot_dir, \"*.kdtree.bin\")))\n",
    "    if not snapshot_files:\n",
    "        print(f\"ERROR: No .kdtree.bin files found in '{snapshot_dir}'\")\n",
    "        print(\"Please run the k-d tree pre-processor cell first.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        file_to_load = snapshot_files[snapshot_index]\n",
    "    except IndexError:\n",
    "        print(f\"ERROR: Snapshot index {snapshot_index} is out of bounds. Only {len(snapshot_files)} files exist.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Loading and inspecting file: '{os.path.basename(file_to_load)}'\")\n",
    "\n",
    "    # --- Step 2: Define the dtypes to read the file ---\n",
    "    # MODIFICATION: This dtype must exactly match the new particle data struct in C\n",
    "    particle_dtype = np.dtype([\n",
    "        ('id', np.int32), \n",
    "        ('pos', 'f8', (3,)), \n",
    "        ('u', 'f8', (4,)), # Changed from 'u_spatial' to 'u'\n",
    "        ('lambda_rest', 'f8'),\n",
    "        ('j_intrinsic', 'f4')\n",
    "    ])\n",
    "    metadata_dtype = np.int32\n",
    "\n",
    "    # --- Step 3: Read the binary file according to the custom format ---\n",
    "    try:\n",
    "        with open(file_to_load, 'rb') as f:\n",
    "            num_particles = np.fromfile(f, dtype=np.uint64, count=1)[0]\n",
    "            dimensions = np.fromfile(f, dtype=np.uint64, count=1)[0]\n",
    "            node_metadata = np.fromfile(f, dtype=metadata_dtype, count=num_particles)\n",
    "            particle_data = np.fromfile(f, dtype=particle_dtype, count=num_particles)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Failed to read or parse the binary file. Exception: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Step 4: Print a summary and verify the contents ---\n",
    "    print(\"\\n--- File Header ---\")\n",
    "    print(f\"  Number of Particles: {num_particles}\")\n",
    "    print(f\"  Dimensions:          {dimensions}\")\n",
    "    \n",
    "    if len(node_metadata) != num_particles or len(particle_data) != num_particles:\n",
    "        print(\"\\nERROR: Mismatch between header particle count and data array lengths!\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Data Verification ---\")\n",
    "    \n",
    "    axis_map = {0: 'X', 1: 'Y', 2: 'Z', -1: 'LEAF'}\n",
    "    \n",
    "    # MODIFICATION: Updated header to show the full 4-velocity\n",
    "    header = (f\"{'Index':<6} | {'Split Axis':<10} | {'Particle ID':<12} | \"\n",
    "              f\"{'Position (x, y, z)':<25} | {'4-Velocity (ut, ux, uy, uz)':<40}\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for i in range(min(num_particles, max_nodes_to_print)):\n",
    "        split_axis_val = node_metadata[i]\n",
    "        particle = particle_data[i]\n",
    "        \n",
    "        split_axis_str = axis_map.get(split_axis_val, 'INVALID')\n",
    "        pos_str = (f\"({particle['pos'][0]:>6.2f}, {particle['pos'][1]:>6.2f}, \"\n",
    "                   f\"{particle['pos'][2]:>6.2f})\")\n",
    "        \n",
    "        # MODIFICATION: Format the full 4-velocity for display\n",
    "        vel_str = (f\"({particle['u'][0]:>6.2f}, {particle['u'][1]:>6.2f}, \"\n",
    "                   f\"{particle['u'][2]:>6.2f}, {particle['u'][3]:>6.2f})\")\n",
    "        \n",
    "        print(f\"{i:<6} | {split_axis_str:<10} | {particle['id']:<12} | \"\n",
    "              f\"{pos_str:<25} | {vel_str:<40}\")\n",
    "\n",
    "    if num_particles > max_nodes_to_print:\n",
    "        print(\"...\")\n",
    "        print(f\"(... and {num_particles - max_nodes_to_print} more nodes)\")\n",
    "\n",
    "    print(\"\\n--- Verification Complete ---\")\n",
    "    print(\"Check that the 4-Velocity column contains four components and looks reasonable.\")\n",
    "view_kdtree_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa7ece",
   "metadata": {},
   "source": [
    "<a id='part_4a'></a>\n",
    "## Part 4: Verification Suite for Keplerian Orbits\n",
    "\n",
    "### 4.a: Analytical Solution and Verification Driver\n",
    "\n",
    "This section provides a comprehensive verification suite to test the numerical accuracy of the `mass_integrator` for stable, circular, equatorial (Keplerian) orbits.\n",
    "\n",
    "*   **`get_analytical_solution`**: This function computes the exact, theoretical trajectory of a massive particle in a Keplerian orbit in the Kerr spacetime. It uses a numerically stable method to avoid precision issues near the ISCO.\n",
    "*   **`verify_mass_integrator`**: This is the main driver function. For a given set of test cases (black hole spin `a` and orbital radius `r`), it:\n",
    "    1.  Writes the analytical initial conditions to the debug file.\n",
    "    2.  Runs the compiled C code in debug mode.\n",
    "    3.  Reads the resulting numerical trajectory.\n",
    "    4.  Compares the numerical result against the analytical solution point-by-point to calculate the error in position, radius, and orbital phase.\n",
    "    5.  Generates plots visualizing the trajectory and the error growth over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical_solution(tau_values, r_initial, M_scale, a_spin):\n",
    "    \"\"\"\n",
    "    Calculates the analytical trajectory (t,x,y,z) for a circular, equatorial\n",
    "    orbit in Kerr spacetime using a NUMERICALLY STABLE method.\n",
    "\n",
    "    This version implements the three-step process that avoids catastrophic\n",
    "    cancellation for orbits near the ISCO.\n",
    "\n",
    "    Args:\n",
    "        tau_values (np.ndarray): Array of proper time values.\n",
    "        r_initial (float): The constant radius of the orbit.\n",
    "        M_scale (float): The mass of the black hole.\n",
    "        a_spin (float): The spin parameter of the black hole.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U_t, Omega_tau, analytical_positions)\n",
    "    \"\"\"\n",
    "    # Use shorter variable names for clarity\n",
    "    r = r_initial\n",
    "    M = M_scale\n",
    "    a = a_spin\n",
    "    \n",
    "    # --- Step 1: Calculate the stable angular velocity Omega = d(phi)/dt ---\n",
    "    sqrt_M = np.sqrt(M)\n",
    "    Omega = sqrt_M / (r**1.5 + a * sqrt_M)\n",
    "    \n",
    "    # --- Step 2: Calculate the required metric components in Boyer-Lindquist ---\n",
    "    # These are specialized for the equatorial plane (theta=pi/2)\n",
    "    g_tt = -(1 - 2*M/r)\n",
    "    g_tphi = -2*a*M/r\n",
    "    g_phiphi = r**2 + a**2 + (2*M*a**2)/r\n",
    "    \n",
    "    # --- Step 3: Solve for u^t and u^phi using the stable formulas ---\n",
    "    ut_squared_inv_denom = g_tt + 2*g_tphi*Omega + g_phiphi*Omega**2\n",
    "    \n",
    "    # Check for instability. If the denominator is non-negative, the orbit is not possible.\n",
    "    if ut_squared_inv_denom >= 0:\n",
    "        # Return NaNs to signal that this orbit is unstable/invalid.\n",
    "        nan_array = np.full((len(tau_values), 4), np.nan)\n",
    "        return np.nan, np.nan, nan_array\n",
    "\n",
    "    ut_squared = -1.0 / ut_squared_inv_denom\n",
    "    U_t = np.sqrt(ut_squared)\n",
    "    \n",
    "    u_phi = Omega * U_t\n",
    "    Omega_tau = u_phi # d(phi)/d(tau) is u^phi\n",
    "\n",
    "    # --- Step 4: Calculate the trajectory over the given proper time values ---\n",
    "    phi_of_tau = Omega_tau * tau_values\n",
    "    t_of_tau = U_t * tau_values\n",
    "    x_of_tau = r_initial * np.cos(phi_of_tau)\n",
    "    y_of_tau = r_initial * np.sin(phi_of_tau)\n",
    "    z_of_tau = np.zeros_like(tau_values)\n",
    "    \n",
    "    analytical_positions = np.vstack([t_of_tau, x_of_tau, y_of_tau, z_of_tau]).T\n",
    "    \n",
    "    return U_t, Omega_tau, analytical_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "def verify_mass_integrator(\n",
    "    test_cases: List[Tuple[float, float]],\n",
    "    M_scale: float = 1.0,\n",
    "    tau_max: float = 2000.0,\n",
    "    project_dir: str = \"project/mass_integrator\",\n",
    "    executable_name: str = \"mass_integrator\",\n",
    "    output_folder: str = \"verification_plots\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Runs a verification suite for the massive particle integrator for specific test cases.\n",
    "    \n",
    "    VERSION 3: Includes robust NaN checking to correctly handle and report unstable orbits.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Mass Integrator Verification Suite ---\")\n",
    "    \n",
    "    full_output_folder = os.path.join(project_dir, output_folder)\n",
    "    os.makedirs(full_output_folder, exist_ok=True)\n",
    "    print(f\"Plots will be saved to: {full_output_folder}\")\n",
    "    \n",
    "    for a_spin, r_initial in test_cases:\n",
    "        print(f\"\\n--- Verifying r_initial={r_initial:.2f}, a_spin={a_spin:.2f} ---\")\n",
    "\n",
    "        # --- 1. Generate Initial Conditions ---\n",
    "        U_t_initial, Omega_tau_initial, _ = get_analytical_solution(np.array([0.0]), r_initial, M_scale, a_spin)\n",
    "        \n",
    "        # *** ROBUSTNESS CHECK 1: Check if analytical solution itself is unstable ***\n",
    "        if np.isnan(U_t_initial):\n",
    "            print(\"  [✗] ANALYTICAL FAILURE: The chosen r_initial is inside the ISCO or numerically unstable.\")\n",
    "            print(\"      Skipping this test case.\")\n",
    "            continue\n",
    "        # *** END OF CHECK ***\n",
    "\n",
    "        u_x_initial = 0.0\n",
    "        u_y_initial = r_initial * Omega_tau_initial\n",
    "        u_z_initial = 0.0\n",
    "        \n",
    "        ic_filename = os.path.join(project_dir, \"particle_debug_initial_conditions.txt\")\n",
    "        with open(ic_filename, \"w\") as f:\n",
    "            f.write(\"# Format: t_initial pos_x pos_y pos_z u_x u_y u_z\\n\")\n",
    "            f.write(f\"0.0 {r_initial:.10f} 0.0 0.0   {u_x_initial:.10f} {u_y_initial:.10f} {u_z_initial:.10f}\\n\")\n",
    "        \n",
    "        # --- 2. Run the C Integrator ---\n",
    "        par_filename = os.path.join(project_dir, \"mass_integrator.par\")\n",
    "        with open(par_filename, \"w\") as f:\n",
    "            f.write(f\"run_in_debug_mode = True\\n\")\n",
    "            f.write(f\"a_spin = {a_spin:.10f}\\n\")\n",
    "            f.write(f\"M_scale = {M_scale:.10f}\\n\")\n",
    "            f.write(f\"t_max_integration = {tau_max * 2 * U_t_initial}\\n\")\n",
    "            f.write(f\"metric_choice = 0\\n\")\n",
    "\n",
    "        output_path = os.path.join(project_dir, \"massive_particle_path.txt\")\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                f\"./{executable_name}\", shell=True, capture_output=True, text=True, check=True, cwd=project_dir\n",
    "            )\n",
    "            print(\"  [✓] C Integrator ran successfully.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  [✗] ERROR: C Integrator failed for r={r_initial}, a={a_spin}.\")\n",
    "            print(e.stderr)\n",
    "            continue\n",
    "\n",
    "        # --- 3. Load Numerical and Generate Analytical Results ---\n",
    "        try:\n",
    "            numerical_data = np.loadtxt(output_path, skiprows=1)\n",
    "            if numerical_data.size == 0:\n",
    "                print(\"  [✗] NUMERICAL FAILURE: Output file is empty.\")\n",
    "                continue\n",
    "            numerical_data = numerical_data[numerical_data[:, 0] <= tau_max]\n",
    "            print(f\"  [✓] Loaded {len(numerical_data)} numerical data points.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [✗] ERROR: Could not load or parse output file '{output_path}'. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # *** ROBUSTNESS CHECK 2: Check if numerical integrator produced NaNs ***\n",
    "        if np.any(np.isnan(numerical_data)):\n",
    "            print(\"  [✗] NUMERICAL FAILURE: The C integrator produced NaN values.\")\n",
    "            print(\"      This indicates the orbit was unstable as predicted. Skipping statistics.\")\n",
    "            continue\n",
    "        # *** END OF CHECK ***\n",
    "\n",
    "        tau_values = numerical_data[:, 0]\n",
    "        _, _, analytical_data = get_analytical_solution(tau_values, r_initial, M_scale, a_spin)\n",
    "        print(\"  [✓] Generated analytical ground truth.\")\n",
    "\n",
    "        # --- 4. Calculate Statistics ---\n",
    "        pos_error = np.sqrt(np.sum((numerical_data[:, 2:5] - analytical_data[:, 1:4])**2, axis=1))\n",
    "        radius_error = np.abs(np.sqrt(numerical_data[:, 2]**2 + numerical_data[:, 3]**2) - r_initial)\n",
    "        \n",
    "        phi_num = np.unwrap(np.arctan2(numerical_data[:, 3], numerical_data[:, 2]))\n",
    "        phi_ana = np.unwrap(np.arctan2(analytical_data[:, 2], analytical_data[:, 1]))\n",
    "        phase_error = np.abs(phi_num - phi_ana)\n",
    "\n",
    "        print(\"\\n  --- STATISTICAL REPORT ---\")\n",
    "        print(f\"  Final Position Error:      {pos_error[-1]:.3e} M\")\n",
    "        print(f\"  Max Radius Error (Drift):  {np.max(radius_error):.3e} M\")\n",
    "        print(f\"  Mean Radius Error:         {np.mean(radius_error):.3e} M\")\n",
    "        print(f\"  Max Phase Error (Timing):  {np.max(phase_error):.3e} radians\")\n",
    "        print(f\"  Mean Phase Error:          {np.mean(phase_error):.3e} radians\")\n",
    "        \n",
    "        # *** ROBUSTNESS CHECK 3: Corrected verdict logic ***\n",
    "        if np.max(radius_error) > 1e-5 or np.max(phase_error) > 1e-5:\n",
    "             print(\"\\n  --- VERDICT ---\\n  FAIL: Error metrics exceed tolerance.\")\n",
    "        else:\n",
    "             print(\"\\n  --- VERDICT ---\\n  PASS: All error metrics are within tolerance.\")\n",
    "        # *** END OF CHECK ***\n",
    "\n",
    "        # --- 5. Generate and Save Plots ---\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        fig.suptitle(f\"Verification for r_initial={r_initial:.2f}, a_spin={a_spin:.2f}\", fontsize=16)\n",
    "\n",
    "        axes[0].plot(analytical_data[:, 1], analytical_data[:, 2], 'r--', label='Analytical', lw=2)\n",
    "        axes[0].plot(numerical_data[:, 2], numerical_data[:, 3], 'c-', label='Numerical', lw=1, alpha=0.8)\n",
    "        axes[0].set_title(\"Orbit Trajectory (Top-Down View)\")\n",
    "        axes[0].set_xlabel(\"x (M)\"); axes[0].set_ylabel(\"y (M)\")\n",
    "        axes[0].set_aspect('equal', 'box')\n",
    "        axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "        axes[1].plot(tau_values, radius_error, label='Radius Error |r_num - r_initial|')\n",
    "        axes[1].plot(tau_values, phase_error, label='Phase Error |φ_num - φ_ana|')\n",
    "        axes[1].set_title(\"Error Growth over Proper Time\")\n",
    "        axes[1].set_xlabel(\"Proper Time (τ) [M]\"); axes[1].set_ylabel(\"Error\")\n",
    "        axes[1].set_yscale('log')\n",
    "        axes[1].legend(); axes[1].grid(True)\n",
    "        \n",
    "        plot_filename = f\"verification_r{r_initial:.2f}_a{a_spin:.2f}.png\"\n",
    "        full_plot_path = os.path.join(full_output_folder, plot_filename)\n",
    "        plt.savefig(full_plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  [✓] Plot saved to '{full_plot_path}'\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7abc5",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-spin (Schwarzschild) orbits (a=0.0)\n",
    "# ISCO is at r = 6.0 M\n",
    "schwarzschild_test_cases = [\n",
    "    # (a_spin, r_initial)\n",
    "    (0.0, 6.0),    # At the ISCO\n",
    "    (0.0, 6.5),    # Near-ISCO\n",
    "    (0.0, 7.0),\n",
    "    (0.0, 8.0),\n",
    "    (0.0, 10.0),   # Intermediate orbit\n",
    "    (0.0, 15.0),\n",
    "    (0.0, 20.0),\n",
    "    (0.0, 30.0),\n",
    "    (0.0, 50.0),\n",
    "    (0.0, 100.0)   # Weak-field orbit\n",
    "]\n",
    "\n",
    "# Example function call:\n",
    "verify_mass_integrator(\n",
    "     test_cases=schwarzschild_test_cases,\n",
    "     output_folder=\"verification_schwarzschild_a0.00\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
